{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3, 28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = np.random.randint(0, 256,(10, 3, 28, 28)) # N x C x H x W\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#def softmax(x):\n",
    "#        c = np.max(x)\n",
    "#        exp_x = np.exp(x - c)\n",
    "#        sum_exp_x = np.sum(exp_x)\n",
    "#        y = exp_x / sum_exp_x\n",
    "#        \n",
    "#        return y\n",
    " \n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        c = np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x - c)\n",
    "        sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return exp_x / sum_exp_x\n",
    "    else:\n",
    "        c = np.max(x)\n",
    "        exp_x = np.exp(x - c)\n",
    "        return exp_x / np.sum(exp_x)\n",
    "       \n",
    "def CEE(y, t):\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(1, -1)\n",
    "        t = t.reshape(1)\n",
    "    \n",
    "    if t.ndim == 1:\n",
    "        return -np.sum(np.log(y[np.arange(len(t)), t] + 1e-7)) / len(t)\n",
    "    else:\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / y.shape[0]\n",
    "\n",
    "def gradient(f, x):\n",
    "    h = 0.0001\n",
    "\n",
    "    shape = x.shape\n",
    "    x_flatten = x.reshape(-1)\n",
    "    size = len(x_flatten)\n",
    "\n",
    "    grad = np.zeros_like(x_flatten)\n",
    "\n",
    "    for i in range(size):\n",
    "        x_val = x_flatten[i]\n",
    "\n",
    "        x_flatten[i] = x_val + h\n",
    "        fxh1 = f(x_flatten.reshape(shape))\n",
    "\n",
    "        x_flatten[i] = x_val - h\n",
    "        fxh2 = f(x_flatten.reshape(shape))\n",
    "\n",
    "        grad[i] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x_flatten[i] = x_val\n",
    "        \n",
    "    return grad.reshape(shape)\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return (x > 0)*x\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        return dz*(self.x > 0).astype(int)\n",
    "    \n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = 1 / (1 + np.exp(-self.x))\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        return dz * self.y *(1 - self.y)\n",
    "    \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.X = None \n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X        \n",
    "        return np.dot(X, self.W) + self.b\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        grads = {}\n",
    "        grads[\"W\"] = np.dot(self.X.T, dz)\n",
    "        grads[\"X\"] = np.dot(dz, self.W.T)\n",
    "        grads[\"b\"] = np.sum(dz, axis=0)\n",
    "        \n",
    "        return grads\n",
    "\n",
    "class Softmax_with_Loss:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, X, t):\n",
    "        self.y = softmax(X)\n",
    "        self.t = t\n",
    "        loss = CEE(self.y, self.t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "        if self.t.ndim == 1:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "        else:\n",
    "            dx = (self.y - self.t)\n",
    "        \n",
    "        return dx / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def padding(input_data, pad=0):\n",
    "    _, _, h, w = input_data.shape\n",
    "    padded_data = []\n",
    "    \n",
    "    for data in input_data:\n",
    "        temp_data = []\n",
    "        for c_data in data:\n",
    "            padded_c_data = np.zeros((h + 2*pad, w + 2*pad))\n",
    "            padded_c_data[pad:pad+h, pad:pad+w] = c_data\n",
    "            temp_data.append(list(padded_c_data))\n",
    "\n",
    "        padded_data.append(temp_data)\n",
    "\n",
    "    return np.array(padded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    input_data = padding(input_data, pad=pad)\n",
    "    \n",
    "    data_h = input_data.shape[2]\n",
    "    data_w = input_data.shape[3]\n",
    "    \n",
    "    cols = []\n",
    "    \n",
    "    end_w_index = data_w - filter_w\n",
    "    end_h_index = data_h - filter_h\n",
    "    \n",
    "    for data in input_data:\n",
    "        for h_index in range(0, end_h_index+1, stride):\n",
    "            for w_index in range(0, end_w_index+1, stride):\n",
    "                feature_map = data[:, h_index:h_index+filter_h, w_index:w_index+filter_w]\n",
    "                cols.append(list(feature_map.reshape(-1,)))\n",
    "\n",
    "    return np.array(cols)\n",
    "\n",
    "def col2im(cols, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    data_n, data_c, data_h, data_w = input_shape\n",
    "    output_h = int((data_h + 2 * pad - filter_h) // stride + 1)\n",
    "    output_w = int((data_w + 2 * pad - filter_w) // stride + 1)\n",
    "\n",
    "    img = np.zeros((data_n, data_c, data_h + 2 * pad, data_w + 2 * pad))\n",
    "\n",
    "    cols_reshaped = cols.reshape(data_n, output_h, output_w, data_c, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    for h_index in range(filter_h):\n",
    "        h_end = h_index + stride * output_h\n",
    "        for w_index in range(filter_w):\n",
    "            w_end = w_index + stride * output_w\n",
    "            img[:, :, h_index:h_end:stride, w_index:w_end:stride] += cols_reshaped[:, :, h_index, w_index, :, :]\n",
    "\n",
    "    return img[:, :, pad:pad + data_h, pad:pad + data_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.grads = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        filter_num, c, filter_h, filter_w = self.W.shape\n",
    "        data_num, _, h, w = x.shape\n",
    "        \n",
    "        output_h = int((h + 2*self.pad - filter_h) / self.stride + 1)\n",
    "        output_w = int((w + 2*self.pad - filter_w) / self.stride + 1)\n",
    "        \n",
    "        self.cols_img_data = im2col(x, filter_h, filter_w, stride=self.stride, pad=self.pad)\n",
    "        #print(f\"shape of cols_img : {self.cols_img_data.shape}\")\n",
    "        self.cols_W = self.W.reshape(filter_num, -1)\n",
    "        #print(f\"shape of cols_W : {cols_W.T.shape}\")\n",
    "        output = np.dot(self.cols_img_data, self.cols_W.T) + self.b\n",
    "        output = output.reshape(data_num, output_h, output_w, filter_num).transpose(0, 3, 1, 2)\n",
    "        #print(f\"shape of output : {output.shape}\")\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        filter_num, c, filter_h, filter_w = self.W.shape\n",
    "\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, filter_num)            # (N*out_h*out_w, FN)\n",
    "\n",
    "        self.grads[\"b\"] = np.sum(dout, axis=0)\n",
    "        self.grads[\"W\"] = np.dot(self.cols_img_data.T, dout).transpose(1, 0).reshape(filter_num, c, filter_h, filter_w)                               # (C*FH*FW, FN)\n",
    "        # input gradient\n",
    "        dcols_img_data = np.dot(dout, self.cols_W)                            # (N*out_h*out_w, C*FH*FW)\n",
    "        self.grads[\"X\"] = col2im(dcols_img_data, self.x.shape, filter_h, filter_w, self.stride, self.pad)\n",
    "\n",
    "        return self.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pooling_h=2, pooling_w=2, stride=2, pad=0):\n",
    "        self.pooling_h = pooling_h\n",
    "        self.pooling_w = pooling_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.output_h = None\n",
    "        self.output_w = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        data_num, c, h, w = x.shape\n",
    "        self.x = x        \n",
    "        self.output_h = int((h + 2*self.pad - self.pooling_h) / self.stride + 1)\n",
    "        self.output_w = int((w + 2*self.pad - self.pooling_w) / self.stride + 1)\n",
    "        \n",
    "        self.cols_data = im2col(x, self.pooling_h, self.pooling_w, stride=self.stride, pad=self.pad) # N*OH*OW x C*PH*PW\n",
    "        cols = self.cols_data.reshape(-1, self.pooling_h*self.pooling_w) # N*OH*OW x C*PH*PW -> N*OH*OW*C x PH*PW\n",
    "        \n",
    "        self.arg_max = np.argmax(cols, axis=1)\n",
    "        pooling_res_flatten = np.max(cols, axis=1)\n",
    "        pooling_res = pooling_res_flatten.reshape(data_num, self.output_h, self.output_w, c).transpose(0, 3, 1, 2) # N x OH x OW x C -> N x C x OH x OW\n",
    "        \n",
    "        return pooling_res # N x C x OH x OW\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        data_num, c, h, w = self.x.shape\n",
    "        \n",
    "        dpolling_res_flatten = dout.transpose(0, 2, 3, 1).reshape(-1)\n",
    "\n",
    "        dcols = np.zeros((data_num * c * self.output_h * self.output_w, self.pooling_h * self.pooling_w))\n",
    "        dcols[np.arange(len(self.arg_max)), self.arg_max] = dpolling_res_flatten\n",
    "\n",
    "        dcols_data = dcols.reshape(self.cols_data.shape[0], -1)\n",
    "        return col2im(dcols_data, self.x.shape, self.pooling_h, self.pooling_w, self.stride, self.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "def padding(input_data, pad=0):\n",
    "    _, _, h, w = input_data.shape\n",
    "    padded_data = []\n",
    "    \n",
    "    for data in input_data:\n",
    "        temp_data = []\n",
    "        for c_data in data:\n",
    "            padded_c_data = np.zeros((h + 2*pad, w + 2*pad))\n",
    "            padded_c_data[pad:pad+h, pad:pad+w] = c_data\n",
    "            temp_data.append(list(padded_c_data))\n",
    "\n",
    "        padded_data.append(temp_data)\n",
    "\n",
    "    return np.array(padded_data)\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    input_data = padding(input_data, pad=pad)\n",
    "    \n",
    "    data_h = input_data.shape[2]\n",
    "    data_w = input_data.shape[3]\n",
    "    \n",
    "    cols = []\n",
    "    \n",
    "    end_w_index = data_w - filter_w\n",
    "    end_h_index = data_h - filter_h\n",
    "    \n",
    "    for data in input_data:\n",
    "        for h_index in range(0, end_h_index+1, stride):\n",
    "            for w_index in range(0, end_w_index+1, stride):\n",
    "                feature_map = data[:, h_index:h_index+filter_h, w_index:w_index+filter_w]\n",
    "                cols.append(list(feature_map.reshape(-1,)))\n",
    "\n",
    "    return np.array(cols)\n",
    "\n",
    "def col2im(cols, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    data_n, data_c, data_h, data_w = input_shape\n",
    "    output_h = int((data_h + 2 * pad - filter_h) // stride + 1)\n",
    "    output_w = int((data_w + 2 * pad - filter_w) // stride + 1)\n",
    "\n",
    "    img = np.zeros((data_n, data_c, data_h + 2 * pad, data_w + 2 * pad))\n",
    "\n",
    "    cols_reshaped = cols.reshape(data_n, output_h, output_w, data_c, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    for h_index in range(filter_h):\n",
    "        h_end = h_index + stride * output_h\n",
    "        for w_index in range(filter_w):\n",
    "            w_end = w_index + stride * output_w\n",
    "            img[:, :, h_index:h_end:stride, w_index:w_end:stride] += cols_reshaped[:, :, h_index, w_index, :, :]\n",
    "\n",
    "    return img[:, :, pad:pad + data_h, pad:pad + data_w]\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.grads = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        filter_num, c, filter_h, filter_w = self.W.shape\n",
    "        data_num, _, h, w = x.shape\n",
    "        \n",
    "        output_h = int((h + 2*self.pad - filter_h) / self.stride + 1)\n",
    "        output_w = int((w + 2*self.pad - filter_w) / self.stride + 1)\n",
    "        \n",
    "        self.cols_img_data = im2col(x, filter_h, filter_w, stride=self.stride, pad=self.pad)\n",
    "        #print(f\"shape of cols_img : {self.cols_img_data.shape}\")\n",
    "        self.cols_W = self.W.reshape(filter_num, -1)\n",
    "        #print(f\"shape of cols_W : {cols_W.T.shape}\")\n",
    "        output = np.dot(self.cols_img_data, self.cols_W.T) + self.b\n",
    "        output = output.reshape(data_num, output_h, output_w, filter_num).transpose(0, 3, 1, 2)\n",
    "        #print(f\"shape of output : {output.shape}\")\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        filter_num, c, filter_h, filter_w = self.W.shape\n",
    "\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, filter_num)            # (N*out_h*out_w, FN)\n",
    "\n",
    "        self.grads[\"b\"] = np.sum(dout, axis=0)\n",
    "        self.grads[\"W\"] = np.dot(self.cols_img_data.T, dout).transpose(1, 0).reshape(filter_num, c, filter_h, filter_w)                               # (C*FH*FW, FN)\n",
    "        # input gradient\n",
    "        dcols_img_data = np.dot(dout, self.cols_W)                            # (N*out_h*out_w, C*FH*FW)\n",
    "        self.grads[\"X\"] = col2im(dcols_img_data, self.x.shape, filter_h, filter_w, self.stride, self.pad)\n",
    "\n",
    "        return self.grads\n",
    "class Pooling:\n",
    "    def __init__(self, pooling_h=2, pooling_w=2, stride=2, pad=0):\n",
    "        self.pooling_h = pooling_h\n",
    "        self.pooling_w = pooling_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.output_h = None\n",
    "        self.output_w = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        data_num, c, h, w = x.shape\n",
    "        self.x = x        \n",
    "        self.output_h = int((h + 2*self.pad - self.pooling_h) / self.stride + 1)\n",
    "        self.output_w = int((w + 2*self.pad - self.pooling_w) / self.stride + 1)\n",
    "        \n",
    "        self.cols_data = im2col(x, self.pooling_h, self.pooling_w, stride=self.stride, pad=self.pad) # N*OH*OW x C*PH*PW\n",
    "        cols = self.cols_data.reshape(-1, self.pooling_h*self.pooling_w) # N*OH*OW x C*PH*PW -> N*OH*OW*C x PH*PW\n",
    "        \n",
    "        self.arg_max = np.argmax(cols, axis=1)\n",
    "        pooling_res_flatten = np.max(cols, axis=1)\n",
    "        pooling_res = pooling_res_flatten.reshape(data_num, self.output_h, self.output_w, c).transpose(0, 3, 1, 2) # N x OH x OW x C -> N x C x OH x OW\n",
    "        \n",
    "        return pooling_res # N x C x OH x OW\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        data_num, c, h, w = self.x.shape\n",
    "        \n",
    "        dpolling_res_flatten = dout.transpose(0, 2, 3, 1).reshape(-1)\n",
    "\n",
    "        dcols = np.zeros((data_num * c * self.output_h * self.output_w, self.pooling_h * self.pooling_w))\n",
    "        dcols[np.arange(len(self.arg_max)), self.arg_max] = dpolling_res_flatten\n",
    "\n",
    "        dcols_data = dcols.reshape(self.cols_data.shape[0], -1)\n",
    "        return col2im(dcols_data, self.x.shape, self.pooling_h, self.pooling_w, self.stride, self.pad)\n",
    "    \n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim, conv_param, hidden_size=100, output_size=10, weight_init_std=0.001):\n",
    "        self.data_c = input_dim[0]\n",
    "        self.data_h = input_dim[1]\n",
    "        self.data_w = input_dim[2]\n",
    "        \n",
    "        self.filter_num = conv_param[\"filter_num\"]\n",
    "        self.filter_h = conv_param[\"filter_size\"]\n",
    "        self.filter_w = conv_param[\"filter_size\"]\n",
    "        \n",
    "        self.stride = conv_param[\"stride\"]\n",
    "        self.pad = conv_param[\"pad\"]\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_init_std = weight_init_std\n",
    "        \n",
    "        self.conv_output_h = None\n",
    "        self.conv_output_w = None\n",
    "        \n",
    "        self.pooling_output_h = None\n",
    "        self.pooling_output_w = None\n",
    "        \n",
    "        self.params = {}\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        # input : N x C x H x W\n",
    "        # output : N x FN x OH x OW\n",
    "        # OH = (H + 2*pad - filter_h)/stride + 1\n",
    "        # OW = (W + 2*pad - filter_w)/stride + 1\n",
    "        self.params[\"W1\"] = np.random.randn(self.filter_num, self.data_c, self.filter_h, self.filter_w) * self.weight_init_std\n",
    "        self.params[\"B1\"] = np.zeros(self.filter_num)\n",
    "        self.layers[\"Conv\"] = Convolution(self.params[\"W1\"], self.params[\"B1\"], stride=self.stride, pad=self.pad)\n",
    "        self.conv_output_h = int((self.data_h + 2*self.pad - self.filter_h)/self.stride + 1)\n",
    "        self.conv_output_w = int((self.data_w + 2*self.pad - self.filter_w)/self.stride + 1)\n",
    "        \n",
    "        self.layers[\"ReLU1\"] = ReLU()\n",
    "        \n",
    "        # input : N x FN x OH x OW\n",
    "        # output : N x FN x Pooling_OH x Pooling_OW\n",
    "        # Pooling_OH = (OH + 2*pad - pooling_h)/stride + 1 -> (OH - 2)/2 + 1 \n",
    "        # Pooling_OW = (OW + 2*pad - pooling_h)/stride + 1\n",
    "        pooling_h = 2\n",
    "        pooling_w = 2\n",
    "        pooling_stride = 2\n",
    "        pooling_pad = 0\n",
    "        self.layers[\"Pooling\"] = Pooling(pooling_h, pooling_w, pooling_stride, pooling_pad)\n",
    "        self.pooling_output_h = int((self.conv_output_h + 2*pooling_pad - pooling_h)/pooling_stride + 1)\n",
    "        self.pooling_output_w = int((self.conv_output_w + 2*pooling_pad - pooling_w)/pooling_stride + 1)\n",
    "        \n",
    "        # input : N x FN x OH x OW\n",
    "        # output : N x FN x Pooling_OH x Pooling_OW\n",
    "        # OH = (OH + 2*pad - pooling_h)/stride + 1\n",
    "        # OW = (OW + 2*pad - pooling_h)/stride + 1\n",
    "        feature_size = self.filter_num*self.pooling_output_h*self.pooling_output_w\n",
    "        self.params[\"W2\"] = np.random.randn(feature_size, self.hidden_size) * self.weight_init_std\n",
    "        self.params[\"B2\"] = np.zeros(self.hidden_size)\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W2\"], self.params[\"B2\"])\n",
    "        self.layers[\"ReLU2\"] = ReLU()\n",
    "        \n",
    "        \n",
    "        self.params[\"W3\"] = np.random.randn(self.hidden_size, self.output_size) * self.weight_init_std\n",
    "        self.params[\"B3\"] = np.zeros(self.output_size)\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W3\"], self.params[\"B3\"])\n",
    "        \n",
    "        self.last_layer = Softmax_with_Loss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        data_n = x.shape[0]\n",
    "        f = self.layers[\"Conv\"].forward(x)\n",
    "        f = self.layers[\"ReLU1\"].forward(f)\n",
    "        f = self.layers[\"Pooling\"].forward(f)\n",
    "        f = f.reshape(data_n, -1)\n",
    "        h = self.layers[\"Affine1\"].forward(f)\n",
    "        h = self.layers[\"ReLU2\"].forward(h)\n",
    "        y = self.layers[\"Affine2\"].forward(h)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        loss = self.last_layer.forward(y, t)\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        logit = self.predict(x)  # 예측 결과 로짓 (N, 10)\n",
    "        y = np.argmax(logit, axis=1)\n",
    "\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "    \n",
    "        return accuracy\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        grads = {}\n",
    "    \n",
    "        self.loss(x, t)  # 내부적으로 forward 수행됨\n",
    "    \n",
    "        # Backward from Softmax\n",
    "        dout = self.last_layer.backward()\n",
    "    \n",
    "        # Affine2 backward\n",
    "        temp_grads = self.layers[\"Affine2\"].backward(dout)\n",
    "        grads[\"W3\"] = temp_grads[\"W\"]\n",
    "        grads[\"B3\"] = temp_grads[\"b\"]\n",
    "        dout = temp_grads[\"X\"]\n",
    "    \n",
    "        # ReLU2 backward\n",
    "        dout = self.layers[\"ReLU2\"].backward(dout)\n",
    "    \n",
    "        # Affine1 backward\n",
    "        temp_grads = self.layers[\"Affine1\"].backward(dout)\n",
    "        grads[\"W2\"] = temp_grads[\"W\"]\n",
    "        grads[\"B2\"] = temp_grads[\"b\"]\n",
    "        dout = temp_grads[\"X\"]\n",
    "    \n",
    "        # Reshape to N x C x H x W before Pooling backward\n",
    "        dout = dout.reshape(-1, self.filter_num, self.pooling_output_h, self.pooling_output_w)\n",
    "    \n",
    "        # Pooling backward\n",
    "        dout = self.layers[\"Pooling\"].backward(dout)\n",
    "    \n",
    "        # ReLU1 backward\n",
    "        dout = self.layers[\"ReLU1\"].backward(dout)\n",
    "    \n",
    "        # Conv backward\n",
    "        temp_grads = self.layers[\"Conv\"].backward(dout)\n",
    "        grads[\"W1\"] = temp_grads[\"W\"]\n",
    "        grads[\"B1\"] = temp_grads[\"b\"]\n",
    "    \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch : 0 | iteration : 0/600 | train loss : 2.302479648966259\n",
      "| epoch : 0 | iteration : 10/600 | train loss : 0.8510506325918848\n",
      "| epoch : 0 | iteration : 20/600 | train loss : 0.48679326733747624\n",
      "| epoch : 0 | iteration : 30/600 | train loss : 0.1991722295205026\n",
      "| epoch : 0 | iteration : 40/600 | train loss : 0.28200213583028505\n",
      "| epoch : 0 | iteration : 50/600 | train loss : 0.2050880366224326\n",
      "| epoch : 0 | iteration : 60/600 | train loss : 0.3101569607678131\n",
      "| epoch : 0 | iteration : 70/600 | train loss : 0.2438750658054127\n",
      "| epoch : 0 | iteration : 80/600 | train loss : 0.14048609547609214\n",
      "| epoch : 0 | iteration : 90/600 | train loss : 0.10229013545419852\n",
      "| epoch : 0 | iteration : 100/600 | train loss : 0.1463477665264461\n",
      "| epoch : 0 | iteration : 110/600 | train loss : 0.16255681323457863\n",
      "| epoch : 0 | iteration : 120/600 | train loss : 0.1007419600387173\n",
      "| epoch : 0 | iteration : 130/600 | train loss : 0.25751932041998754\n",
      "| epoch : 0 | iteration : 140/600 | train loss : 0.11489977849955631\n",
      "| epoch : 0 | iteration : 150/600 | train loss : 0.15869192356142128\n",
      "| epoch : 0 | iteration : 160/600 | train loss : 0.07350235162140235\n",
      "| epoch : 0 | iteration : 170/600 | train loss : 0.0949351440107196\n",
      "| epoch : 0 | iteration : 180/600 | train loss : 0.20649130425399867\n",
      "| epoch : 0 | iteration : 190/600 | train loss : 0.04420189771827529\n",
      "| epoch : 0 | iteration : 200/600 | train loss : 0.09125514949991595\n",
      "| epoch : 0 | iteration : 210/600 | train loss : 0.1578607921700807\n",
      "| epoch : 0 | iteration : 220/600 | train loss : 0.050787513529169104\n",
      "| epoch : 0 | iteration : 230/600 | train loss : 0.0457181683671372\n",
      "| epoch : 0 | iteration : 240/600 | train loss : 0.04928737182906338\n",
      "| epoch : 0 | iteration : 250/600 | train loss : 0.15476444347964646\n",
      "| epoch : 0 | iteration : 260/600 | train loss : 0.09331658469284385\n",
      "| epoch : 0 | iteration : 270/600 | train loss : 0.03339597027808733\n",
      "| epoch : 0 | iteration : 280/600 | train loss : 0.09920808490471886\n",
      "| epoch : 0 | iteration : 290/600 | train loss : 0.0926940997250087\n",
      "| epoch : 0 | iteration : 300/600 | train loss : 0.03563101996105862\n",
      "| epoch : 0 | iteration : 310/600 | train loss : 0.0978490211601396\n",
      "| epoch : 0 | iteration : 320/600 | train loss : 0.0988660514708938\n",
      "| epoch : 0 | iteration : 330/600 | train loss : 0.059605463819418184\n",
      "| epoch : 0 | iteration : 340/600 | train loss : 0.05190622752084006\n",
      "| epoch : 0 | iteration : 350/600 | train loss : 0.07290711464676472\n",
      "| epoch : 0 | iteration : 360/600 | train loss : 0.0849303679899859\n",
      "| epoch : 0 | iteration : 370/600 | train loss : 0.047010248261871175\n",
      "| epoch : 0 | iteration : 380/600 | train loss : 0.057273450420955004\n",
      "| epoch : 0 | iteration : 390/600 | train loss : 0.08530952796525808\n",
      "| epoch : 0 | iteration : 400/600 | train loss : 0.12649334339285256\n",
      "| epoch : 0 | iteration : 410/600 | train loss : 0.02030493955559064\n",
      "| epoch : 0 | iteration : 420/600 | train loss : 0.10621573813871224\n",
      "| epoch : 0 | iteration : 430/600 | train loss : 0.16717083839186567\n",
      "| epoch : 0 | iteration : 440/600 | train loss : 0.147376874731908\n",
      "| epoch : 0 | iteration : 450/600 | train loss : 0.020360870746133564\n",
      "| epoch : 0 | iteration : 460/600 | train loss : 0.10256140940851723\n",
      "| epoch : 0 | iteration : 470/600 | train loss : 0.07964119214307556\n",
      "| epoch : 0 | iteration : 480/600 | train loss : 0.06877947513799314\n",
      "| epoch : 0 | iteration : 490/600 | train loss : 0.06783925615906203\n",
      "| epoch : 0 | iteration : 500/600 | train loss : 0.052460843697987626\n",
      "| epoch : 0 | iteration : 510/600 | train loss : 0.05427323965505498\n",
      "| epoch : 0 | iteration : 520/600 | train loss : 0.041132250987291716\n",
      "| epoch : 0 | iteration : 530/600 | train loss : 0.05387652174551114\n",
      "| epoch : 0 | iteration : 540/600 | train loss : 0.07044270319346006\n",
      "| epoch : 0 | iteration : 550/600 | train loss : 0.04783240695611432\n",
      "| epoch : 0 | iteration : 560/600 | train loss : 0.12643651287880578\n",
      "| epoch : 0 | iteration : 570/600 | train loss : 0.04243259977509431\n",
      "| epoch : 0 | iteration : 580/600 | train loss : 0.09946032436061966\n",
      "| epoch : 0 | iteration : 590/600 | train loss : 0.09623800869590125\n",
      "| epoch 0 | train_accuracy : 0.9800 | test_accuracy : 0.9790 |\n",
      "| epoch : 1 | iteration : 0/600 | train loss : 0.015343178345584314\n",
      "| epoch : 1 | iteration : 10/600 | train loss : 0.03101245288880018\n",
      "| epoch : 1 | iteration : 20/600 | train loss : 0.0960416986190638\n",
      "| epoch : 1 | iteration : 30/600 | train loss : 0.048531751931985626\n",
      "| epoch : 1 | iteration : 40/600 | train loss : 0.05583594547800282\n",
      "| epoch : 1 | iteration : 50/600 | train loss : 0.03499433447020754\n",
      "| epoch : 1 | iteration : 60/600 | train loss : 0.08464827616432444\n",
      "| epoch : 1 | iteration : 70/600 | train loss : 0.03685628489303116\n",
      "| epoch : 1 | iteration : 80/600 | train loss : 0.01032400555186022\n",
      "| epoch : 1 | iteration : 90/600 | train loss : 0.006882993909360518\n",
      "| epoch : 1 | iteration : 100/600 | train loss : 0.01137424978993904\n",
      "| epoch : 1 | iteration : 110/600 | train loss : 0.10185408837464477\n",
      "| epoch : 1 | iteration : 120/600 | train loss : 0.022883305664247174\n",
      "| epoch : 1 | iteration : 130/600 | train loss : 0.047556022915927414\n",
      "| epoch : 1 | iteration : 140/600 | train loss : 0.029686344121934938\n",
      "| epoch : 1 | iteration : 150/600 | train loss : 0.0475227419444014\n",
      "| epoch : 1 | iteration : 160/600 | train loss : 0.2030351437638638\n",
      "| epoch : 1 | iteration : 170/600 | train loss : 0.02487348713401129\n",
      "| epoch : 1 | iteration : 180/600 | train loss : 0.035552670373612816\n",
      "| epoch : 1 | iteration : 190/600 | train loss : 0.05157243170349732\n",
      "| epoch : 1 | iteration : 200/600 | train loss : 0.1175286301208772\n",
      "| epoch : 1 | iteration : 210/600 | train loss : 0.10423431684309126\n",
      "| epoch : 1 | iteration : 220/600 | train loss : 0.07333185397115298\n",
      "| epoch : 1 | iteration : 230/600 | train loss : 0.013685577000403022\n",
      "| epoch : 1 | iteration : 240/600 | train loss : 0.026673290840421693\n",
      "| epoch : 1 | iteration : 250/600 | train loss : 0.019178350447062207\n",
      "| epoch : 1 | iteration : 260/600 | train loss : 0.009772657187405893\n",
      "| epoch : 1 | iteration : 270/600 | train loss : 0.010944870654984095\n",
      "| epoch : 1 | iteration : 280/600 | train loss : 0.06307143157773715\n",
      "| epoch : 1 | iteration : 290/600 | train loss : 0.04368070483706439\n",
      "| epoch : 1 | iteration : 300/600 | train loss : 0.048049615211670546\n",
      "| epoch : 1 | iteration : 310/600 | train loss : 0.04051421668323662\n",
      "| epoch : 1 | iteration : 320/600 | train loss : 0.011999000429070768\n",
      "| epoch : 1 | iteration : 330/600 | train loss : 0.043065733845821186\n",
      "| epoch : 1 | iteration : 340/600 | train loss : 0.04259025108987626\n",
      "| epoch : 1 | iteration : 350/600 | train loss : 0.03525408390797367\n",
      "| epoch : 1 | iteration : 360/600 | train loss : 0.021939976205376736\n",
      "| epoch : 1 | iteration : 370/600 | train loss : 0.021870549535082714\n",
      "| epoch : 1 | iteration : 380/600 | train loss : 0.09137949842950314\n",
      "| epoch : 1 | iteration : 390/600 | train loss : 0.02351155575602144\n",
      "| epoch : 1 | iteration : 400/600 | train loss : 0.05176077055858357\n",
      "| epoch : 1 | iteration : 410/600 | train loss : 0.06204944801593859\n",
      "| epoch : 1 | iteration : 420/600 | train loss : 0.14347938184143488\n",
      "| epoch : 1 | iteration : 430/600 | train loss : 0.1258315698289146\n",
      "| epoch : 1 | iteration : 440/600 | train loss : 0.05212800940462252\n",
      "| epoch : 1 | iteration : 450/600 | train loss : 0.04884193106187043\n",
      "| epoch : 1 | iteration : 460/600 | train loss : 0.08210771266222917\n",
      "| epoch : 1 | iteration : 470/600 | train loss : 0.034809223649249266\n",
      "| epoch : 1 | iteration : 480/600 | train loss : 0.02227364913255633\n",
      "| epoch : 1 | iteration : 490/600 | train loss : 0.034184192875499085\n",
      "| epoch : 1 | iteration : 500/600 | train loss : 0.048953856839429905\n",
      "| epoch : 1 | iteration : 510/600 | train loss : 0.07182166301548876\n",
      "| epoch : 1 | iteration : 520/600 | train loss : 0.09043260583602448\n",
      "| epoch : 1 | iteration : 530/600 | train loss : 0.08127880481223734\n",
      "| epoch : 1 | iteration : 540/600 | train loss : 0.180333968536544\n",
      "| epoch : 1 | iteration : 550/600 | train loss : 0.03589087568649724\n",
      "| epoch : 1 | iteration : 560/600 | train loss : 0.020335320459528433\n",
      "| epoch : 1 | iteration : 570/600 | train loss : 0.00997983912535353\n",
      "| epoch : 1 | iteration : 580/600 | train loss : 0.057362260570944235\n",
      "| epoch : 1 | iteration : 590/600 | train loss : 0.15080059430857123\n",
      "| epoch 1 | train_accuracy : 0.9700 | test_accuracy : 0.9870 |\n",
      "| epoch : 2 | iteration : 0/600 | train loss : 0.030538628085802676\n",
      "| epoch : 2 | iteration : 10/600 | train loss : 0.07132545695322615\n",
      "| epoch : 2 | iteration : 20/600 | train loss : 0.0036382546727480565\n",
      "| epoch : 2 | iteration : 30/600 | train loss : 0.036100176992981695\n",
      "| epoch : 2 | iteration : 40/600 | train loss : 0.006280050019788097\n",
      "| epoch : 2 | iteration : 50/600 | train loss : 0.0037232058946133125\n",
      "| epoch : 2 | iteration : 60/600 | train loss : 0.02817971866208634\n",
      "| epoch : 2 | iteration : 70/600 | train loss : 0.03528317455666124\n",
      "| epoch : 2 | iteration : 80/600 | train loss : 0.002634720653099425\n",
      "| epoch : 2 | iteration : 90/600 | train loss : 0.04221037569534503\n",
      "| epoch : 2 | iteration : 100/600 | train loss : 0.07999009924464144\n",
      "| epoch : 2 | iteration : 110/600 | train loss : 0.021982276750074674\n",
      "| epoch : 2 | iteration : 120/600 | train loss : 0.015714289412411923\n",
      "| epoch : 2 | iteration : 130/600 | train loss : 0.07814201821870094\n",
      "| epoch : 2 | iteration : 140/600 | train loss : 0.08347447134373269\n",
      "| epoch : 2 | iteration : 150/600 | train loss : 0.02922689900841169\n",
      "| epoch : 2 | iteration : 160/600 | train loss : 0.007697010505154652\n",
      "| epoch : 2 | iteration : 170/600 | train loss : 0.00400616034516673\n",
      "| epoch : 2 | iteration : 180/600 | train loss : 0.022974578543558045\n",
      "| epoch : 2 | iteration : 190/600 | train loss : 0.011703724511704448\n",
      "| epoch : 2 | iteration : 200/600 | train loss : 0.05596596416748149\n",
      "| epoch : 2 | iteration : 210/600 | train loss : 0.07841596318797715\n",
      "| epoch : 2 | iteration : 220/600 | train loss : 0.009414937524245975\n",
      "| epoch : 2 | iteration : 230/600 | train loss : 0.010627131407510808\n",
      "| epoch : 2 | iteration : 240/600 | train loss : 0.0005117317747194768\n",
      "| epoch : 2 | iteration : 250/600 | train loss : 0.04255250500099808\n",
      "| epoch : 2 | iteration : 260/600 | train loss : 0.10086642360897873\n",
      "| epoch : 2 | iteration : 270/600 | train loss : 0.01794415610151152\n",
      "| epoch : 2 | iteration : 280/600 | train loss : 0.032235928915893414\n",
      "| epoch : 2 | iteration : 290/600 | train loss : 0.024617754879804176\n",
      "| epoch : 2 | iteration : 300/600 | train loss : 0.07861078127990444\n",
      "| epoch : 2 | iteration : 310/600 | train loss : 0.002364452939956166\n",
      "| epoch : 2 | iteration : 320/600 | train loss : 0.05207152908974521\n",
      "| epoch : 2 | iteration : 330/600 | train loss : 0.08445942172820255\n",
      "| epoch : 2 | iteration : 340/600 | train loss : 0.028000336643691025\n",
      "| epoch : 2 | iteration : 350/600 | train loss : 0.06729674760316849\n",
      "| epoch : 2 | iteration : 360/600 | train loss : 0.13079560373533208\n",
      "| epoch : 2 | iteration : 370/600 | train loss : 0.03881048441317728\n",
      "| epoch : 2 | iteration : 380/600 | train loss : 0.009836907113942705\n",
      "| epoch : 2 | iteration : 390/600 | train loss : 0.03947889937573319\n",
      "| epoch : 2 | iteration : 400/600 | train loss : 0.010879250412359176\n",
      "| epoch : 2 | iteration : 410/600 | train loss : 0.03673888752607106\n",
      "| epoch : 2 | iteration : 420/600 | train loss : 0.0432640294064483\n",
      "| epoch : 2 | iteration : 430/600 | train loss : 0.02042072586843618\n",
      "| epoch : 2 | iteration : 440/600 | train loss : 0.0022600400823436315\n",
      "| epoch : 2 | iteration : 450/600 | train loss : 0.027181874189316665\n",
      "| epoch : 2 | iteration : 460/600 | train loss : 0.005213165222503388\n",
      "| epoch : 2 | iteration : 470/600 | train loss : 0.035458931720398754\n",
      "| epoch : 2 | iteration : 480/600 | train loss : 0.01851042013978765\n",
      "| epoch : 2 | iteration : 490/600 | train loss : 0.02929671271915884\n",
      "| epoch : 2 | iteration : 500/600 | train loss : 0.018057264380193692\n",
      "| epoch : 2 | iteration : 510/600 | train loss : 0.036052624250957115\n",
      "| epoch : 2 | iteration : 520/600 | train loss : 0.07687590018347705\n",
      "| epoch : 2 | iteration : 530/600 | train loss : 0.06778692952863598\n",
      "| epoch : 2 | iteration : 540/600 | train loss : 0.0065235072213514714\n",
      "| epoch : 2 | iteration : 550/600 | train loss : 0.03167782668741567\n",
      "| epoch : 2 | iteration : 560/600 | train loss : 0.061440612241107646\n",
      "| epoch : 2 | iteration : 570/600 | train loss : 0.01723781249883211\n",
      "| epoch : 2 | iteration : 580/600 | train loss : 0.004853426264963739\n",
      "| epoch : 2 | iteration : 590/600 | train loss : 0.10442680044108067\n",
      "| epoch 2 | train_accuracy : 0.9910 | test_accuracy : 0.9850 |\n",
      "| epoch : 3 | iteration : 0/600 | train loss : 0.031056715316576215\n",
      "| epoch : 3 | iteration : 10/600 | train loss : 0.004027632483713147\n",
      "| epoch : 3 | iteration : 20/600 | train loss : 0.0963069611011167\n",
      "| epoch : 3 | iteration : 30/600 | train loss : 0.0226250238845801\n",
      "| epoch : 3 | iteration : 40/600 | train loss : 0.0038593552459497064\n",
      "| epoch : 3 | iteration : 50/600 | train loss : 0.017221606976257946\n",
      "| epoch : 3 | iteration : 60/600 | train loss : 0.038003456209923576\n",
      "| epoch : 3 | iteration : 70/600 | train loss : 0.03805941558669022\n",
      "| epoch : 3 | iteration : 80/600 | train loss : 0.06269948044724996\n",
      "| epoch : 3 | iteration : 90/600 | train loss : 0.0057593066943658665\n",
      "| epoch : 3 | iteration : 100/600 | train loss : 0.009979312270618934\n",
      "| epoch : 3 | iteration : 110/600 | train loss : 0.0027878354466666835\n",
      "| epoch : 3 | iteration : 120/600 | train loss : 0.017221465904944545\n",
      "| epoch : 3 | iteration : 130/600 | train loss : 0.00788689298474428\n",
      "| epoch : 3 | iteration : 140/600 | train loss : 0.005880760143845398\n",
      "| epoch : 3 | iteration : 150/600 | train loss : 0.028487039457709903\n",
      "| epoch : 3 | iteration : 160/600 | train loss : 0.005057244559647428\n",
      "| epoch : 3 | iteration : 170/600 | train loss : 0.03428504382910352\n",
      "| epoch : 3 | iteration : 180/600 | train loss : 0.07760440381934428\n",
      "| epoch : 3 | iteration : 190/600 | train loss : 0.0892144728417257\n",
      "| epoch : 3 | iteration : 200/600 | train loss : 0.027038774292024264\n",
      "| epoch : 3 | iteration : 210/600 | train loss : 0.09679487751459342\n",
      "| epoch : 3 | iteration : 220/600 | train loss : 0.040976250771803266\n",
      "| epoch : 3 | iteration : 230/600 | train loss : 0.0034001122797998784\n",
      "| epoch : 3 | iteration : 240/600 | train loss : 0.000177116426045413\n",
      "| epoch : 3 | iteration : 250/600 | train loss : 0.022795326548885865\n",
      "| epoch : 3 | iteration : 260/600 | train loss : 0.04408512523785025\n",
      "| epoch : 3 | iteration : 270/600 | train loss : 0.026115311869590322\n",
      "| epoch : 3 | iteration : 280/600 | train loss : 0.009553610412355472\n",
      "| epoch : 3 | iteration : 290/600 | train loss : 0.06475274290329874\n",
      "| epoch : 3 | iteration : 300/600 | train loss : 0.006925402179285884\n",
      "| epoch : 3 | iteration : 310/600 | train loss : 0.011511273315453756\n",
      "| epoch : 3 | iteration : 320/600 | train loss : 0.012403755418559557\n",
      "| epoch : 3 | iteration : 330/600 | train loss : 0.13587709785913948\n",
      "| epoch : 3 | iteration : 340/600 | train loss : 0.006825158514150328\n",
      "| epoch : 3 | iteration : 350/600 | train loss : 0.001980215344329637\n",
      "| epoch : 3 | iteration : 360/600 | train loss : 0.022091613569327855\n",
      "| epoch : 3 | iteration : 370/600 | train loss : 0.0038147714155635087\n",
      "| epoch : 3 | iteration : 380/600 | train loss : 0.009028629025558847\n",
      "| epoch : 3 | iteration : 390/600 | train loss : 0.10813921139174151\n",
      "| epoch : 3 | iteration : 400/600 | train loss : 0.08335235586998486\n",
      "| epoch : 3 | iteration : 410/600 | train loss : 0.008921509381503203\n",
      "| epoch : 3 | iteration : 420/600 | train loss : 0.0116451246000505\n",
      "| epoch : 3 | iteration : 430/600 | train loss : 0.021292657488945554\n",
      "| epoch : 3 | iteration : 440/600 | train loss : 0.02189206087063524\n",
      "| epoch : 3 | iteration : 450/600 | train loss : 0.04090184253511218\n",
      "| epoch : 3 | iteration : 460/600 | train loss : 0.028064467508852168\n",
      "| epoch : 3 | iteration : 470/600 | train loss : 0.014436579957096428\n",
      "| epoch : 3 | iteration : 480/600 | train loss : 0.04326772059517366\n",
      "| epoch : 3 | iteration : 490/600 | train loss : 0.023762286702024335\n",
      "| epoch : 3 | iteration : 500/600 | train loss : 0.0697717435778384\n",
      "| epoch : 3 | iteration : 510/600 | train loss : 0.010135824502282322\n",
      "| epoch : 3 | iteration : 520/600 | train loss : 0.03206168746808514\n",
      "| epoch : 3 | iteration : 530/600 | train loss : 0.005929839723138039\n",
      "| epoch : 3 | iteration : 540/600 | train loss : 0.007800136483045778\n",
      "| epoch : 3 | iteration : 550/600 | train loss : 0.008659278094256224\n",
      "| epoch : 3 | iteration : 560/600 | train loss : 0.08073321651230794\n",
      "| epoch : 3 | iteration : 570/600 | train loss : 0.024690634016696383\n",
      "| epoch : 3 | iteration : 580/600 | train loss : 0.04472587592636172\n",
      "| epoch : 3 | iteration : 590/600 | train loss : 0.06659229382928299\n",
      "| epoch 3 | train_accuracy : 0.9970 | test_accuracy : 0.9830 |\n",
      "| epoch : 4 | iteration : 0/600 | train loss : 0.05233156049891841\n",
      "| epoch : 4 | iteration : 10/600 | train loss : 0.004831872412556537\n",
      "| epoch : 4 | iteration : 20/600 | train loss : 0.0028613392431889244\n",
      "| epoch : 4 | iteration : 30/600 | train loss : 0.005000208470870786\n",
      "| epoch : 4 | iteration : 40/600 | train loss : 0.002859051711955807\n",
      "| epoch : 4 | iteration : 50/600 | train loss : 0.004605506769774809\n",
      "| epoch : 4 | iteration : 60/600 | train loss : 0.00015078295775624144\n",
      "| epoch : 4 | iteration : 70/600 | train loss : 0.016681420860647757\n",
      "| epoch : 4 | iteration : 80/600 | train loss : 0.021678934534036274\n",
      "| epoch : 4 | iteration : 90/600 | train loss : 0.000361864283787746\n",
      "| epoch : 4 | iteration : 100/600 | train loss : 0.0007728008562174689\n",
      "| epoch : 4 | iteration : 110/600 | train loss : 0.013055741559339959\n",
      "| epoch : 4 | iteration : 120/600 | train loss : 0.0652750776943577\n",
      "| epoch : 4 | iteration : 130/600 | train loss : 0.0006239521310951214\n",
      "| epoch : 4 | iteration : 140/600 | train loss : 0.029719308133941535\n",
      "| epoch : 4 | iteration : 150/600 | train loss : 0.028552870444522746\n",
      "| epoch : 4 | iteration : 160/600 | train loss : 0.03709782957208331\n",
      "| epoch : 4 | iteration : 170/600 | train loss : 0.08270027041094942\n",
      "| epoch : 4 | iteration : 180/600 | train loss : 0.001097695589635086\n",
      "| epoch : 4 | iteration : 190/600 | train loss : 0.10990009840415854\n",
      "| epoch : 4 | iteration : 200/600 | train loss : 0.007284306643656071\n",
      "| epoch : 4 | iteration : 210/600 | train loss : 0.0005053755917803232\n",
      "| epoch : 4 | iteration : 220/600 | train loss : 0.004648041829029838\n",
      "| epoch : 4 | iteration : 230/600 | train loss : 0.0010867481810927795\n",
      "| epoch : 4 | iteration : 240/600 | train loss : 0.031113569704013862\n",
      "| epoch : 4 | iteration : 250/600 | train loss : 0.041459077178146\n",
      "| epoch : 4 | iteration : 260/600 | train loss : 0.028058458785919383\n",
      "| epoch : 4 | iteration : 270/600 | train loss : 0.05126843929158037\n",
      "| epoch : 4 | iteration : 280/600 | train loss : 0.03978591568285399\n",
      "| epoch : 4 | iteration : 290/600 | train loss : 0.02733764130704599\n",
      "| epoch : 4 | iteration : 300/600 | train loss : 0.015841302735997227\n",
      "| epoch : 4 | iteration : 310/600 | train loss : 0.0031148046917388746\n",
      "| epoch : 4 | iteration : 320/600 | train loss : 0.02145416559687487\n",
      "| epoch : 4 | iteration : 330/600 | train loss : 0.024988195141115978\n",
      "| epoch : 4 | iteration : 340/600 | train loss : 0.004442756338338349\n",
      "| epoch : 4 | iteration : 350/600 | train loss : 0.008025412065848073\n",
      "| epoch : 4 | iteration : 360/600 | train loss : 0.002730610450923646\n",
      "| epoch : 4 | iteration : 370/600 | train loss : 0.005157070810684427\n",
      "| epoch : 4 | iteration : 380/600 | train loss : 0.08099836541584646\n",
      "| epoch : 4 | iteration : 390/600 | train loss : 0.06296059828480298\n",
      "| epoch : 4 | iteration : 400/600 | train loss : 0.0181508004377999\n",
      "| epoch : 4 | iteration : 410/600 | train loss : 0.013664699208981036\n",
      "| epoch : 4 | iteration : 420/600 | train loss : 0.010059276503099348\n",
      "| epoch : 4 | iteration : 430/600 | train loss : 0.012982212159144609\n",
      "| epoch : 4 | iteration : 440/600 | train loss : 0.004513898110205633\n",
      "| epoch : 4 | iteration : 450/600 | train loss : 0.05280788772110061\n",
      "| epoch : 4 | iteration : 460/600 | train loss : 0.008402523867779803\n",
      "| epoch : 4 | iteration : 470/600 | train loss : 0.0019573093405487667\n",
      "| epoch : 4 | iteration : 480/600 | train loss : 0.017015759799554723\n",
      "| epoch : 4 | iteration : 490/600 | train loss : 0.008443440635241864\n",
      "| epoch : 4 | iteration : 500/600 | train loss : 0.021274768873027216\n",
      "| epoch : 4 | iteration : 510/600 | train loss : 0.07092396264136414\n",
      "| epoch : 4 | iteration : 520/600 | train loss : 0.055236649060183544\n",
      "| epoch : 4 | iteration : 530/600 | train loss : 0.006905815718354861\n",
      "| epoch : 4 | iteration : 540/600 | train loss : 0.10724466999180227\n",
      "| epoch : 4 | iteration : 550/600 | train loss : 0.013826321940778635\n",
      "| epoch : 4 | iteration : 560/600 | train loss : 0.007681133070223143\n",
      "| epoch : 4 | iteration : 570/600 | train loss : 0.17655472431102873\n",
      "| epoch : 4 | iteration : 580/600 | train loss : 0.15222775835724608\n",
      "| epoch : 4 | iteration : 590/600 | train loss : 0.04146481922220604\n",
      "| epoch 4 | train_accuracy : 0.9930 | test_accuracy : 0.9820 |\n",
      "| epoch : 5 | iteration : 0/600 | train loss : 0.0504793637833008\n",
      "| epoch : 5 | iteration : 10/600 | train loss : 0.0476163371616645\n",
      "| epoch : 5 | iteration : 20/600 | train loss : 0.013267298559654835\n",
      "| epoch : 5 | iteration : 30/600 | train loss : 0.06734981034663848\n",
      "| epoch : 5 | iteration : 40/600 | train loss : 0.0005716924521778515\n",
      "| epoch : 5 | iteration : 50/600 | train loss : 0.017174333092787847\n",
      "| epoch : 5 | iteration : 60/600 | train loss : 0.0016606541707802677\n",
      "| epoch : 5 | iteration : 70/600 | train loss : 0.0019064968919914305\n",
      "| epoch : 5 | iteration : 80/600 | train loss : 0.03898734274791209\n",
      "| epoch : 5 | iteration : 90/600 | train loss : 0.054346494836341776\n",
      "| epoch : 5 | iteration : 100/600 | train loss : 0.0966475727917988\n",
      "| epoch : 5 | iteration : 110/600 | train loss : 0.0034365904140655373\n",
      "| epoch : 5 | iteration : 120/600 | train loss : 0.008172703858114498\n",
      "| epoch : 5 | iteration : 130/600 | train loss : 0.002470686177703553\n",
      "| epoch : 5 | iteration : 140/600 | train loss : 0.004109170514247723\n",
      "| epoch : 5 | iteration : 150/600 | train loss : 0.0008657466714444524\n",
      "| epoch : 5 | iteration : 160/600 | train loss : 0.0008946441718866538\n",
      "| epoch : 5 | iteration : 170/600 | train loss : 0.00045586631181826926\n",
      "| epoch : 5 | iteration : 180/600 | train loss : 0.07821440304644628\n",
      "| epoch : 5 | iteration : 190/600 | train loss : 0.05391588755016381\n",
      "| epoch : 5 | iteration : 200/600 | train loss : 0.01626058414042998\n",
      "| epoch : 5 | iteration : 210/600 | train loss : 0.0013737780013280629\n",
      "| epoch : 5 | iteration : 220/600 | train loss : 0.0056140001762503625\n",
      "| epoch : 5 | iteration : 230/600 | train loss : 0.03721548805615096\n",
      "| epoch : 5 | iteration : 240/600 | train loss : 0.0003083901364203817\n",
      "| epoch : 5 | iteration : 250/600 | train loss : 0.00117279967577858\n",
      "| epoch : 5 | iteration : 260/600 | train loss : 0.010315140033758672\n",
      "| epoch : 5 | iteration : 270/600 | train loss : 0.0012004937271302735\n",
      "| epoch : 5 | iteration : 280/600 | train loss : 0.07512943300714615\n",
      "| epoch : 5 | iteration : 290/600 | train loss : 0.22974439724126408\n",
      "| epoch : 5 | iteration : 300/600 | train loss : 0.10029126572003479\n",
      "| epoch : 5 | iteration : 310/600 | train loss : 0.0006876431659602808\n",
      "| epoch : 5 | iteration : 320/600 | train loss : 0.01407427346817102\n",
      "| epoch : 5 | iteration : 330/600 | train loss : 0.00910313028167065\n",
      "| epoch : 5 | iteration : 340/600 | train loss : 0.013072351574243763\n",
      "| epoch : 5 | iteration : 350/600 | train loss : 0.08568721027878701\n",
      "| epoch : 5 | iteration : 360/600 | train loss : 0.01618096579775965\n",
      "| epoch : 5 | iteration : 370/600 | train loss : 0.166270360650799\n",
      "| epoch : 5 | iteration : 380/600 | train loss : 0.045529223759249664\n",
      "| epoch : 5 | iteration : 390/600 | train loss : 0.03395167722623743\n",
      "| epoch : 5 | iteration : 400/600 | train loss : 0.05167560747236195\n",
      "| epoch : 5 | iteration : 410/600 | train loss : 0.012431081512948514\n",
      "| epoch : 5 | iteration : 420/600 | train loss : 0.026879195264860285\n",
      "| epoch : 5 | iteration : 430/600 | train loss : 0.0697521329275468\n",
      "| epoch : 5 | iteration : 440/600 | train loss : 0.012178247634531849\n",
      "| epoch : 5 | iteration : 450/600 | train loss : 0.13432890686221208\n",
      "| epoch : 5 | iteration : 460/600 | train loss : 0.07475777194445107\n",
      "| epoch : 5 | iteration : 470/600 | train loss : 0.011400902031248961\n",
      "| epoch : 5 | iteration : 480/600 | train loss : 0.0024120297374409677\n",
      "| epoch : 5 | iteration : 490/600 | train loss : 0.0030205436912951905\n",
      "| epoch : 5 | iteration : 500/600 | train loss : 0.05367232363917061\n",
      "| epoch : 5 | iteration : 510/600 | train loss : 0.16034474901401297\n",
      "| epoch : 5 | iteration : 520/600 | train loss : 0.06264615916398522\n",
      "| epoch : 5 | iteration : 530/600 | train loss : 0.0077714708847018065\n",
      "| epoch : 5 | iteration : 540/600 | train loss : 0.11267730233928995\n",
      "| epoch : 5 | iteration : 550/600 | train loss : 0.0031645788566144668\n",
      "| epoch : 5 | iteration : 560/600 | train loss : 0.06794254039363087\n",
      "| epoch : 5 | iteration : 570/600 | train loss : 0.09263763055384217\n",
      "| epoch : 5 | iteration : 580/600 | train loss : 0.001442661239539583\n",
      "| epoch : 5 | iteration : 590/600 | train loss : 0.0699004689900065\n",
      "| epoch 5 | train_accuracy : 0.9950 | test_accuracy : 0.9820 |\n",
      "| epoch : 6 | iteration : 0/600 | train loss : 0.01238347382421825\n",
      "| epoch : 6 | iteration : 10/600 | train loss : 0.013207019232373622\n",
      "| epoch : 6 | iteration : 20/600 | train loss : 0.016718162663428085\n",
      "| epoch : 6 | iteration : 30/600 | train loss : 0.024844222169599153\n",
      "| epoch : 6 | iteration : 40/600 | train loss : 0.04183458409211884\n",
      "| epoch : 6 | iteration : 50/600 | train loss : 0.003534765365597908\n",
      "| epoch : 6 | iteration : 60/600 | train loss : 0.0019160734669621715\n",
      "| epoch : 6 | iteration : 70/600 | train loss : 0.0009905274688177294\n",
      "| epoch : 6 | iteration : 80/600 | train loss : 0.07495240965275492\n",
      "| epoch : 6 | iteration : 90/600 | train loss : 0.03392034813258015\n",
      "| epoch : 6 | iteration : 100/600 | train loss : 0.0061379845234641696\n",
      "| epoch : 6 | iteration : 110/600 | train loss : 0.00022183771788760357\n",
      "| epoch : 6 | iteration : 120/600 | train loss : 0.011498525724245402\n",
      "| epoch : 6 | iteration : 130/600 | train loss : 0.00018206342660460277\n",
      "| epoch : 6 | iteration : 140/600 | train loss : 0.005967467818686764\n",
      "| epoch : 6 | iteration : 150/600 | train loss : 0.0024418130361342698\n",
      "| epoch : 6 | iteration : 160/600 | train loss : 0.08743575300248299\n",
      "| epoch : 6 | iteration : 170/600 | train loss : 0.00033683849583820947\n",
      "| epoch : 6 | iteration : 180/600 | train loss : 0.0008641493726925463\n",
      "| epoch : 6 | iteration : 190/600 | train loss : 0.00019993826945945256\n",
      "| epoch : 6 | iteration : 200/600 | train loss : 0.019110484133197922\n",
      "| epoch : 6 | iteration : 210/600 | train loss : 0.0013198270198853225\n",
      "| epoch : 6 | iteration : 220/600 | train loss : 0.18807615407330158\n",
      "| epoch : 6 | iteration : 230/600 | train loss : 0.03806004961897129\n",
      "| epoch : 6 | iteration : 240/600 | train loss : 0.00486554652290567\n",
      "| epoch : 6 | iteration : 250/600 | train loss : 0.01241039394890534\n",
      "| epoch : 6 | iteration : 260/600 | train loss : 0.018242433997316173\n",
      "| epoch : 6 | iteration : 270/600 | train loss : 7.244751306855644e-05\n",
      "| epoch : 6 | iteration : 280/600 | train loss : 0.06981055562151106\n",
      "| epoch : 6 | iteration : 290/600 | train loss : 0.09063925455421362\n",
      "| epoch : 6 | iteration : 300/600 | train loss : 0.006298789039154129\n",
      "| epoch : 6 | iteration : 310/600 | train loss : 0.013613051778892448\n",
      "| epoch : 6 | iteration : 320/600 | train loss : 0.19974620204845217\n",
      "| epoch : 6 | iteration : 330/600 | train loss : 0.09463133341537923\n",
      "| epoch : 6 | iteration : 340/600 | train loss : 0.00048015689024130984\n",
      "| epoch : 6 | iteration : 350/600 | train loss : 0.010498457928479827\n",
      "| epoch : 6 | iteration : 360/600 | train loss : 0.02675049492520758\n",
      "| epoch : 6 | iteration : 370/600 | train loss : 0.009867137222547473\n",
      "| epoch : 6 | iteration : 380/600 | train loss : 0.1794242188848369\n",
      "| epoch : 6 | iteration : 390/600 | train loss : 0.0035901935961329973\n",
      "| epoch : 6 | iteration : 400/600 | train loss : 0.002136295823134838\n",
      "| epoch : 6 | iteration : 410/600 | train loss : 0.0007736692907135921\n",
      "| epoch : 6 | iteration : 420/600 | train loss : 0.001839060384545313\n",
      "| epoch : 6 | iteration : 430/600 | train loss : 0.09493462205764552\n",
      "| epoch : 6 | iteration : 440/600 | train loss : 0.0024043785078421373\n",
      "| epoch : 6 | iteration : 450/600 | train loss : 0.006006122611051941\n",
      "| epoch : 6 | iteration : 460/600 | train loss : 0.01219354454026258\n",
      "| epoch : 6 | iteration : 470/600 | train loss : 0.001149032769290022\n",
      "| epoch : 6 | iteration : 480/600 | train loss : 0.0037075286650246142\n",
      "| epoch : 6 | iteration : 490/600 | train loss : 0.0010075661105303794\n",
      "| epoch : 6 | iteration : 500/600 | train loss : 0.0002745943585958589\n",
      "| epoch : 6 | iteration : 510/600 | train loss : 0.1405627941558379\n",
      "| epoch : 6 | iteration : 520/600 | train loss : 0.017124594557739466\n",
      "| epoch : 6 | iteration : 530/600 | train loss : 0.0005852145452571959\n",
      "| epoch : 6 | iteration : 540/600 | train loss : 0.09091547373505827\n",
      "| epoch : 6 | iteration : 550/600 | train loss : 0.027902123717148203\n",
      "| epoch : 6 | iteration : 560/600 | train loss : 0.036171406700743314\n",
      "| epoch : 6 | iteration : 570/600 | train loss : 0.0021433262797062556\n",
      "| epoch : 6 | iteration : 580/600 | train loss : 0.002786336960989716\n",
      "| epoch : 6 | iteration : 590/600 | train loss : 0.0009222403683652534\n",
      "| epoch 6 | train_accuracy : 0.9920 | test_accuracy : 0.9830 |\n",
      "| epoch : 7 | iteration : 0/600 | train loss : 0.05981000567678704\n",
      "| epoch : 7 | iteration : 10/600 | train loss : 0.007206198624639884\n",
      "| epoch : 7 | iteration : 20/600 | train loss : 0.028452226211307042\n",
      "| epoch : 7 | iteration : 30/600 | train loss : 0.005088638272296919\n",
      "| epoch : 7 | iteration : 40/600 | train loss : 0.033630052228522035\n",
      "| epoch : 7 | iteration : 50/600 | train loss : 0.00936576141250228\n",
      "| epoch : 7 | iteration : 60/600 | train loss : 0.0030765855144748388\n",
      "| epoch : 7 | iteration : 70/600 | train loss : 0.0072430997913526555\n",
      "| epoch : 7 | iteration : 80/600 | train loss : 0.0004163276584638918\n",
      "| epoch : 7 | iteration : 90/600 | train loss : 0.0059003677229949045\n",
      "| epoch : 7 | iteration : 100/600 | train loss : 0.0010458010994072711\n",
      "| epoch : 7 | iteration : 110/600 | train loss : 0.13148985382949216\n",
      "| epoch : 7 | iteration : 120/600 | train loss : 0.0014926090807908376\n",
      "| epoch : 7 | iteration : 130/600 | train loss : 0.024159654341348986\n",
      "| epoch : 7 | iteration : 140/600 | train loss : 0.03454513267809698\n",
      "| epoch : 7 | iteration : 150/600 | train loss : 0.05651094734797659\n",
      "| epoch : 7 | iteration : 160/600 | train loss : 0.028427450400860078\n",
      "| epoch : 7 | iteration : 170/600 | train loss : 0.007016992729778795\n",
      "| epoch : 7 | iteration : 180/600 | train loss : 0.09494142320814354\n",
      "| epoch : 7 | iteration : 190/600 | train loss : 0.004419636293985209\n",
      "| epoch : 7 | iteration : 200/600 | train loss : 0.00632569111597301\n",
      "| epoch : 7 | iteration : 210/600 | train loss : 0.018230662668867956\n",
      "| epoch : 7 | iteration : 220/600 | train loss : 0.0014418735045398376\n",
      "| epoch : 7 | iteration : 230/600 | train loss : 0.00016985924653531093\n",
      "| epoch : 7 | iteration : 240/600 | train loss : 0.04995728097930694\n",
      "| epoch : 7 | iteration : 250/600 | train loss : 0.0013096547923060276\n",
      "| epoch : 7 | iteration : 260/600 | train loss : 0.06130051493589167\n",
      "| epoch : 7 | iteration : 270/600 | train loss : 0.0029111095541133093\n",
      "| epoch : 7 | iteration : 280/600 | train loss : 0.00037123036953919304\n",
      "| epoch : 7 | iteration : 290/600 | train loss : 0.0005020487325200759\n",
      "| epoch : 7 | iteration : 300/600 | train loss : 0.31757527065023494\n",
      "| epoch : 7 | iteration : 310/600 | train loss : 0.05658603205015595\n",
      "| epoch : 7 | iteration : 320/600 | train loss : 0.12052725722598875\n",
      "| epoch : 7 | iteration : 330/600 | train loss : 0.047499416309919044\n",
      "| epoch : 7 | iteration : 340/600 | train loss : 0.0016275343659959735\n",
      "| epoch : 7 | iteration : 350/600 | train loss : 0.019747291384404887\n",
      "| epoch : 7 | iteration : 360/600 | train loss : 0.042946193912134986\n",
      "| epoch : 7 | iteration : 370/600 | train loss : 0.02536322311629806\n",
      "| epoch : 7 | iteration : 380/600 | train loss : 0.07559279141336588\n",
      "| epoch : 7 | iteration : 390/600 | train loss : 0.0031554957533568863\n",
      "| epoch : 7 | iteration : 400/600 | train loss : 0.0008473069465070096\n",
      "| epoch : 7 | iteration : 410/600 | train loss : 0.10494525073710456\n",
      "| epoch : 7 | iteration : 420/600 | train loss : 0.04447555844632209\n",
      "| epoch : 7 | iteration : 430/600 | train loss : 0.006675342814922464\n",
      "| epoch : 7 | iteration : 440/600 | train loss : 0.06679951302383544\n",
      "| epoch : 7 | iteration : 450/600 | train loss : 0.05545280849310022\n",
      "| epoch : 7 | iteration : 460/600 | train loss : 0.0003385842434178423\n",
      "| epoch : 7 | iteration : 470/600 | train loss : 0.00017044016945166364\n",
      "| epoch : 7 | iteration : 480/600 | train loss : 0.0005011531464749983\n",
      "| epoch : 7 | iteration : 490/600 | train loss : 0.05607755550535917\n",
      "| epoch : 7 | iteration : 500/600 | train loss : 0.005442011065002635\n",
      "| epoch : 7 | iteration : 510/600 | train loss : 0.0019786778969346293\n",
      "| epoch : 7 | iteration : 520/600 | train loss : 0.0006103491631623552\n",
      "| epoch : 7 | iteration : 530/600 | train loss : 0.018798107017369378\n",
      "| epoch : 7 | iteration : 540/600 | train loss : 0.021891044774868668\n",
      "| epoch : 7 | iteration : 550/600 | train loss : 0.00048648381079951787\n",
      "| epoch : 7 | iteration : 560/600 | train loss : 0.04337562846726624\n",
      "| epoch : 7 | iteration : 570/600 | train loss : 0.025673649023813833\n",
      "| epoch : 7 | iteration : 580/600 | train loss : 0.0005115450692718656\n",
      "| epoch : 7 | iteration : 590/600 | train loss : 0.0027047099391505846\n",
      "| epoch 7 | train_accuracy : 0.9970 | test_accuracy : 0.9790 |\n",
      "| epoch : 8 | iteration : 0/600 | train loss : 0.003014792446818357\n",
      "| epoch : 8 | iteration : 10/600 | train loss : 0.0020464578862534376\n",
      "| epoch : 8 | iteration : 20/600 | train loss : 0.001463315606245975\n",
      "| epoch : 8 | iteration : 30/600 | train loss : 0.01677731279476633\n",
      "| epoch : 8 | iteration : 40/600 | train loss : 0.0011465193979455942\n",
      "| epoch : 8 | iteration : 50/600 | train loss : 0.0009690757619725588\n",
      "| epoch : 8 | iteration : 60/600 | train loss : 0.00765966882134677\n",
      "| epoch : 8 | iteration : 70/600 | train loss : 0.012701490764150199\n",
      "| epoch : 8 | iteration : 80/600 | train loss : 0.0005942674914955845\n",
      "| epoch : 8 | iteration : 90/600 | train loss : 0.03465935072873803\n",
      "| epoch : 8 | iteration : 100/600 | train loss : 0.002859026146035648\n",
      "| epoch : 8 | iteration : 110/600 | train loss : 0.1332906213596987\n",
      "| epoch : 8 | iteration : 120/600 | train loss : 0.018899385641258714\n",
      "| epoch : 8 | iteration : 130/600 | train loss : 0.030103629458932392\n",
      "| epoch : 8 | iteration : 140/600 | train loss : 4.564676747765286e-05\n",
      "| epoch : 8 | iteration : 150/600 | train loss : 0.0002030929841499708\n",
      "| epoch : 8 | iteration : 160/600 | train loss : 0.013972294793620905\n",
      "| epoch : 8 | iteration : 170/600 | train loss : 0.011624550207820758\n",
      "| epoch : 8 | iteration : 180/600 | train loss : 0.00042763223418299786\n",
      "| epoch : 8 | iteration : 190/600 | train loss : 0.0017876111739285557\n",
      "| epoch : 8 | iteration : 200/600 | train loss : 7.25929173993858e-05\n",
      "| epoch : 8 | iteration : 210/600 | train loss : 3.129364687060059e-05\n",
      "| epoch : 8 | iteration : 220/600 | train loss : 0.005070846109981388\n",
      "| epoch : 8 | iteration : 230/600 | train loss : 0.0021643650067220267\n",
      "| epoch : 8 | iteration : 240/600 | train loss : 0.0003903031881542097\n",
      "| epoch : 8 | iteration : 250/600 | train loss : 0.047009777924629875\n",
      "| epoch : 8 | iteration : 260/600 | train loss : 0.1764801906798006\n",
      "| epoch : 8 | iteration : 270/600 | train loss : 0.0783244246134035\n",
      "| epoch : 8 | iteration : 280/600 | train loss : 0.08445050248952812\n",
      "| epoch : 8 | iteration : 290/600 | train loss : 0.010537591524196255\n",
      "| epoch : 8 | iteration : 300/600 | train loss : 0.006409481359817636\n",
      "| epoch : 8 | iteration : 310/600 | train loss : 0.1311070993423478\n",
      "| epoch : 8 | iteration : 320/600 | train loss : 0.048160088376485735\n",
      "| epoch : 8 | iteration : 330/600 | train loss : 0.016504961090434013\n",
      "| epoch : 8 | iteration : 340/600 | train loss : 0.00041822804024569673\n",
      "| epoch : 8 | iteration : 350/600 | train loss : 0.010925485898539708\n",
      "| epoch : 8 | iteration : 360/600 | train loss : 0.009950134929014516\n",
      "| epoch : 8 | iteration : 370/600 | train loss : 0.017808176477664237\n",
      "| epoch : 8 | iteration : 380/600 | train loss : 0.028077120202262648\n",
      "| epoch : 8 | iteration : 390/600 | train loss : 0.005605712799066926\n",
      "| epoch : 8 | iteration : 400/600 | train loss : 0.05416199318148141\n",
      "| epoch : 8 | iteration : 410/600 | train loss : 0.014926085389514137\n",
      "| epoch : 8 | iteration : 420/600 | train loss : 0.008221468776717438\n",
      "| epoch : 8 | iteration : 430/600 | train loss : 0.0029994803164220102\n",
      "| epoch : 8 | iteration : 440/600 | train loss : 0.005601948418292383\n",
      "| epoch : 8 | iteration : 450/600 | train loss : 7.357890761838506e-05\n",
      "| epoch : 8 | iteration : 460/600 | train loss : 0.0019403895064003013\n",
      "| epoch : 8 | iteration : 470/600 | train loss : 0.000469069376559636\n",
      "| epoch : 8 | iteration : 480/600 | train loss : 0.09972931597729713\n",
      "| epoch : 8 | iteration : 490/600 | train loss : 0.0022144912664401064\n",
      "| epoch : 8 | iteration : 500/600 | train loss : 0.09184831838953653\n",
      "| epoch : 8 | iteration : 510/600 | train loss : 0.004967732015793943\n",
      "| epoch : 8 | iteration : 520/600 | train loss : 0.13377317829454452\n",
      "| epoch : 8 | iteration : 530/600 | train loss : 3.77317681194312e-05\n",
      "| epoch : 8 | iteration : 540/600 | train loss : 0.007557703899491755\n",
      "| epoch : 8 | iteration : 550/600 | train loss : 0.009761568822341849\n",
      "| epoch : 8 | iteration : 560/600 | train loss : 0.08570712678729002\n",
      "| epoch : 8 | iteration : 570/600 | train loss : 0.02100764797817139\n",
      "| epoch : 8 | iteration : 580/600 | train loss : 0.001123491773133101\n",
      "| epoch : 8 | iteration : 590/600 | train loss : 0.0004103291556743327\n",
      "| epoch 8 | train_accuracy : 0.9930 | test_accuracy : 0.9860 |\n",
      "| epoch : 9 | iteration : 0/600 | train loss : 8.853060385536997e-06\n",
      "| epoch : 9 | iteration : 10/600 | train loss : 0.009386537264849967\n",
      "| epoch : 9 | iteration : 20/600 | train loss : 0.06585409890173712\n",
      "| epoch : 9 | iteration : 30/600 | train loss : 0.014497938761142897\n",
      "| epoch : 9 | iteration : 40/600 | train loss : 0.056707564278868354\n",
      "| epoch : 9 | iteration : 50/600 | train loss : 0.00040979909831376845\n",
      "| epoch : 9 | iteration : 60/600 | train loss : 0.00023130012817673768\n",
      "| epoch : 9 | iteration : 70/600 | train loss : 0.001527075153904711\n",
      "| epoch : 9 | iteration : 80/600 | train loss : 0.016973499321096604\n",
      "| epoch : 9 | iteration : 90/600 | train loss : 0.09992593746256376\n",
      "| epoch : 9 | iteration : 100/600 | train loss : 0.000142148902176176\n",
      "| epoch : 9 | iteration : 110/600 | train loss : 0.07825106339682852\n",
      "| epoch : 9 | iteration : 120/600 | train loss : 0.06519386251599896\n",
      "| epoch : 9 | iteration : 130/600 | train loss : 0.0068177975399011746\n",
      "| epoch : 9 | iteration : 140/600 | train loss : 0.001499133550946764\n",
      "| epoch : 9 | iteration : 150/600 | train loss : 0.11439802395078238\n",
      "| epoch : 9 | iteration : 160/600 | train loss : 0.03360988514312255\n",
      "| epoch : 9 | iteration : 170/600 | train loss : 8.391246671934952e-05\n",
      "| epoch : 9 | iteration : 180/600 | train loss : 0.018839946978790843\n",
      "| epoch : 9 | iteration : 190/600 | train loss : 0.02473118538840905\n",
      "| epoch : 9 | iteration : 200/600 | train loss : 0.012401509104472299\n",
      "| epoch : 9 | iteration : 210/600 | train loss : 0.09207052016732187\n",
      "| epoch : 9 | iteration : 220/600 | train loss : 0.06311641247038673\n",
      "| epoch : 9 | iteration : 230/600 | train loss : 0.10243884571676092\n",
      "| epoch : 9 | iteration : 240/600 | train loss : 0.037036722425158715\n",
      "| epoch : 9 | iteration : 250/600 | train loss : 9.948204846385671e-05\n",
      "| epoch : 9 | iteration : 260/600 | train loss : 0.0004394807911518212\n",
      "| epoch : 9 | iteration : 270/600 | train loss : 0.016501134678249433\n",
      "| epoch : 9 | iteration : 280/600 | train loss : 0.022974544163048397\n",
      "| epoch : 9 | iteration : 290/600 | train loss : 0.05019720503358701\n",
      "| epoch : 9 | iteration : 300/600 | train loss : 0.01828945287289019\n",
      "| epoch : 9 | iteration : 310/600 | train loss : 0.011444239035388215\n",
      "| epoch : 9 | iteration : 320/600 | train loss : 0.003240941424706344\n",
      "| epoch : 9 | iteration : 330/600 | train loss : 0.003463233828844361\n",
      "| epoch : 9 | iteration : 340/600 | train loss : 0.00019970967876821593\n",
      "| epoch : 9 | iteration : 350/600 | train loss : 0.1355016154784291\n",
      "| epoch : 9 | iteration : 360/600 | train loss : 0.00544016983293362\n",
      "| epoch : 9 | iteration : 370/600 | train loss : 0.001801654715225305\n",
      "| epoch : 9 | iteration : 380/600 | train loss : 0.05656607374087292\n",
      "| epoch : 9 | iteration : 390/600 | train loss : 0.07454888385394842\n",
      "| epoch : 9 | iteration : 400/600 | train loss : 0.011021060273945166\n",
      "| epoch : 9 | iteration : 410/600 | train loss : 0.02096356272632646\n",
      "| epoch : 9 | iteration : 420/600 | train loss : 0.0065417671311444395\n",
      "| epoch : 9 | iteration : 430/600 | train loss : 0.034483981283922\n",
      "| epoch : 9 | iteration : 440/600 | train loss : 0.02042289884597965\n",
      "| epoch : 9 | iteration : 450/600 | train loss : 0.07207178157290159\n",
      "| epoch : 9 | iteration : 460/600 | train loss : 0.09765451827402644\n",
      "| epoch : 9 | iteration : 470/600 | train loss : 0.09261828615056952\n",
      "| epoch : 9 | iteration : 480/600 | train loss : 0.07111707360651058\n",
      "| epoch : 9 | iteration : 490/600 | train loss : 0.0027391675684307853\n",
      "| epoch : 9 | iteration : 500/600 | train loss : 0.0009258604165743829\n",
      "| epoch : 9 | iteration : 510/600 | train loss : 0.03361321850819847\n",
      "| epoch : 9 | iteration : 520/600 | train loss : 0.02197130735065035\n",
      "| epoch : 9 | iteration : 530/600 | train loss : 0.006506248464640318\n",
      "| epoch : 9 | iteration : 540/600 | train loss : 0.1219874380671398\n",
      "| epoch : 9 | iteration : 550/600 | train loss : 0.0003540218918639857\n",
      "| epoch : 9 | iteration : 560/600 | train loss : 0.011982003495868991\n",
      "| epoch : 9 | iteration : 570/600 | train loss : 0.019335406916199668\n",
      "| epoch : 9 | iteration : 580/600 | train loss : 0.00048461386539604525\n",
      "| epoch : 9 | iteration : 590/600 | train loss : 0.008349586990173363\n",
      "| epoch 9 | train_accuracy : 0.9950 | test_accuracy : 0.9940 |\n",
      "| epoch : 10 | iteration : 0/600 | train loss : 0.05079623357423344\n",
      "| epoch : 10 | iteration : 10/600 | train loss : 0.08649517747398133\n",
      "| epoch : 10 | iteration : 20/600 | train loss : 0.011778992339704374\n",
      "| epoch : 10 | iteration : 30/600 | train loss : 2.99435901830172e-06\n",
      "| epoch : 10 | iteration : 40/600 | train loss : 0.001347003460314502\n",
      "| epoch : 10 | iteration : 50/600 | train loss : 0.04718403958208294\n",
      "| epoch : 10 | iteration : 60/600 | train loss : 0.06560589735131002\n",
      "| epoch : 10 | iteration : 70/600 | train loss : 0.00011700332174631876\n",
      "| epoch : 10 | iteration : 80/600 | train loss : 0.00015225420951461724\n",
      "| epoch : 10 | iteration : 90/600 | train loss : 0.04118777701928182\n",
      "| epoch : 10 | iteration : 100/600 | train loss : 0.0022241965465834212\n",
      "| epoch : 10 | iteration : 110/600 | train loss : 0.0861705507482097\n",
      "| epoch : 10 | iteration : 120/600 | train loss : 0.006859015447612259\n",
      "| epoch : 10 | iteration : 130/600 | train loss : 0.028732351026521115\n",
      "| epoch : 10 | iteration : 140/600 | train loss : 0.04445584711956667\n",
      "| epoch : 10 | iteration : 150/600 | train loss : 0.0003240823568802157\n",
      "| epoch : 10 | iteration : 160/600 | train loss : 0.0003600448423574283\n",
      "| epoch : 10 | iteration : 170/600 | train loss : 0.02365997145731309\n",
      "| epoch : 10 | iteration : 180/600 | train loss : 0.0007896186913248514\n",
      "| epoch : 10 | iteration : 190/600 | train loss : 0.00031470938490084713\n",
      "| epoch : 10 | iteration : 200/600 | train loss : 0.005305720389446985\n",
      "| epoch : 10 | iteration : 210/600 | train loss : 0.00012488736018611884\n",
      "| epoch : 10 | iteration : 220/600 | train loss : 0.0026509235535356773\n",
      "| epoch : 10 | iteration : 230/600 | train loss : 0.05879211313308708\n",
      "| epoch : 10 | iteration : 240/600 | train loss : 0.00019909194197458016\n",
      "| epoch : 10 | iteration : 250/600 | train loss : 0.00014170569160234234\n",
      "| epoch : 10 | iteration : 260/600 | train loss : 1.6825848911720896e-05\n",
      "| epoch : 10 | iteration : 270/600 | train loss : 0.017538412430410714\n",
      "| epoch : 10 | iteration : 280/600 | train loss : 0.01305921265819907\n",
      "| epoch : 10 | iteration : 290/600 | train loss : 0.13594854271140153\n",
      "| epoch : 10 | iteration : 300/600 | train loss : 0.15964597516255272\n",
      "| epoch : 10 | iteration : 310/600 | train loss : 0.0006359530219909998\n",
      "| epoch : 10 | iteration : 320/600 | train loss : 0.0004941785899798456\n",
      "| epoch : 10 | iteration : 330/600 | train loss : 0.018711387883663777\n",
      "| epoch : 10 | iteration : 340/600 | train loss : 0.0004918500712883197\n",
      "| epoch : 10 | iteration : 350/600 | train loss : 0.009696146469101163\n",
      "| epoch : 10 | iteration : 360/600 | train loss : 0.02450699636960485\n",
      "| epoch : 10 | iteration : 370/600 | train loss : 0.0005598012069713229\n",
      "| epoch : 10 | iteration : 380/600 | train loss : 0.036052161935330014\n",
      "| epoch : 10 | iteration : 390/600 | train loss : 0.0013497842570097864\n",
      "| epoch : 10 | iteration : 400/600 | train loss : 0.027010158705040702\n",
      "| epoch : 10 | iteration : 410/600 | train loss : 0.005821065102324785\n",
      "| epoch : 10 | iteration : 420/600 | train loss : 0.0035933240328564304\n",
      "| epoch : 10 | iteration : 430/600 | train loss : 0.0003438474943736057\n",
      "| epoch : 10 | iteration : 440/600 | train loss : 0.10535308881602122\n",
      "| epoch : 10 | iteration : 450/600 | train loss : 0.0005335274914869472\n",
      "| epoch : 10 | iteration : 460/600 | train loss : 0.013414475704890327\n",
      "| epoch : 10 | iteration : 470/600 | train loss : 0.0023971352522309452\n",
      "| epoch : 10 | iteration : 480/600 | train loss : 2.2472262044090133e-05\n",
      "| epoch : 10 | iteration : 490/600 | train loss : 0.014777316865444376\n",
      "| epoch : 10 | iteration : 500/600 | train loss : 0.005656529441857075\n",
      "| epoch : 10 | iteration : 510/600 | train loss : 0.0012721407785019024\n",
      "| epoch : 10 | iteration : 520/600 | train loss : 0.003984154694968911\n",
      "| epoch : 10 | iteration : 530/600 | train loss : 0.0058133780596862205\n",
      "| epoch : 10 | iteration : 540/600 | train loss : 0.0011700802448486353\n",
      "| epoch : 10 | iteration : 550/600 | train loss : 0.00023662965900395118\n",
      "| epoch : 10 | iteration : 560/600 | train loss : 0.04325405764431259\n",
      "| epoch : 10 | iteration : 570/600 | train loss : 0.00028694079991258627\n",
      "| epoch : 10 | iteration : 580/600 | train loss : 0.019799330375639707\n",
      "| epoch : 10 | iteration : 590/600 | train loss : 0.0018399305334554934\n",
      "| epoch 10 | train_accuracy : 0.9930 | test_accuracy : 0.9830 |\n",
      "| epoch : 11 | iteration : 0/600 | train loss : 0.001508943844004352\n",
      "| epoch : 11 | iteration : 10/600 | train loss : 0.0118905697946013\n",
      "| epoch : 11 | iteration : 20/600 | train loss : 0.0008242523772787004\n",
      "| epoch : 11 | iteration : 30/600 | train loss : 0.01007978054456946\n",
      "| epoch : 11 | iteration : 40/600 | train loss : 3.683818107355588e-05\n",
      "| epoch : 11 | iteration : 50/600 | train loss : 0.06230851408548776\n",
      "| epoch : 11 | iteration : 60/600 | train loss : 0.0004292106720243745\n",
      "| epoch : 11 | iteration : 70/600 | train loss : 0.002755038176423732\n",
      "| epoch : 11 | iteration : 80/600 | train loss : 0.09504461914575117\n",
      "| epoch : 11 | iteration : 90/600 | train loss : 0.01943071872071815\n",
      "| epoch : 11 | iteration : 100/600 | train loss : 0.0670061551202604\n",
      "| epoch : 11 | iteration : 110/600 | train loss : 1.0669735735357069e-05\n",
      "| epoch : 11 | iteration : 120/600 | train loss : 0.03645366485577953\n",
      "| epoch : 11 | iteration : 130/600 | train loss : 0.007678426434374537\n",
      "| epoch : 11 | iteration : 140/600 | train loss : 0.018363542247781826\n",
      "| epoch : 11 | iteration : 150/600 | train loss : 0.010560041617896015\n",
      "| epoch : 11 | iteration : 160/600 | train loss : 0.025800338791174608\n",
      "| epoch : 11 | iteration : 170/600 | train loss : 0.00147403123232703\n",
      "| epoch : 11 | iteration : 180/600 | train loss : 0.01761908099806294\n",
      "| epoch : 11 | iteration : 190/600 | train loss : 0.06958641146504721\n",
      "| epoch : 11 | iteration : 200/600 | train loss : 1.3551154605943033e-05\n",
      "| epoch : 11 | iteration : 210/600 | train loss : 0.1569352902908646\n",
      "| epoch : 11 | iteration : 220/600 | train loss : 6.348757482796554e-06\n",
      "| epoch : 11 | iteration : 230/600 | train loss : 0.010727735076742179\n",
      "| epoch : 11 | iteration : 240/600 | train loss : 0.0007591957864794412\n",
      "| epoch : 11 | iteration : 250/600 | train loss : 0.029775488370980763\n",
      "| epoch : 11 | iteration : 260/600 | train loss : 0.0001500908133304321\n",
      "| epoch : 11 | iteration : 270/600 | train loss : 0.0015253933299967499\n",
      "| epoch : 11 | iteration : 280/600 | train loss : 0.03730181456829164\n",
      "| epoch : 11 | iteration : 290/600 | train loss : 0.00027840479313139183\n",
      "| epoch : 11 | iteration : 300/600 | train loss : 0.0061215769299333665\n",
      "| epoch : 11 | iteration : 310/600 | train loss : 0.0002719339277892065\n",
      "| epoch : 11 | iteration : 320/600 | train loss : 0.19235249730943013\n",
      "| epoch : 11 | iteration : 330/600 | train loss : 8.30960511962065e-05\n",
      "| epoch : 11 | iteration : 340/600 | train loss : 0.007504047699228285\n",
      "| epoch : 11 | iteration : 350/600 | train loss : 0.14979109198510593\n",
      "| epoch : 11 | iteration : 360/600 | train loss : 6.743437027108326e-05\n",
      "| epoch : 11 | iteration : 370/600 | train loss : 0.02183054639600793\n",
      "| epoch : 11 | iteration : 380/600 | train loss : 0.00017053139304699968\n",
      "| epoch : 11 | iteration : 390/600 | train loss : 0.008646748307562792\n",
      "| epoch : 11 | iteration : 400/600 | train loss : 0.06843606568534052\n",
      "| epoch : 11 | iteration : 410/600 | train loss : 0.05238642377759192\n",
      "| epoch : 11 | iteration : 420/600 | train loss : 0.027409312239263493\n",
      "| epoch : 11 | iteration : 430/600 | train loss : 0.008094120536275777\n",
      "| epoch : 11 | iteration : 440/600 | train loss : 0.00040025968944476265\n",
      "| epoch : 11 | iteration : 450/600 | train loss : 0.027630799856349683\n",
      "| epoch : 11 | iteration : 460/600 | train loss : 2.1627394819278797e-05\n",
      "| epoch : 11 | iteration : 470/600 | train loss : 0.00568399288955489\n",
      "| epoch : 11 | iteration : 480/600 | train loss : 0.013302991789479522\n",
      "| epoch : 11 | iteration : 490/600 | train loss : 0.29844506861408016\n",
      "| epoch : 11 | iteration : 500/600 | train loss : 1.9211019735899673e-06\n",
      "| epoch : 11 | iteration : 510/600 | train loss : 0.0038589475327259403\n",
      "| epoch : 11 | iteration : 520/600 | train loss : 0.00013939538897081633\n",
      "| epoch : 11 | iteration : 530/600 | train loss : 0.005134483007163002\n",
      "| epoch : 11 | iteration : 540/600 | train loss : -7.691608905511992e-08\n",
      "| epoch : 11 | iteration : 550/600 | train loss : 6.191645388289329e-05\n",
      "| epoch : 11 | iteration : 560/600 | train loss : 0.10281531568270642\n",
      "| epoch : 11 | iteration : 570/600 | train loss : 0.02928238441558577\n",
      "| epoch : 11 | iteration : 580/600 | train loss : 0.018485083913396358\n",
      "| epoch : 11 | iteration : 590/600 | train loss : 0.00017761991080573667\n",
      "| epoch 11 | train_accuracy : 0.9970 | test_accuracy : 0.9780 |\n",
      "| epoch : 12 | iteration : 0/600 | train loss : 0.02479718716791898\n",
      "| epoch : 12 | iteration : 10/600 | train loss : 0.005805487332217047\n",
      "| epoch : 12 | iteration : 20/600 | train loss : 0.02678589539319355\n",
      "| epoch : 12 | iteration : 30/600 | train loss : 7.016243024777313e-05\n",
      "| epoch : 12 | iteration : 40/600 | train loss : 0.0663180440680307\n",
      "| epoch : 12 | iteration : 50/600 | train loss : 1.810168712602239e-06\n",
      "| epoch : 12 | iteration : 60/600 | train loss : 0.06288993406406092\n",
      "| epoch : 12 | iteration : 70/600 | train loss : 0.04675631582057355\n",
      "| epoch : 12 | iteration : 80/600 | train loss : 0.0009187432736035451\n",
      "| epoch : 12 | iteration : 90/600 | train loss : 0.023548250039359564\n",
      "| epoch : 12 | iteration : 100/600 | train loss : 0.05395410458827615\n",
      "| epoch : 12 | iteration : 110/600 | train loss : 8.814441725689675e-07\n",
      "| epoch : 12 | iteration : 120/600 | train loss : 1.260106293998561e-05\n",
      "| epoch : 12 | iteration : 130/600 | train loss : 0.0005247649642353189\n",
      "| epoch : 12 | iteration : 140/600 | train loss : 4.981751996916689e-05\n",
      "| epoch : 12 | iteration : 150/600 | train loss : 0.0004926918321617761\n",
      "| epoch : 12 | iteration : 160/600 | train loss : 1.0175285427541208e-06\n",
      "| epoch : 12 | iteration : 170/600 | train loss : 0.00032087433406860293\n",
      "| epoch : 12 | iteration : 180/600 | train loss : 0.17393290375774145\n",
      "| epoch : 12 | iteration : 190/600 | train loss : 0.0040747412382553675\n",
      "| epoch : 12 | iteration : 200/600 | train loss : 9.527923398043354e-06\n",
      "| epoch : 12 | iteration : 210/600 | train loss : 0.05036867675085249\n",
      "| epoch : 12 | iteration : 220/600 | train loss : 0.016800281775828704\n",
      "| epoch : 12 | iteration : 230/600 | train loss : 4.368020159146268e-06\n",
      "| epoch : 12 | iteration : 240/600 | train loss : 0.002611775307284633\n",
      "| epoch : 12 | iteration : 250/600 | train loss : 0.021907966462773\n",
      "| epoch : 12 | iteration : 260/600 | train loss : 1.77727613378668e-05\n",
      "| epoch : 12 | iteration : 270/600 | train loss : 0.0012719737029778157\n",
      "| epoch : 12 | iteration : 280/600 | train loss : 0.025114945994859496\n",
      "| epoch : 12 | iteration : 290/600 | train loss : 0.0002188978535449653\n",
      "| epoch : 12 | iteration : 300/600 | train loss : 4.468609769876202e-05\n",
      "| epoch : 12 | iteration : 310/600 | train loss : 0.0014960403035203013\n",
      "| epoch : 12 | iteration : 320/600 | train loss : 0.0008194249138590731\n",
      "| epoch : 12 | iteration : 330/600 | train loss : 0.0018185967036610526\n",
      "| epoch : 12 | iteration : 340/600 | train loss : 0.0001108264065400953\n",
      "| epoch : 12 | iteration : 350/600 | train loss : 0.0006103659620697458\n",
      "| epoch : 12 | iteration : 360/600 | train loss : 7.621579791154652e-05\n",
      "| epoch : 12 | iteration : 370/600 | train loss : 0.001963975031833622\n",
      "| epoch : 12 | iteration : 380/600 | train loss : 0.00023344192046435716\n",
      "| epoch : 12 | iteration : 390/600 | train loss : 4.761293095546618e-05\n",
      "| epoch : 12 | iteration : 400/600 | train loss : 0.07381342708987944\n",
      "| epoch : 12 | iteration : 410/600 | train loss : 0.00029058594269726985\n",
      "| epoch : 12 | iteration : 420/600 | train loss : 0.16148114628561994\n",
      "| epoch : 12 | iteration : 430/600 | train loss : 0.00022717397918225232\n",
      "| epoch : 12 | iteration : 440/600 | train loss : 0.05287204640099921\n",
      "| epoch : 12 | iteration : 450/600 | train loss : 0.014534175345396529\n",
      "| epoch : 12 | iteration : 460/600 | train loss : 0.0019834396315570133\n",
      "| epoch : 12 | iteration : 470/600 | train loss : 0.0034460375211011153\n",
      "| epoch : 12 | iteration : 480/600 | train loss : 0.008720131638912849\n",
      "| epoch : 12 | iteration : 490/600 | train loss : 0.05327575829992062\n",
      "| epoch : 12 | iteration : 500/600 | train loss : 0.0002502589469330583\n",
      "| epoch : 12 | iteration : 510/600 | train loss : 0.10500899950939711\n",
      "| epoch : 12 | iteration : 520/600 | train loss : 0.0009991209905073509\n",
      "| epoch : 12 | iteration : 530/600 | train loss : 0.0008722649199227362\n",
      "| epoch : 12 | iteration : 540/600 | train loss : 0.02279587714316784\n",
      "| epoch : 12 | iteration : 550/600 | train loss : 0.0016696436249918633\n",
      "| epoch : 12 | iteration : 560/600 | train loss : 2.152549969740683e-06\n",
      "| epoch : 12 | iteration : 570/600 | train loss : 0.0018461916011849088\n",
      "| epoch : 12 | iteration : 580/600 | train loss : 0.0002874303498293315\n",
      "| epoch : 12 | iteration : 590/600 | train loss : 0.03618542851806838\n",
      "| epoch 12 | train_accuracy : 0.9970 | test_accuracy : 0.9890 |\n",
      "| epoch : 13 | iteration : 0/600 | train loss : 0.14998059036501007\n",
      "| epoch : 13 | iteration : 10/600 | train loss : 0.002777302289889578\n",
      "| epoch : 13 | iteration : 20/600 | train loss : 0.06266071547277488\n",
      "| epoch : 13 | iteration : 30/600 | train loss : 0.02039387667641936\n",
      "| epoch : 13 | iteration : 40/600 | train loss : 0.0009187488468480051\n",
      "| epoch : 13 | iteration : 50/600 | train loss : 0.026182166192194437\n",
      "| epoch : 13 | iteration : 60/600 | train loss : 0.02725194295626032\n",
      "| epoch : 13 | iteration : 70/600 | train loss : 0.007535604736217485\n",
      "| epoch : 13 | iteration : 80/600 | train loss : 3.9494871264287325e-05\n",
      "| epoch : 13 | iteration : 90/600 | train loss : 5.11448454098677e-05\n",
      "| epoch : 13 | iteration : 100/600 | train loss : 8.154470289496176e-05\n",
      "| epoch : 13 | iteration : 110/600 | train loss : 0.0006697167768677384\n",
      "| epoch : 13 | iteration : 120/600 | train loss : 0.009233074664287107\n",
      "| epoch : 13 | iteration : 130/600 | train loss : 0.005616494535468622\n",
      "| epoch : 13 | iteration : 140/600 | train loss : 0.03495880720942237\n",
      "| epoch : 13 | iteration : 150/600 | train loss : 0.00828174246017064\n",
      "| epoch : 13 | iteration : 160/600 | train loss : 0.10496873504154039\n",
      "| epoch : 13 | iteration : 170/600 | train loss : 0.004735824135965842\n",
      "| epoch : 13 | iteration : 180/600 | train loss : 0.03125130707536036\n",
      "| epoch : 13 | iteration : 190/600 | train loss : 4.593173999421716e-06\n",
      "| epoch : 13 | iteration : 200/600 | train loss : 0.0007021964955145739\n",
      "| epoch : 13 | iteration : 210/600 | train loss : 3.21868769310362e-05\n",
      "| epoch : 13 | iteration : 220/600 | train loss : 0.00025216271481084946\n",
      "| epoch : 13 | iteration : 230/600 | train loss : 0.161140068037114\n",
      "| epoch : 13 | iteration : 240/600 | train loss : 7.577312428592153e-05\n",
      "| epoch : 13 | iteration : 250/600 | train loss : 1.953301979227727e-06\n",
      "| epoch : 13 | iteration : 260/600 | train loss : 2.136498063874674e-05\n",
      "| epoch : 13 | iteration : 270/600 | train loss : 0.006898616579686887\n",
      "| epoch : 13 | iteration : 280/600 | train loss : 0.0004814254517137605\n",
      "| epoch : 13 | iteration : 290/600 | train loss : 1.8611422631185747e-05\n",
      "| epoch : 13 | iteration : 300/600 | train loss : 1.1329498609017475e-05\n",
      "| epoch : 13 | iteration : 310/600 | train loss : 1.3197490766902766e-05\n",
      "| epoch : 13 | iteration : 320/600 | train loss : 0.0001644344598922999\n",
      "| epoch : 13 | iteration : 330/600 | train loss : 0.001007712670444221\n",
      "| epoch : 13 | iteration : 340/600 | train loss : 0.04649892095771055\n",
      "| epoch : 13 | iteration : 350/600 | train loss : 0.00012120544207276564\n",
      "| epoch : 13 | iteration : 360/600 | train loss : 0.03871203622977513\n",
      "| epoch : 13 | iteration : 370/600 | train loss : 0.009792508184335708\n",
      "| epoch : 13 | iteration : 380/600 | train loss : 0.008256453594354398\n",
      "| epoch : 13 | iteration : 390/600 | train loss : 0.08513429299413387\n",
      "| epoch : 13 | iteration : 400/600 | train loss : 3.719472346563953e-05\n",
      "| epoch : 13 | iteration : 410/600 | train loss : 0.017269303228836897\n",
      "| epoch : 13 | iteration : 420/600 | train loss : 0.0002245186188724602\n",
      "| epoch : 13 | iteration : 430/600 | train loss : 0.07802212479963898\n",
      "| epoch : 13 | iteration : 440/600 | train loss : 6.072077071593495e-05\n",
      "| epoch : 13 | iteration : 450/600 | train loss : 0.00713549866727278\n",
      "| epoch : 13 | iteration : 460/600 | train loss : 0.05387700871633671\n",
      "| epoch : 13 | iteration : 470/600 | train loss : 0.0007126383699029128\n",
      "| epoch : 13 | iteration : 480/600 | train loss : 8.791504685867788e-05\n",
      "| epoch : 13 | iteration : 490/600 | train loss : 0.05998528516405838\n",
      "| epoch : 13 | iteration : 500/600 | train loss : 0.030569848986453582\n",
      "| epoch : 13 | iteration : 510/600 | train loss : 0.02319946212468864\n",
      "| epoch : 13 | iteration : 520/600 | train loss : 5.563761189040602e-07\n",
      "| epoch : 13 | iteration : 530/600 | train loss : 0.00016634868184357468\n",
      "| epoch : 13 | iteration : 540/600 | train loss : 0.07861949415137234\n",
      "| epoch : 13 | iteration : 550/600 | train loss : 0.145187487634823\n",
      "| epoch : 13 | iteration : 560/600 | train loss : 0.14007730688057854\n",
      "| epoch : 13 | iteration : 570/600 | train loss : 0.0016244628155400642\n",
      "| epoch : 13 | iteration : 580/600 | train loss : 0.19355125733637038\n",
      "| epoch : 13 | iteration : 590/600 | train loss : 0.2048223628162114\n",
      "| epoch 13 | train_accuracy : 0.9950 | test_accuracy : 0.9840 |\n",
      "| epoch : 14 | iteration : 0/600 | train loss : 0.005711325645459311\n",
      "| epoch : 14 | iteration : 10/600 | train loss : 0.010856163340119658\n",
      "| epoch : 14 | iteration : 20/600 | train loss : 0.0008304027477724345\n",
      "| epoch : 14 | iteration : 30/600 | train loss : 1.102731213301898e-05\n",
      "| epoch : 14 | iteration : 40/600 | train loss : 3.192941318822154e-05\n",
      "| epoch : 14 | iteration : 50/600 | train loss : 0.0003297740939448957\n",
      "| epoch : 14 | iteration : 60/600 | train loss : 2.291461154096754e-06\n",
      "| epoch : 14 | iteration : 70/600 | train loss : 0.12159284628893423\n",
      "| epoch : 14 | iteration : 80/600 | train loss : 0.0021393699773523695\n",
      "| epoch : 14 | iteration : 90/600 | train loss : 0.002055038500614412\n",
      "| epoch : 14 | iteration : 100/600 | train loss : 0.10947771615070476\n",
      "| epoch : 14 | iteration : 110/600 | train loss : 4.013816480131536e-05\n",
      "| epoch : 14 | iteration : 120/600 | train loss : 1.4596898430680736e-06\n",
      "| epoch : 14 | iteration : 130/600 | train loss : 0.0003186015901763757\n",
      "| epoch : 14 | iteration : 140/600 | train loss : 8.196982626675063e-06\n",
      "| epoch : 14 | iteration : 150/600 | train loss : 0.009558444070776063\n",
      "| epoch : 14 | iteration : 160/600 | train loss : 0.0008027666943893732\n",
      "| epoch : 14 | iteration : 170/600 | train loss : 0.0007211257865556742\n",
      "| epoch : 14 | iteration : 180/600 | train loss : 4.59278661432631e-05\n",
      "| epoch : 14 | iteration : 190/600 | train loss : 0.07531471342824737\n",
      "| epoch : 14 | iteration : 200/600 | train loss : 1.0663508250224645e-06\n",
      "| epoch : 14 | iteration : 210/600 | train loss : 2.1156870315475198e-07\n",
      "| epoch : 14 | iteration : 220/600 | train loss : 8.347939581018518e-05\n",
      "| epoch : 14 | iteration : 230/600 | train loss : 0.01800638561933889\n",
      "| epoch : 14 | iteration : 240/600 | train loss : 0.16103755587784233\n",
      "| epoch : 14 | iteration : 250/600 | train loss : 0.006960079391880927\n",
      "| epoch : 14 | iteration : 260/600 | train loss : 0.0010684212793068964\n",
      "| epoch : 14 | iteration : 270/600 | train loss : 0.011586796680492608\n",
      "| epoch : 14 | iteration : 280/600 | train loss : 0.0878048818307345\n",
      "| epoch : 14 | iteration : 290/600 | train loss : 0.005084179563785662\n",
      "| epoch : 14 | iteration : 300/600 | train loss : 0.016742403392537807\n",
      "| epoch : 14 | iteration : 310/600 | train loss : 4.678795705370184e-05\n",
      "| epoch : 14 | iteration : 320/600 | train loss : 0.02783506803594618\n",
      "| epoch : 14 | iteration : 330/600 | train loss : 0.05525211871122833\n",
      "| epoch : 14 | iteration : 340/600 | train loss : 0.0062358959067412316\n",
      "| epoch : 14 | iteration : 350/600 | train loss : 0.00875306448919091\n",
      "| epoch : 14 | iteration : 360/600 | train loss : 0.013302916970801704\n",
      "| epoch : 14 | iteration : 370/600 | train loss : 2.050598137978522e-06\n",
      "| epoch : 14 | iteration : 380/600 | train loss : 1.146211411018313e-05\n",
      "| epoch : 14 | iteration : 390/600 | train loss : 0.00032157824560277227\n",
      "| epoch : 14 | iteration : 400/600 | train loss : 2.647384520219472e-05\n",
      "| epoch : 14 | iteration : 410/600 | train loss : 0.0002687991755018475\n",
      "| epoch : 14 | iteration : 420/600 | train loss : 0.0030649262823618023\n",
      "| epoch : 14 | iteration : 430/600 | train loss : 0.00012637992081189562\n",
      "| epoch : 14 | iteration : 440/600 | train loss : 0.0006878108116797963\n",
      "| epoch : 14 | iteration : 450/600 | train loss : 1.0380786772119426e-06\n",
      "| epoch : 14 | iteration : 460/600 | train loss : 0.00012129556808373616\n",
      "| epoch : 14 | iteration : 470/600 | train loss : 0.055230838489137526\n",
      "| epoch : 14 | iteration : 480/600 | train loss : 0.08675208212873993\n",
      "| epoch : 14 | iteration : 490/600 | train loss : 0.10967945309126356\n",
      "| epoch : 14 | iteration : 500/600 | train loss : 0.10355427559493904\n",
      "| epoch : 14 | iteration : 510/600 | train loss : 4.089319629035534e-06\n",
      "| epoch : 14 | iteration : 520/600 | train loss : 0.006165209041563962\n",
      "| epoch : 14 | iteration : 530/600 | train loss : 0.0522020109574306\n",
      "| epoch : 14 | iteration : 540/600 | train loss : 0.00017698258576269405\n",
      "| epoch : 14 | iteration : 550/600 | train loss : 0.0009060834614988206\n",
      "| epoch : 14 | iteration : 560/600 | train loss : 0.03967600285646229\n",
      "| epoch : 14 | iteration : 570/600 | train loss : 0.03682584033358203\n",
      "| epoch : 14 | iteration : 580/600 | train loss : 0.16149759186453178\n",
      "| epoch : 14 | iteration : 590/600 | train loss : 0.09524310378061401\n",
      "| epoch 14 | train_accuracy : 0.9980 | test_accuracy : 0.9800 |\n",
      "| epoch : 15 | iteration : 0/600 | train loss : 0.019704528244731692\n",
      "| epoch : 15 | iteration : 10/600 | train loss : 0.0015869337065007305\n",
      "| epoch : 15 | iteration : 20/600 | train loss : 0.01003617067030131\n",
      "| epoch : 15 | iteration : 30/600 | train loss : 7.86119276685526e-05\n",
      "| epoch : 15 | iteration : 40/600 | train loss : -9.158368211163887e-08\n",
      "| epoch : 15 | iteration : 50/600 | train loss : 0.0517432243922964\n",
      "| epoch : 15 | iteration : 60/600 | train loss : 5.489266037544798e-05\n",
      "| epoch : 15 | iteration : 70/600 | train loss : 1.0615276986764077e-05\n",
      "| epoch : 15 | iteration : 80/600 | train loss : 0.00782233767470209\n",
      "| epoch : 15 | iteration : 90/600 | train loss : 0.056057452427020336\n",
      "| epoch : 15 | iteration : 100/600 | train loss : 0.0001966948003433097\n",
      "| epoch : 15 | iteration : 110/600 | train loss : 4.2943275257201213e-07\n",
      "| epoch : 15 | iteration : 120/600 | train loss : 2.4257631317346184e-05\n",
      "| epoch : 15 | iteration : 130/600 | train loss : 0.00014430252958920868\n",
      "| epoch : 15 | iteration : 140/600 | train loss : 5.8303354022259844e-08\n",
      "| epoch : 15 | iteration : 150/600 | train loss : 1.988730946081044e-06\n",
      "| epoch : 15 | iteration : 160/600 | train loss : 0.0215895988610774\n",
      "| epoch : 15 | iteration : 170/600 | train loss : 2.0568517364117486e-05\n",
      "| epoch : 15 | iteration : 180/600 | train loss : 0.06179731492149046\n",
      "| epoch : 15 | iteration : 190/600 | train loss : 8.591067041537203e-06\n",
      "| epoch : 15 | iteration : 200/600 | train loss : 7.186827865403852e-07\n",
      "| epoch : 15 | iteration : 210/600 | train loss : 0.004517386984053072\n",
      "| epoch : 15 | iteration : 220/600 | train loss : 0.00011877190652238012\n",
      "| epoch : 15 | iteration : 230/600 | train loss : 0.04740795877315966\n",
      "| epoch : 15 | iteration : 240/600 | train loss : 7.599810448816264e-06\n",
      "| epoch : 15 | iteration : 250/600 | train loss : 2.0817198455543754e-05\n",
      "| epoch : 15 | iteration : 260/600 | train loss : 0.015492049715479635\n",
      "| epoch : 15 | iteration : 270/600 | train loss : 0.0014911371844666748\n",
      "| epoch : 15 | iteration : 280/600 | train loss : 0.04270779824341942\n",
      "| epoch : 15 | iteration : 290/600 | train loss : 0.12296332875080612\n",
      "| epoch : 15 | iteration : 300/600 | train loss : 0.019090473738443452\n",
      "| epoch : 15 | iteration : 310/600 | train loss : 0.09854970458245639\n",
      "| epoch : 15 | iteration : 320/600 | train loss : 0.04006794584503744\n",
      "| epoch : 15 | iteration : 330/600 | train loss : 0.00010213754009132227\n",
      "| epoch : 15 | iteration : 340/600 | train loss : 1.1699757001300868e-05\n",
      "| epoch : 15 | iteration : 350/600 | train loss : 5.5058625326196484e-05\n",
      "| epoch : 15 | iteration : 360/600 | train loss : 2.8891342541591905e-05\n",
      "| epoch : 15 | iteration : 370/600 | train loss : 0.012856502930552826\n",
      "| epoch : 15 | iteration : 380/600 | train loss : 0.0018704253480679466\n",
      "| epoch : 15 | iteration : 390/600 | train loss : 2.9610794919637753e-05\n",
      "| epoch : 15 | iteration : 400/600 | train loss : 0.02332129745694345\n",
      "| epoch : 15 | iteration : 410/600 | train loss : 0.1011754051978075\n",
      "| epoch : 15 | iteration : 420/600 | train loss : 0.004899299291124136\n",
      "| epoch : 15 | iteration : 430/600 | train loss : 0.027551944278331214\n",
      "| epoch : 15 | iteration : 440/600 | train loss : 0.018044480686914287\n",
      "| epoch : 15 | iteration : 450/600 | train loss : 8.997334826869568e-06\n",
      "| epoch : 15 | iteration : 460/600 | train loss : 0.002069954972788365\n",
      "| epoch : 15 | iteration : 470/600 | train loss : 3.93826665586669e-05\n",
      "| epoch : 15 | iteration : 480/600 | train loss : 0.0010029048346283916\n",
      "| epoch : 15 | iteration : 490/600 | train loss : 0.008511680181341212\n",
      "| epoch : 15 | iteration : 500/600 | train loss : 1.1388578224095107e-06\n",
      "| epoch : 15 | iteration : 510/600 | train loss : 1.5160629414380335e-07\n",
      "| epoch : 15 | iteration : 520/600 | train loss : 0.14831445124495451\n",
      "| epoch : 15 | iteration : 530/600 | train loss : 0.11566041943491777\n",
      "| epoch : 15 | iteration : 540/600 | train loss : 5.040781685498658e-05\n",
      "| epoch : 15 | iteration : 550/600 | train loss : 0.0006671482435766086\n",
      "| epoch : 15 | iteration : 560/600 | train loss : 0.06113944632060045\n",
      "| epoch : 15 | iteration : 570/600 | train loss : 0.08857835332140525\n",
      "| epoch : 15 | iteration : 580/600 | train loss : 0.017065446006622253\n",
      "| epoch : 15 | iteration : 590/600 | train loss : 0.00015300359983183242\n",
      "| epoch 15 | train_accuracy : 0.9970 | test_accuracy : 0.9780 |\n",
      "| epoch : 16 | iteration : 0/600 | train loss : 0.04508710474427411\n",
      "| epoch : 16 | iteration : 10/600 | train loss : 0.026961580924800985\n",
      "| epoch : 16 | iteration : 20/600 | train loss : 7.530577052593895e-05\n",
      "| epoch : 16 | iteration : 30/600 | train loss : 0.0001047521509403647\n",
      "| epoch : 16 | iteration : 40/600 | train loss : 4.185345767600369e-07\n",
      "| epoch : 16 | iteration : 50/600 | train loss : 0.08315182029370101\n",
      "| epoch : 16 | iteration : 60/600 | train loss : 0.0014455090442227045\n",
      "| epoch : 16 | iteration : 70/600 | train loss : 0.005145461821265646\n",
      "| epoch : 16 | iteration : 80/600 | train loss : 0.07069359886649677\n",
      "| epoch : 16 | iteration : 90/600 | train loss : 1.6724426230343325e-05\n",
      "| epoch : 16 | iteration : 100/600 | train loss : 4.0461423309138994e-05\n",
      "| epoch : 16 | iteration : 110/600 | train loss : 7.826148095553273e-06\n",
      "| epoch : 16 | iteration : 120/600 | train loss : 1.1626225162395915e-05\n",
      "| epoch : 16 | iteration : 130/600 | train loss : 0.0037786384452400982\n",
      "| epoch : 16 | iteration : 140/600 | train loss : 0.005500520497906972\n",
      "| epoch : 16 | iteration : 150/600 | train loss : 0.028393018310397974\n",
      "| epoch : 16 | iteration : 160/600 | train loss : 1.581684895286844e-07\n",
      "| epoch : 16 | iteration : 170/600 | train loss : 0.0002508465661332324\n",
      "| epoch : 16 | iteration : 180/600 | train loss : 0.00030902315597954906\n",
      "| epoch : 16 | iteration : 190/600 | train loss : 0.01965718191948965\n",
      "| epoch : 16 | iteration : 200/600 | train loss : 1.9864750978528023e-07\n",
      "| epoch : 16 | iteration : 210/600 | train loss : 6.96779426504719e-06\n",
      "| epoch : 16 | iteration : 220/600 | train loss : 0.0021102571730980957\n",
      "| epoch : 16 | iteration : 230/600 | train loss : 3.3477783097403487e-06\n",
      "| epoch : 16 | iteration : 240/600 | train loss : 0.16119651359880616\n",
      "| epoch : 16 | iteration : 250/600 | train loss : 1.3107818980323796e-05\n",
      "| epoch : 16 | iteration : 260/600 | train loss : 0.008261204405706305\n",
      "| epoch : 16 | iteration : 270/600 | train loss : 0.00023783190705492437\n",
      "| epoch : 16 | iteration : 280/600 | train loss : 0.0004280988025148608\n",
      "| epoch : 16 | iteration : 290/600 | train loss : 0.012912874107138192\n",
      "| epoch : 16 | iteration : 300/600 | train loss : 0.029087140906359554\n",
      "| epoch : 16 | iteration : 310/600 | train loss : 1.121617895578485e-05\n",
      "| epoch : 16 | iteration : 320/600 | train loss : 0.12409810429566238\n",
      "| epoch : 16 | iteration : 330/600 | train loss : 3.380446519679034e-05\n",
      "| epoch : 16 | iteration : 340/600 | train loss : 3.240037969345945e-05\n",
      "| epoch : 16 | iteration : 350/600 | train loss : 0.025309273171752876\n",
      "| epoch : 16 | iteration : 360/600 | train loss : 0.016881894187013306\n",
      "| epoch : 16 | iteration : 370/600 | train loss : -5.988835812135051e-08\n",
      "| epoch : 16 | iteration : 380/600 | train loss : 0.00018050541349028083\n",
      "| epoch : 16 | iteration : 390/600 | train loss : 3.4642296435337617e-07\n",
      "| epoch : 16 | iteration : 400/600 | train loss : 0.035614571712047245\n",
      "| epoch : 16 | iteration : 410/600 | train loss : 0.00040957268819745137\n",
      "| epoch : 16 | iteration : 420/600 | train loss : 8.555220075342521e-07\n",
      "| epoch : 16 | iteration : 430/600 | train loss : 0.12650097531263987\n",
      "| epoch : 16 | iteration : 440/600 | train loss : 0.07443868772770991\n",
      "| epoch : 16 | iteration : 450/600 | train loss : 6.050380954233634e-06\n",
      "| epoch : 16 | iteration : 460/600 | train loss : 1.7543302025011133e-07\n",
      "| epoch : 16 | iteration : 470/600 | train loss : 8.635803676206582e-07\n",
      "| epoch : 16 | iteration : 480/600 | train loss : 0.00012777254654303463\n",
      "| epoch : 16 | iteration : 490/600 | train loss : 0.0003669934846378427\n",
      "| epoch : 16 | iteration : 500/600 | train loss : 0.00012616766744025438\n",
      "| epoch : 16 | iteration : 510/600 | train loss : 0.0035177852078631857\n",
      "| epoch : 16 | iteration : 520/600 | train loss : 3.7153295099542428e-06\n",
      "| epoch : 16 | iteration : 530/600 | train loss : 0.001552448948967258\n",
      "| epoch : 16 | iteration : 540/600 | train loss : 0.0015876601522750791\n",
      "| epoch : 16 | iteration : 550/600 | train loss : 0.013645885568548548\n",
      "| epoch : 16 | iteration : 560/600 | train loss : 0.015967560940151507\n",
      "| epoch : 16 | iteration : 570/600 | train loss : 0.13843805663402112\n",
      "| epoch : 16 | iteration : 580/600 | train loss : 0.027729602739370213\n",
      "| epoch : 16 | iteration : 590/600 | train loss : 0.0022092309224596295\n",
      "| epoch 16 | train_accuracy : 0.9950 | test_accuracy : 0.9860 |\n",
      "| epoch : 17 | iteration : 0/600 | train loss : 0.00014662974582796472\n",
      "| epoch : 17 | iteration : 10/600 | train loss : 0.00024100718124469588\n",
      "| epoch : 17 | iteration : 20/600 | train loss : 0.005559415145778095\n",
      "| epoch : 17 | iteration : 30/600 | train loss : 0.009513324589121911\n",
      "| epoch : 17 | iteration : 40/600 | train loss : 2.0582096883126865e-06\n",
      "| epoch : 17 | iteration : 50/600 | train loss : 6.229768225571615e-07\n",
      "| epoch : 17 | iteration : 60/600 | train loss : 0.001538161233457559\n",
      "| epoch : 17 | iteration : 70/600 | train loss : 0.046462755957155204\n",
      "| epoch : 17 | iteration : 80/600 | train loss : 0.0639020306202115\n",
      "| epoch : 17 | iteration : 90/600 | train loss : 0.0048705035478530025\n",
      "| epoch : 17 | iteration : 100/600 | train loss : 9.69396468944922e-06\n",
      "| epoch : 17 | iteration : 110/600 | train loss : 1.2481816488540662e-06\n",
      "| epoch : 17 | iteration : 120/600 | train loss : 0.00015332871957990285\n",
      "| epoch : 17 | iteration : 130/600 | train loss : 0.13938318666668706\n",
      "| epoch : 17 | iteration : 140/600 | train loss : 0.06585034512388109\n",
      "| epoch : 17 | iteration : 150/600 | train loss : 1.7720503453163062e-06\n",
      "| epoch : 17 | iteration : 160/600 | train loss : 1.6518352821282083e-06\n",
      "| epoch : 17 | iteration : 170/600 | train loss : 3.872255766077106e-05\n",
      "| epoch : 17 | iteration : 180/600 | train loss : 0.0002885397266859651\n",
      "| epoch : 17 | iteration : 190/600 | train loss : 7.392934316179432e-06\n",
      "| epoch : 17 | iteration : 200/600 | train loss : 0.07247385540681035\n",
      "| epoch : 17 | iteration : 210/600 | train loss : 0.07365905954930092\n",
      "| epoch : 17 | iteration : 220/600 | train loss : 7.198536783333959e-07\n",
      "| epoch : 17 | iteration : 230/600 | train loss : 0.06405453273653249\n",
      "| epoch : 17 | iteration : 240/600 | train loss : 0.0006180273640638738\n",
      "| epoch : 17 | iteration : 250/600 | train loss : 0.00017227011381850483\n",
      "| epoch : 17 | iteration : 260/600 | train loss : 1.9850783380633147e-06\n",
      "| epoch : 17 | iteration : 270/600 | train loss : 1.8803538333936738e-06\n",
      "| epoch : 17 | iteration : 280/600 | train loss : 8.327221885768384e-07\n",
      "| epoch : 17 | iteration : 290/600 | train loss : 0.02588916454827865\n",
      "| epoch : 17 | iteration : 300/600 | train loss : 0.039899839069819074\n",
      "| epoch : 17 | iteration : 310/600 | train loss : 0.011220380964019288\n",
      "| epoch : 17 | iteration : 320/600 | train loss : 3.148377375760251e-05\n",
      "| epoch : 17 | iteration : 330/600 | train loss : 0.08026681088514605\n",
      "| epoch : 17 | iteration : 340/600 | train loss : 0.10065314853803424\n",
      "| epoch : 17 | iteration : 350/600 | train loss : 2.9370445204034955e-05\n",
      "| epoch : 17 | iteration : 360/600 | train loss : 0.06370467238823933\n",
      "| epoch : 17 | iteration : 370/600 | train loss : 0.0006568718749393704\n",
      "| epoch : 17 | iteration : 380/600 | train loss : 0.07109274489513365\n",
      "| epoch : 17 | iteration : 390/600 | train loss : 0.0026235775830369245\n",
      "| epoch : 17 | iteration : 400/600 | train loss : 0.006793905690732829\n",
      "| epoch : 17 | iteration : 410/600 | train loss : 0.0006491206524154926\n",
      "| epoch : 17 | iteration : 420/600 | train loss : 2.8479360746299724e-05\n",
      "| epoch : 17 | iteration : 430/600 | train loss : 4.706300561372194e-05\n",
      "| epoch : 17 | iteration : 440/600 | train loss : 3.19354513264173e-05\n",
      "| epoch : 17 | iteration : 450/600 | train loss : 0.10996701401651565\n",
      "| epoch : 17 | iteration : 460/600 | train loss : 0.002681048591357404\n",
      "| epoch : 17 | iteration : 470/600 | train loss : 0.02911488902422025\n",
      "| epoch : 17 | iteration : 480/600 | train loss : 2.4488392987474784e-06\n",
      "| epoch : 17 | iteration : 490/600 | train loss : 0.015445488816032717\n",
      "| epoch : 17 | iteration : 500/600 | train loss : 0.18798237867136106\n",
      "| epoch : 17 | iteration : 510/600 | train loss : 0.03302954757997938\n",
      "| epoch : 17 | iteration : 520/600 | train loss : 0.00011622870076536278\n",
      "| epoch : 17 | iteration : 530/600 | train loss : 0.0004868062141908412\n",
      "| epoch : 17 | iteration : 540/600 | train loss : 0.16118149338521587\n",
      "| epoch : 17 | iteration : 550/600 | train loss : 0.000394863291888294\n",
      "| epoch : 17 | iteration : 560/600 | train loss : 0.0003358162442570599\n",
      "| epoch : 17 | iteration : 570/600 | train loss : 0.16097235855478434\n",
      "| epoch : 17 | iteration : 580/600 | train loss : 0.00018235521256624264\n",
      "| epoch : 17 | iteration : 590/600 | train loss : 0.0003566428317582613\n",
      "| epoch 17 | train_accuracy : 0.9930 | test_accuracy : 0.9800 |\n",
      "| epoch : 18 | iteration : 0/600 | train loss : 0.005131664265843867\n",
      "| epoch : 18 | iteration : 10/600 | train loss : 0.21659353268832568\n",
      "| epoch : 18 | iteration : 20/600 | train loss : 2.6313477935946187e-05\n",
      "| epoch : 18 | iteration : 30/600 | train loss : 0.001057699362356849\n",
      "| epoch : 18 | iteration : 40/600 | train loss : 0.0002034874249882246\n",
      "| epoch : 18 | iteration : 50/600 | train loss : 0.07859356393609977\n",
      "| epoch : 18 | iteration : 60/600 | train loss : 9.85887248636082e-06\n",
      "| epoch : 18 | iteration : 70/600 | train loss : 0.001099897677508342\n",
      "| epoch : 18 | iteration : 80/600 | train loss : 1.998670348349705e-05\n",
      "| epoch : 18 | iteration : 90/600 | train loss : 3.193551078051336e-07\n",
      "| epoch : 18 | iteration : 100/600 | train loss : 0.0008611918108431933\n",
      "| epoch : 18 | iteration : 110/600 | train loss : -9.999988830157209e-08\n",
      "| epoch : 18 | iteration : 120/600 | train loss : 0.0028946317639840197\n",
      "| epoch : 18 | iteration : 130/600 | train loss : 0.010700960352943682\n",
      "| epoch : 18 | iteration : 140/600 | train loss : 0.000230974094210943\n",
      "| epoch : 18 | iteration : 150/600 | train loss : 0.00026594955783328973\n",
      "| epoch : 18 | iteration : 160/600 | train loss : 1.941609465244338e-06\n",
      "| epoch : 18 | iteration : 170/600 | train loss : 0.016680637246894055\n",
      "| epoch : 18 | iteration : 180/600 | train loss : 0.13058905664310633\n",
      "| epoch : 18 | iteration : 190/600 | train loss : 0.06886541635644729\n",
      "| epoch : 18 | iteration : 200/600 | train loss : 0.07437515575173245\n",
      "| epoch : 18 | iteration : 210/600 | train loss : 1.1212224180455228e-05\n",
      "| epoch : 18 | iteration : 220/600 | train loss : 0.0035477653790193615\n",
      "| epoch : 18 | iteration : 230/600 | train loss : 8.06405602073247e-07\n",
      "| epoch : 18 | iteration : 240/600 | train loss : 0.034259157193415796\n",
      "| epoch : 18 | iteration : 250/600 | train loss : 0.10206157518295743\n",
      "| epoch : 18 | iteration : 260/600 | train loss : 8.897476283322556e-05\n",
      "| epoch : 18 | iteration : 270/600 | train loss : 0.022869526658999276\n",
      "| epoch : 18 | iteration : 280/600 | train loss : 0.04610749869957417\n",
      "| epoch : 18 | iteration : 290/600 | train loss : 1.8046025878729525e-08\n",
      "| epoch : 18 | iteration : 300/600 | train loss : 6.71439679798682e-07\n",
      "| epoch : 18 | iteration : 310/600 | train loss : 0.10744870039168886\n",
      "| epoch : 18 | iteration : 320/600 | train loss : 0.08167158343082034\n",
      "| epoch : 18 | iteration : 330/600 | train loss : 0.002138287226675365\n",
      "| epoch : 18 | iteration : 340/600 | train loss : 2.544968464821278e-06\n",
      "| epoch : 18 | iteration : 350/600 | train loss : -8.280253907290803e-08\n",
      "| epoch : 18 | iteration : 360/600 | train loss : 0.009584135575339655\n",
      "| epoch : 18 | iteration : 370/600 | train loss : 2.2051712682871942e-08\n",
      "| epoch : 18 | iteration : 380/600 | train loss : 0.000586733840253844\n",
      "| epoch : 18 | iteration : 390/600 | train loss : 0.038247995184277205\n",
      "| epoch : 18 | iteration : 400/600 | train loss : -9.96863875278835e-08\n",
      "| epoch : 18 | iteration : 410/600 | train loss : -9.908559989051956e-08\n",
      "| epoch : 18 | iteration : 420/600 | train loss : 1.6411781252950623e-07\n",
      "| epoch : 18 | iteration : 430/600 | train loss : 0.09771869426560176\n",
      "| epoch : 18 | iteration : 440/600 | train loss : 0.0010330957306971102\n",
      "| epoch : 18 | iteration : 450/600 | train loss : 0.00030883369912170096\n",
      "| epoch : 18 | iteration : 460/600 | train loss : 0.025841595153791596\n",
      "| epoch : 18 | iteration : 470/600 | train loss : 0.005290965008765675\n",
      "| epoch : 18 | iteration : 480/600 | train loss : 0.0003356746859981358\n",
      "| epoch : 18 | iteration : 490/600 | train loss : 1.4043172191779796e-07\n",
      "| epoch : 18 | iteration : 500/600 | train loss : -9.834759786188932e-08\n",
      "| epoch : 18 | iteration : 510/600 | train loss : 0.00016443277795533912\n",
      "| epoch : 18 | iteration : 520/600 | train loss : 0.1622574515044452\n",
      "| epoch : 18 | iteration : 530/600 | train loss : 0.00012199198860053408\n",
      "| epoch : 18 | iteration : 540/600 | train loss : -9.968442780956804e-08\n",
      "| epoch : 18 | iteration : 550/600 | train loss : 0.004593541570536589\n",
      "| epoch : 18 | iteration : 560/600 | train loss : 0.15868963454488288\n",
      "| epoch : 18 | iteration : 570/600 | train loss : 0.033036016342163375\n",
      "| epoch : 18 | iteration : 580/600 | train loss : 0.012758594565173342\n",
      "| epoch : 18 | iteration : 590/600 | train loss : 0.0002167927341084719\n",
      "| epoch 18 | train_accuracy : 1.0000 | test_accuracy : 0.9830 |\n",
      "| epoch : 19 | iteration : 0/600 | train loss : 0.004780927674264936\n",
      "| epoch : 19 | iteration : 10/600 | train loss : 0.00034735427697969045\n",
      "| epoch : 19 | iteration : 20/600 | train loss : 0.06269031395373646\n",
      "| epoch : 19 | iteration : 30/600 | train loss : 2.738947697834977e-06\n",
      "| epoch : 19 | iteration : 40/600 | train loss : 0.00037013940213698415\n",
      "| epoch : 19 | iteration : 50/600 | train loss : -8.06214719748152e-08\n",
      "| epoch : 19 | iteration : 60/600 | train loss : 0.0008529409927630928\n",
      "| epoch : 19 | iteration : 70/600 | train loss : 0.021297651029691528\n",
      "| epoch : 19 | iteration : 80/600 | train loss : 4.273231599098733e-06\n",
      "| epoch : 19 | iteration : 90/600 | train loss : 0.0006712827887404051\n",
      "| epoch : 19 | iteration : 100/600 | train loss : 2.69181647572258e-07\n",
      "| epoch : 19 | iteration : 110/600 | train loss : 0.0012723481374235612\n",
      "| epoch : 19 | iteration : 120/600 | train loss : 0.0006661088659250424\n",
      "| epoch : 19 | iteration : 130/600 | train loss : 0.16118878358730093\n",
      "| epoch : 19 | iteration : 140/600 | train loss : 7.361405447763747e-07\n",
      "| epoch : 19 | iteration : 150/600 | train loss : 0.054011441319804045\n",
      "| epoch : 19 | iteration : 160/600 | train loss : 4.552364220660495e-05\n",
      "| epoch : 19 | iteration : 170/600 | train loss : 0.0005659731852574452\n",
      "| epoch : 19 | iteration : 180/600 | train loss : 0.16118089326010956\n",
      "| epoch : 19 | iteration : 190/600 | train loss : 4.570548497467565e-05\n",
      "| epoch : 19 | iteration : 200/600 | train loss : 2.483553235119766e-05\n",
      "| epoch : 19 | iteration : 210/600 | train loss : 0.0022938190617280154\n",
      "| epoch : 19 | iteration : 220/600 | train loss : 0.008836932711264026\n",
      "| epoch : 19 | iteration : 230/600 | train loss : 0.1495453341566711\n",
      "| epoch : 19 | iteration : 240/600 | train loss : 8.547631400982128e-06\n",
      "| epoch : 19 | iteration : 250/600 | train loss : 0.16098660973592172\n",
      "| epoch : 19 | iteration : 260/600 | train loss : 0.016593648468853896\n",
      "| epoch : 19 | iteration : 270/600 | train loss : 0.0964797160260052\n",
      "| epoch : 19 | iteration : 280/600 | train loss : 2.611259982858757e-07\n",
      "| epoch : 19 | iteration : 290/600 | train loss : 9.837239079908571e-06\n",
      "| epoch : 19 | iteration : 300/600 | train loss : 0.07895356923391597\n",
      "| epoch : 19 | iteration : 310/600 | train loss : 4.768404749470088e-07\n",
      "| epoch : 19 | iteration : 320/600 | train loss : 0.10011622305907285\n",
      "| epoch : 19 | iteration : 330/600 | train loss : 0.0639745364154857\n",
      "| epoch : 19 | iteration : 340/600 | train loss : 0.006701341348073442\n",
      "| epoch : 19 | iteration : 350/600 | train loss : 0.014514291494421103\n",
      "| epoch : 19 | iteration : 360/600 | train loss : 5.2262575371320516e-05\n",
      "| epoch : 19 | iteration : 370/600 | train loss : 2.1523990892320762e-05\n",
      "| epoch : 19 | iteration : 380/600 | train loss : 0.00936861799479382\n",
      "| epoch : 19 | iteration : 390/600 | train loss : 0.09692220093883831\n",
      "| epoch : 19 | iteration : 400/600 | train loss : 5.0781499362963855e-08\n",
      "| epoch : 19 | iteration : 410/600 | train loss : 0.02537502528472507\n",
      "| epoch : 19 | iteration : 420/600 | train loss : 1.1675285964368424e-05\n",
      "| epoch : 19 | iteration : 430/600 | train loss : 0.0012392568573276375\n",
      "| epoch : 19 | iteration : 440/600 | train loss : 1.3213983961670343e-06\n",
      "| epoch : 19 | iteration : 450/600 | train loss : 0.011096173757888848\n",
      "| epoch : 19 | iteration : 460/600 | train loss : -9.896539725474768e-08\n",
      "| epoch : 19 | iteration : 470/600 | train loss : 2.4497467685842713e-06\n",
      "| epoch : 19 | iteration : 480/600 | train loss : 6.215023439056472e-06\n",
      "| epoch : 19 | iteration : 490/600 | train loss : 4.811844543490221e-06\n",
      "| epoch : 19 | iteration : 500/600 | train loss : -9.985365520071001e-08\n",
      "| epoch : 19 | iteration : 510/600 | train loss : 0.0034555901539599083\n",
      "| epoch : 19 | iteration : 520/600 | train loss : 1.0118037188540256e-06\n",
      "| epoch : 19 | iteration : 530/600 | train loss : 6.9540411348332e-05\n",
      "| epoch : 19 | iteration : 540/600 | train loss : 0.00033492464704792707\n",
      "| epoch : 19 | iteration : 550/600 | train loss : 0.04548462496184852\n",
      "| epoch : 19 | iteration : 560/600 | train loss : 0.0036565483568529646\n",
      "| epoch : 19 | iteration : 570/600 | train loss : 0.0010861309754798534\n",
      "| epoch : 19 | iteration : 580/600 | train loss : 0.0016130492054677104\n",
      "| epoch : 19 | iteration : 590/600 | train loss : 1.955034827199768e-06\n",
      "| epoch 19 | train_accuracy : 0.9970 | test_accuracy : 0.9860 |\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "from dataset.mnist import load_mnist\n",
    "from optimizer import Adam\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "lr = 0.01\n",
    "iter_per_epoch = ceil(train_size / batch_size)\n",
    "epochs_num = 20\n",
    "evaluate_sample_num_per_epoch = 1000\n",
    "\n",
    "def shuffle_data(x, t):\n",
    "    idx = np.random.permutation(x.shape[0])  # 무작위 인덱스 생성\n",
    "    return x[idx], t[idx]\n",
    "\n",
    "#def shuffle_data(x, t):\n",
    "#    x_dim = x.shape[1]\n",
    "#    xt_concat = np.concatenate([x, t], axis=1)\n",
    "#    \n",
    "#    np.random.shuffle(xt_concat)\n",
    "#    \n",
    "#    x_shuffled = xt_concat[:, :x_dim]\n",
    "#    t_shuffled = xt_concat[:, x_dim:]\n",
    "#    \n",
    "#    return x_shuffled, t_shuffled\n",
    "\n",
    "optimizer = Adam(lr)\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "train_accuracy = network.accuracy(x_train[:evaluate_sample_num_per_epoch], t_train[:evaluate_sample_num_per_epoch])\n",
    "train_accuracy_list.append(train_accuracy)\n",
    "test_accuracy = network.accuracy(x_test[:evaluate_sample_num_per_epoch], t_test[:evaluate_sample_num_per_epoch])\n",
    "test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "for epoch in range(epochs_num):\n",
    "    x_train_shuffled, t_train_shuffled = shuffle_data(x_train, t_train)\n",
    "    \n",
    "    offset = 0\n",
    "    for i in range(iter_per_epoch):\n",
    "        x_batch = x_train_shuffled[offset:offset+batch_size]\n",
    "        t_batch = t_train_shuffled[offset:offset+batch_size]\n",
    "        \n",
    "        grads = network.gradient(x_batch, t_batch)\n",
    "        if i % 10 == 0:\n",
    "            loss = network.loss(x_batch, t_batch)\n",
    "            print(f\"| epoch : {epoch} | iteration : {i}/{iter_per_epoch} | train loss : {loss} |\")\n",
    "        optimizer.update(network.params, grads)\n",
    "\n",
    "        offset += batch_size\n",
    "    \n",
    "    x_train_eval, t_train_eval = shuffle_data(x_train, t_train)\n",
    "    x_test_eval, t_test_eval = shuffle_data(x_test, t_test)\n",
    "    \n",
    "    train_accuracy = network.accuracy(x_train_eval[:evaluate_sample_num_per_epoch], t_train_eval[:evaluate_sample_num_per_epoch])\n",
    "    test_accuracy = network.accuracy(x_test_eval[:evaluate_sample_num_per_epoch], t_test_eval[:evaluate_sample_num_per_epoch])\n",
    "    \n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    print(f\"| epoch {epoch} | train_accuracy : {train_accuracy:.4f} | test_accuracy : {test_accuracy:.4f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = list(range(epochs_num + 1))  # 초기 accuracy 포함\n",
    "\n",
    "# 전체 라인 그리기 (마커 없이)\n",
    "plt.plot(epochs, train_accuracy_list, label='Train Accuracy', linestyle='--', color='blue')\n",
    "plt.plot(epochs, test_accuracy_list, label='Test Accuracy', linestyle='-', color='orange')\n",
    "\n",
    "\n",
    "plt.title('Accuracy over Epochs (with BatchNorm)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.0, 1.05)\n",
    "plt.xticks(np.arange(0, epochs_num + 1, 10))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0,   1,   2,   3,   4],\n",
       "         [  5,   6,   7,   8,   9],\n",
       "         [ 10,  11,  12,  13,  14],\n",
       "         [ 15,  16,  17,  18,  19],\n",
       "         [ 20,  21,  22,  23,  24]],\n",
       "\n",
       "        [[ 25,  26,  27,  28,  29],\n",
       "         [ 30,  31,  32,  33,  34],\n",
       "         [ 35,  36,  37,  38,  39],\n",
       "         [ 40,  41,  42,  43,  44],\n",
       "         [ 45,  46,  47,  48,  49]],\n",
       "\n",
       "        [[ 50,  51,  52,  53,  54],\n",
       "         [ 55,  56,  57,  58,  59],\n",
       "         [ 60,  61,  62,  63,  64],\n",
       "         [ 65,  66,  67,  68,  69],\n",
       "         [ 70,  71,  72,  73,  74]]],\n",
       "\n",
       "\n",
       "       [[[ 75,  76,  77,  78,  79],\n",
       "         [ 80,  81,  82,  83,  84],\n",
       "         [ 85,  86,  87,  88,  89],\n",
       "         [ 90,  91,  92,  93,  94],\n",
       "         [ 95,  96,  97,  98,  99]],\n",
       "\n",
       "        [[100, 101, 102, 103, 104],\n",
       "         [105, 106, 107, 108, 109],\n",
       "         [110, 111, 112, 113, 114],\n",
       "         [115, 116, 117, 118, 119],\n",
       "         [120, 121, 122, 123, 124]],\n",
       "\n",
       "        [[125, 126, 127, 128, 129],\n",
       "         [130, 131, 132, 133, 134],\n",
       "         [135, 136, 137, 138, 139],\n",
       "         [140, 141, 142, 143, 144],\n",
       "         [145, 146, 147, 148, 149]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(2*3*5*5).reshape(2, 3, 5, 5)\n",
    "x.reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
