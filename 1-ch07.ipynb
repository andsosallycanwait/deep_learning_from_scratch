{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = np.random.randint(0, 256,(10, 3, 28, 28)) # N x C x H x W\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def padding(input_data, pad=0):\n",
    "    _, _, h, w = input_data.shape\n",
    "    padded_data = []\n",
    "    \n",
    "    for data in input_data:\n",
    "        temp_data = []\n",
    "        for c_data in data:\n",
    "            padded_c_data = np.zeros((h + 2*pad, w + 2*pad))\n",
    "            padded_c_data[pad:pad+h, pad:pad+w] = c_data\n",
    "            temp_data.append(list(padded_c_data))\n",
    "\n",
    "        padded_data.append(temp_data)\n",
    "\n",
    "    return np.array(padded_data)\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    input_data = padding(input_data, pad=pad)\n",
    "    \n",
    "    data_h = input_data.shape[2]\n",
    "    data_w = input_data.shape[3]\n",
    "    \n",
    "    cols = []\n",
    "    \n",
    "    end_w_index = data_w - filter_w\n",
    "    end_h_index = data_h - filter_h\n",
    "    \n",
    "    for data in input_data:\n",
    "        for h_index in range(0, end_h_index+1, stride):\n",
    "            for w_index in range(0, end_w_index+1, stride):\n",
    "                feature_map = data[:, h_index:h_index+filter_h, w_index:w_index+filter_w]\n",
    "                cols.append(list(feature_map.reshape(-1,)))\n",
    "\n",
    "    return np.array(cols)\n",
    "\n",
    "def col2im(cols, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    data_n, data_c, data_h, data_w = input_shape\n",
    "    output_h = int((data_h + 2 * pad - filter_h) // stride + 1)\n",
    "    output_w = int((data_w + 2 * pad - filter_w) // stride + 1)\n",
    "\n",
    "    img = np.zeros((data_n, data_c, data_h + 2 * pad, data_w + 2 * pad))\n",
    "\n",
    "    cols_reshaped = cols.reshape(data_n, output_h, output_w, data_c, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    for h_index in range(filter_h):\n",
    "        h_end = h_index + stride * output_h\n",
    "        for w_index in range(filter_w):\n",
    "            w_end = w_index + stride * output_w\n",
    "            img[:, :, h_index:h_end:stride, w_index:w_end:stride] += cols_reshaped[:, :, h_index, w_index, :, :]\n",
    "\n",
    "    return img[:, :, pad:pad + data_h, pad:pad + data_w]\n",
    "\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W              # (N x C x H x W)\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        filter_num, c, filter_h, filter_w = self.W.shape\n",
    "        data_num, _, h, w = x.shape\n",
    "        \n",
    "        output_h = int((h + 2*self.pad - filter_h) / self.stride + 1)\n",
    "        output_w = int((w + 2*self.pad - filter_w) / self.stride + 1)\n",
    "        \n",
    "        cols_data = im2col(x, filter_h, filter_w, stride=self.stride, pad=self.pad)\n",
    "        print(f\"shape of cols_img : {cols_data.shape}\")\n",
    "        cols_W = self.W.reshape(filter_num, -1)\n",
    "        print(f\"shape of cols_W : {cols_W.T.shape}\")\n",
    "        output = np.dot(cols_data, cols_W.T) + self.b\n",
    "        print(f\"shape of output : {output.shape}\")\n",
    "        \n",
    "        return output.T.reshape(data_num, filter_num, output_h, output_w)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        grads = {}\n",
    "        grads[\"W\"] = np.dot(self.X.T, dout)\n",
    "        grads[\"X\"] = np.dot(dout, self.W.T)\n",
    "        grads[\"b\"] = np.sum(dout, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pooling_h=2, pooling_w=2, stride=2, pad=0):\n",
    "        self.pooling_h = pooling_h\n",
    "        self.pooling_w = pooling_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.OH = None\n",
    "        self.OW = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        data_n, data_c, data_h, data_w = x.shape\n",
    "        output_h = int((data_h + 2*self.pad - self.pooling_h) / self.stride + 1)\n",
    "        self.OH = output_h\n",
    "        output_w = int((data_w + 2*self.pad - self.pooling_w) / self.stride + 1)\n",
    "        self.OW = output_w\n",
    "        \n",
    "        cols = im2col(x, self.pooling_h, self.pooling_w, stride=self.stride, pad=self.pad)\n",
    "        cols = cols.reshape(-1, self.pooling_h*self.pooling_w)\n",
    "        pooling_res = np.max(cols, axis=1)\n",
    "        pooling_res = pooling_res.reshape(data_n, output_h, output_w, data_c).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return pooling_res\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1).flatten()\n",
    "\n",
    "        dcol = np.zeros_like(self.col)\n",
    "        dcol[np.arange(len(self.arg_max)), self.arg_max] = dout\n",
    "\n",
    "        dcol = dcol.reshape(self.col.shape[0], -1)\n",
    "        return col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from layer import Affine, ReLU, Softmax_with_Loss\n",
    "\n",
    "def padding(input_data, pad=0):\n",
    "    _, _, h, w = input_data.shape\n",
    "    padded_data = []\n",
    "    \n",
    "    for data in input_data:\n",
    "        temp_data = []\n",
    "        for c_data in data:\n",
    "            padded_c_data = np.zeros((h + 2*pad, w + 2*pad))\n",
    "            padded_c_data[pad:pad+h, pad:pad+w] = c_data\n",
    "            temp_data.append(list(padded_c_data))\n",
    "\n",
    "        padded_data.append(temp_data)\n",
    "\n",
    "    return np.array(padded_data)\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    input_data = padding(input_data, pad=pad)\n",
    "    \n",
    "    data_h = input_data.shape[2]\n",
    "    data_w = input_data.shape[3]\n",
    "    \n",
    "    cols = []\n",
    "    \n",
    "    end_w_index = data_w - filter_w\n",
    "    end_h_index = data_h - filter_h\n",
    "    \n",
    "    for data in input_data:\n",
    "        for h_index in range(0, end_h_index+1, stride):\n",
    "            for w_index in range(0, end_w_index+1, stride):\n",
    "                feature_map = data[:, h_index:h_index+filter_h, w_index:w_index+filter_w]\n",
    "                cols.append(list(feature_map.reshape(-1,)))\n",
    "\n",
    "    return np.array(cols)\n",
    "\n",
    "def col2im(cols, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    data_n, data_c, data_h, data_w = input_shape\n",
    "    output_h = int((data_h + 2 * pad - filter_h) // stride + 1)\n",
    "    output_w = int((data_w + 2 * pad - filter_w) // stride + 1)\n",
    "\n",
    "    img = np.zeros((data_n, data_c, data_h + 2 * pad, data_w + 2 * pad))\n",
    "\n",
    "    cols_reshaped = cols.reshape(data_n, output_h, output_w, data_c, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    for h_index in range(filter_h):\n",
    "        h_end = h_index + stride * output_h\n",
    "        for w_index in range(filter_w):\n",
    "            w_end = w_index + stride * output_w\n",
    "            img[:, :, h_index:h_end:stride, w_index:w_end:stride] += cols_reshaped[:, :, h_index, w_index, :, :]\n",
    "\n",
    "    return img[:, :, pad:pad + data_h, pad:pad + data_w]\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W              # (N x C x H x W)\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        filter_num, filter_c, filter_h, filter_w = self.W.shape\n",
    "        data_num, _, h, w = x.shape\n",
    "        \n",
    "        output_h = int((h + 2*self.pad - filter_h) / self.stride + 1)\n",
    "        output_w = int((w + 2*self.pad - filter_w) / self.stride + 1)\n",
    "        \n",
    "        cols_data = im2col(x, filter_h, filter_w, stride=self.stride, pad=self.pad)\n",
    "        print(f\"shape of cols_img : {cols_data.shape}\")\n",
    "        cols_W = self.W.reshape(filter_num, -1)\n",
    "        print(f\"shape of cols_W : {cols_W.T.shape}\")\n",
    "        output = np.dot(cols_data, cols_W.T) + self.b\n",
    "        print(f\"shape of output : {output.shape}\")\n",
    "        \n",
    "        return output.T.reshape(data_num, filter_num, output_h, output_w)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)  # (N*out_h*out_w, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)  # (C*FH*FW, FN)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)  # (N*out_h*out_w, C*FH*FW)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "        \n",
    "class Pooling:\n",
    "    def __init__(self, pooling_h=2, pooling_w=2, stride=2, pad=0):\n",
    "        self.pooling_h = pooling_h\n",
    "        self.pooling_w = pooling_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.OH = None\n",
    "        self.OW = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        data_n, data_c, data_h, data_w = x.shape\n",
    "        output_h = int((data_h + 2*self.pad - self.pooling_h) / self.stride + 1)\n",
    "        self.OH = output_h\n",
    "        output_w = int((data_w + 2*self.pad - self.pooling_w) / self.stride + 1)\n",
    "        self.OW = output_w\n",
    "        \n",
    "        cols = im2col(x, self.pooling_h, self.pooling_w, stride=self.stride, pad=self.pad)\n",
    "        cols = cols.reshape(-1, self.pooling_h*self.pooling_w)\n",
    "        pooling_res = np.max(cols, axis=1)\n",
    "        pooling_res = pooling_res.reshape(data_n, output_h, output_w, data_c).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return pooling_res\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1).flatten()\n",
    "\n",
    "        dcol = np.zeros_like(self.col)\n",
    "        dcol[np.arange(len(self.arg_max)), self.arg_max] = dout\n",
    "\n",
    "        dcol = dcol.reshape(self.col.shape[0], -1)\n",
    "        return col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "    \n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim, conv_param, hidden_size=100, output_size=10, weight_init_std=0.001):\n",
    "        self.data_c = input_dim[0]\n",
    "        self.data_h = input_dim[1]\n",
    "        self.data_w = input_dim[2]\n",
    "        \n",
    "        self.filter_num = conv_param[\"filter_num\"]\n",
    "        self.filter_h = conv_param[\"filter_size\"][0]\n",
    "        self.filter_w = conv_param[\"filter_size\"][1]\n",
    "        \n",
    "        self.stride = conv_param[\"stride\"]\n",
    "        self.pad = conv_param[\"pad\"]\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_init_std = weight_init_std\n",
    "        \n",
    "        self.conv_output_h = None\n",
    "        self.conv_output_w = None\n",
    "        \n",
    "        self.pooling_output_h = None\n",
    "        self.pooling_output_w = None\n",
    "        \n",
    "        self.params = {}\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        # input : N x C x H x W\n",
    "        # output : N x FN x OH x OW\n",
    "        # OH = (H + 2*pad - filter_h)/stride + 1\n",
    "        # OW = (W + 2*pad - filter_w)/stride + 1\n",
    "        self.params[\"W1\"] = np.random.randn(self.filter_num, self.data_c, self.filter_h, self.filter_w) * self.weight_init_std\n",
    "        self.params[\"B1\"] = np.random.randn(self.filter_num) * self.weight_init_std\n",
    "        self.layers[\"Conv\"] = Convolution(self.params[\"W1\"], self.params[\"B1\"], stride=self.stride, pad=self.pad)\n",
    "        self.conv_output_h = int((self.data_h + 2*self.pad - self.filter_h)/self.stride + 1)\n",
    "        self.conv_output_w = int((self.data_w + 2*self.pad - self.filter_w)/self.stride + 1)\n",
    "        \n",
    "        self.layers[\"ReLU1\"] = ReLU()\n",
    "        \n",
    "        # input : N x FN x OH x OW\n",
    "        # output : N x FN x Pooling_OH x Pooling_OW\n",
    "        # Pooling_OH = (OH + 2*pad - pooling_h)/stride + 1 -> (OH - 2)/2 + 1 \n",
    "        # Pooling_OW = (OW + 2*pad - pooling_h)/stride + 1\n",
    "        pooling_h = 2\n",
    "        pooling_w = 2\n",
    "        pooling_stride = 2\n",
    "        pooling_pad = 0\n",
    "        self.layers[\"Pooling\"] = Pooling(pooling_h, pooling_w, pooling_stride, pooling_pad)\n",
    "        self.pooling_output_h = int((self.conv_output_h + 2*pooling_pad - pooling_h)/pooling_stride + 1)\n",
    "        self.pooling_output_w = int((self.conv_output_w + 2*pooling_pad - pooling_w)/pooling_stride + 1)\n",
    "        \n",
    "        # input : N x FN x OH x OW\n",
    "        # output : N x FN x Pooling_OH x Pooling_OW\n",
    "        # OH = (OH + 2*pad - pooling_h)/stride + 1\n",
    "        # OW = (OW + 2*pad - pooling_h)/stride + 1\n",
    "        feature_size = self.filter_num*self.pooling_output_h*self.pooling_output_w\n",
    "        self.params[\"W2\"] = np.random.randn(feature_size, self.hidden_size) * self.weight_init_std\n",
    "        self.params[\"B2\"] = np.random.randn(self.hidden_size) * self.weight_init_std\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W2\"], self.params[\"B2\"])\n",
    "        self.layers[\"ReLU2\"] = ReLU()\n",
    "        \n",
    "        \n",
    "        self.params[\"W3\"] = np.random.randn(self.hidden_size, self.output_size) * self.weight_init_std\n",
    "        self.params[\"B3\"] = np.random.randn(self.output_size)\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W3\"], self.params[\"B3\"])\n",
    "        \n",
    "        self.last_layer = Softmax_with_Loss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        data_n = x.shape[0]\n",
    "        f = self.layers[\"Conv\"].forward(x)\n",
    "        f = self.layers[\"ReLU1\"].forward(f)\n",
    "        f = self.layers[\"Pooling\"].forward(f)\n",
    "        f = f.reshape(data_n, -1)\n",
    "        h = self.layers[\"Affine1\"].forward(f)\n",
    "        h = self.layers[\"ReLU2\"].forward(h)\n",
    "        y = self.layers[\"Affine2\"].forward(h)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        loss = self.last_layer.forward(y, t)\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        grads = {}\n",
    "    \n",
    "        # Forward pass 저장용\n",
    "        self.loss(x, t)  # 내부적으로 forward 수행됨\n",
    "    \n",
    "        # Backward from Softmax\n",
    "        dout = self.last_layer.backward()\n",
    "    \n",
    "        # Affine2 backward\n",
    "        temp_grads = self.layers[\"Affine2\"].backward(dout)\n",
    "        grads[\"W3\"] = temp_grads[\"W\"]\n",
    "        grads[\"B3\"] = temp_grads[\"b\"]\n",
    "        dout = temp_grads[\"X\"]\n",
    "    \n",
    "        # ReLU2 backward\n",
    "        dout = self.layers[\"ReLU2\"].backward(dout)\n",
    "    \n",
    "        # Affine1 backward\n",
    "        temp_grads = self.layers[\"Affine1\"].backward(dout)\n",
    "        grads[\"W2\"] = temp_grads[\"W\"]\n",
    "        grads[\"B2\"] = temp_grads[\"b\"]\n",
    "        dout = temp_grads[\"X\"]\n",
    "    \n",
    "        # Reshape to N x C x H x W before Pooling backward\n",
    "        dout = dout.reshape(-1, self.filter_num, self.pooling_output_h, self.pooling_output_w)\n",
    "    \n",
    "        # Pooling backward\n",
    "        dout = self.layers[\"Pooling\"].backward(dout)\n",
    "    \n",
    "        # ReLU1 backward\n",
    "        dout = self.layers[\"ReLU1\"].backward(dout)\n",
    "    \n",
    "        # Conv backward\n",
    "        temp_grads = self.layers[\"Conv\"].backward(dout)\n",
    "        grads[\"W1\"] = temp_grads[\"W\"]\n",
    "        grads[\"B1\"] = temp_grads[\"b\"]\n",
    "    \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈 10, 채널 3, 높이 28, 너비 28\n",
    "x = np.random.rand(10, 3, 28, 28)\n",
    "\n",
    "conv_param = {\n",
    "    \"filter_num\": 16,\n",
    "    \"filter_size\": (3, 3),\n",
    "    \"stride\": 1,\n",
    "    \"pad\": 0  # padding 1 → 크기 유지\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SimpleConvNet(input_dim=x.shape, conv_param=conv_param, hidden_size=100, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of cols_img : (6760, 27)\n",
      "shape of cols_W : (27, 16)\n",
      "shape of output : (6760, 16)\n",
      "출력 shape: (10, 10)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "out = network.predict(x)\n",
    "print(\"출력 shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2704)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((10, 16, 13, 13)).reshape(10, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0,   1,   2,   3,   4],\n",
       "         [  5,   6,   7,   8,   9],\n",
       "         [ 10,  11,  12,  13,  14],\n",
       "         [ 15,  16,  17,  18,  19],\n",
       "         [ 20,  21,  22,  23,  24]],\n",
       "\n",
       "        [[ 25,  26,  27,  28,  29],\n",
       "         [ 30,  31,  32,  33,  34],\n",
       "         [ 35,  36,  37,  38,  39],\n",
       "         [ 40,  41,  42,  43,  44],\n",
       "         [ 45,  46,  47,  48,  49]],\n",
       "\n",
       "        [[ 50,  51,  52,  53,  54],\n",
       "         [ 55,  56,  57,  58,  59],\n",
       "         [ 60,  61,  62,  63,  64],\n",
       "         [ 65,  66,  67,  68,  69],\n",
       "         [ 70,  71,  72,  73,  74]]],\n",
       "\n",
       "\n",
       "       [[[ 75,  76,  77,  78,  79],\n",
       "         [ 80,  81,  82,  83,  84],\n",
       "         [ 85,  86,  87,  88,  89],\n",
       "         [ 90,  91,  92,  93,  94],\n",
       "         [ 95,  96,  97,  98,  99]],\n",
       "\n",
       "        [[100, 101, 102, 103, 104],\n",
       "         [105, 106, 107, 108, 109],\n",
       "         [110, 111, 112, 113, 114],\n",
       "         [115, 116, 117, 118, 119],\n",
       "         [120, 121, 122, 123, 124]],\n",
       "\n",
       "        [[125, 126, 127, 128, 129],\n",
       "         [130, 131, 132, 133, 134],\n",
       "         [135, 136, 137, 138, 139],\n",
       "         [140, 141, 142, 143, 144],\n",
       "         [145, 146, 147, 148, 149]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(2*3*5*5).reshape(2, 3, 5, 5)\n",
    "x.reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
