{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "647d175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        self.hs = hs\n",
    "        \n",
    "        self.T = T\n",
    "        self.ar = a.reshape(N, T, 1).repeat(H, axis=2) # N x T x H\n",
    "        t = hs*self.ar # N x T x H\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        return c\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, H = dout.shape\n",
    "        dt = dout.reshape(N, 1, H).repeat(self.T, axis=1) # N x T x H\n",
    "        dhs = dt * self.ar\n",
    "        dar = dt * self.hs\n",
    "        da = np.sum(dar, axis=2)\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe39c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "        self.hs = hs\n",
    "        self.H = H\n",
    "        \n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        t = hs*hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = softmax(s)\n",
    "        \n",
    "        self.hr = hr\n",
    "        self.a = a\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        N, T = da.shape\n",
    "        \n",
    "        ds = (self.a * da) - (np.sum(da * self.a, axis=1, keepdims=True) * self.a) # N x S\n",
    "        dt = ds.reshape(N, T, 1).repeat(self.H, axis=2)\n",
    "        dhs = dt * self.hr\n",
    "        dhr = dt * self.hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf62756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        self.attention_weight = a\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dhs1, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs2, dh = self.attention_weight_layer.backward(da)\n",
    "        \n",
    "        dhs = dhs1 + dhs2\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517e7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "    def forward(self, hs_encoder, hs_decoder):\n",
    "        # hs_encoder : N x T x H\n",
    "        # hs_decoder : N x T x H\n",
    "        _, T, _ = hs_encoder.shape\n",
    "        \n",
    "        out = []\n",
    "        \n",
    "        for i in range(T):\n",
    "            attention_layer = Attention()\n",
    "            res = attention_layer.forward(hs_encoder, hs_decoder[:, i, :])\n",
    "            out.append(res)\n",
    "            self.attention_weights.append(attention_layer.attention_weight)\n",
    "            \n",
    "            self.layers.append(attention_layer)\n",
    "        \n",
    "        return np.array(out).transpose(1, 0, 2) # N x T x H\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        _, T, _ = dout.shape\n",
    "        \n",
    "        dhs_encoder = np.zeros_like(dout)\n",
    "        dhs_decoder = np.zeros_like(dout)\n",
    "        \n",
    "        for i in range(T-1, -1, -1):\n",
    "            dout_cur = dout[:, i, :]\n",
    "            dhs, dh = self.layers[i].backward(dout_cur)\n",
    "            dhs_decoder[:, i, :] = dh\n",
    "            dhs_encoder += dhs\n",
    "        \n",
    "        return dhs_encoder, dhs_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import Encoder, TimeEmbedding, TimeAffine, TimeLSTM\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        out = xs\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        \n",
    "        self.hs = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = dhs\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        self.V = vocab_size\n",
    "        self.D = wordvec_size\n",
    "        self.H = hidden_size\n",
    "        \n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "        self.W_embed = (0.01 * np.random.randn(self.V, self.D)).astype('float32')\n",
    "        self.W_x_lstm = ((1 / np.sqrt(self.D)) * np.random.randn(self.D, 4*self.H)).astype('float32')\n",
    "        self.W_h_lstm = ((1 / np.sqrt(self.H)) * np.random.randn(self.H, 4*self.H)).astype('float32')\n",
    "        self.b_lstm = np.zeros(4*self.H).astype('float32')\n",
    "        self.W_affine = ((1 / np.sqrt(2*self.H)) * np.random.randn(2*self.H, self.V)).astype('float32')\n",
    "        self.b_affine = np.zeros(self.V).astype('float32')\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(TimeEmbedding(self.W_embed))\n",
    "        self.layers.append(TimeLSTM(self.W_x_lstm, self.W_h_lstm, self.b_lstm))\n",
    "        self.layers.append(TimeAttention())\n",
    "        self.layers.append(TimeAffine(self.W_affine, self.b_affine))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            for param, grad in zip(layer.params, layer.grads):\n",
    "                self.params.append(param)\n",
    "                self.grads.append(grad)\n",
    "    \n",
    "    def forward(self, xs, hs):\n",
    "        self.layers[1].h = hs[:, -1, :]\n",
    "        \n",
    "        word_vecs = self.layers[0].forward(xs) # N x T x D\n",
    "        \n",
    "        hs_decoder = self.layers[1].forward(word_vecs)  # N x T x H\n",
    "        \n",
    "        c = self.layers[2].forward(hs, hs_decoder)    # N x T x H\n",
    "        \n",
    "        concat_vec = np.concatenate([c, hs_decoder], axis=2) # N x T x 2H\n",
    "        logit = self.layers[3].forward(concat_vec) # N x T x V\n",
    "        \n",
    "        return logit\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dconcat_vec = self.layers[3].backward(dout)\n",
    "        dc = dconcat_vec[:, :, :self.H]\n",
    "        dhs_decoder1 = dconcat_vec[:, :, self.H:]\n",
    "        \n",
    "        dhs1, dhs_decoder2 = self.layers[2].backward(dc) # dhs1 : N x T x H\n",
    "        \n",
    "        dword_vecs = self.layers[1].backward(dhs_decoder1 + dhs_decoder2)\n",
    "        \n",
    "        self.layers[0].backward(dword_vecs)\n",
    "        \n",
    "        dhs2 = self.layers[1].dh # dhs2 : N x H\n",
    "        dhs = np.zeros_like(dhs1)\n",
    "        dhs[:, -1, :] = dhs2\n",
    "        dhs += dhs1\n",
    "        \n",
    "        return dhs\n",
    "    \n",
    "    def generate(self, hs, start_id, sample_size):\n",
    "        self.layers[1].h = hs[:, -1, :] # N x H\n",
    "        \n",
    "        input = start_id\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        while len(word_ids) < sample_size:\n",
    "            input = np.array(input, dtype=np.int32).reshape(1, 1)\n",
    "            \n",
    "            word_vecs = self.layers[0].forward(input) # N x T x D\n",
    "            \n",
    "            hs_decoder = self.layers[1].forward(word_vecs)  # N x T x H\n",
    "            \n",
    "            c = self.layers[2].forward(hs, hs_decoder)    # N x T x H\n",
    "            \n",
    "            concat_vec = np.concatenate([c, hs_decoder], axis=2) # N x T x 2H\n",
    "            logit = self.layers[3].forward(concat_vec) # N x T x V\n",
    "\n",
    "            sample = int(np.argmax(logit.flatten()))\n",
    "\n",
    "            word_ids.append(sample)\n",
    "            input = sample\n",
    "\n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c375c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import Seq2seq, TimeSoftmaxWithLoss\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        self.V = vocab_size\n",
    "        self.D = wordvec_size\n",
    "        self.H = hidden_size\n",
    "        \n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "        self.encoder = AttentionEncoder(vocab_size, wordvec_size, hidden_size)\n",
    "        self.decoder = AttentionDecoder(vocab_size, wordvec_size, hidden_size)\n",
    "        self.softmax_with_loss = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        for param, grad in zip(self.encoder.params, self.encoder.grads):\n",
    "            self.params.append(param)\n",
    "            self.grads.append(grad)\n",
    "            \n",
    "        for param, grad in zip(self.decoder.params, self.decoder.grads):\n",
    "            self.params.append(param)\n",
    "            self.grads.append(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5fa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
