{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9d67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, W_x, W_h, b):\n",
    "        self.params = [W_x, W_h, b]\n",
    "        self.grads = [np.zeros_like(W_x), np.zeros_like(W_h), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        # x, W_x : N x D, D x H\n",
    "        # h, W_h : N x H, H x H\n",
    "        # b : N x 1\n",
    "        W_x, W_h, b = self.params\n",
    "        \n",
    "        x_proj = np.dot(x, W_x)         # N x H\n",
    "        h_proj = np.dot(h_prev, W_h)    # N x H\n",
    "        proj_sum = x_proj + h_proj      # N x H\n",
    "        h = proj_sum + b                # N x H\n",
    "        h_next = np.tanh(h)             # N x H\n",
    "        \n",
    "        self.cache = (x, h_prev, h, h_next)\n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        W_x, W_h, b = self.params\n",
    "        x, h_prev, h, h_next = self.cache\n",
    "        \n",
    "        dtanh = dh_next * (1 - np.tanh(h)**2) # N x H\n",
    "        \n",
    "        db = np.sum(dtanh, axis=0) # 1 x H\n",
    "        dproj_sum = dtanh # N x H\n",
    "        \n",
    "        dx_proj = dproj_sum # N x H\n",
    "        dh_proj = dproj_sum # N x H\n",
    "        \n",
    "        dW_x = np.dot(x.T, dx_proj) # D x H\n",
    "        dx = np.dot(dx_proj, W_x.T) # N x D\n",
    "        \n",
    "        dW_h = np.dot(h_prev.T, dh_proj)\n",
    "        dh_prev = np.dot(dh_proj, W_h.T) # N x H\n",
    "        \n",
    "        self.grads[0][...] = dW_x\n",
    "        self.grads[1][...] = dW_h\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4113f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, W_x, W_h, b, stateful=True):\n",
    "        self.params = [W_x, W_h, b]\n",
    "        self.grads = [np.zeros_like(W_x), np.zeros_like(W_h), np.zeros_like(b)]\n",
    "        self.stateful = stateful\n",
    "        \n",
    "        self.h, self.dh = None, None\n",
    "        self.layers = []\n",
    "        \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        W_x, W_h, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        _, H = W_h.shape\n",
    "        \n",
    "        hs = []\n",
    "        \n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='float')\n",
    "            \n",
    "        for i in range(T):\n",
    "            rnn = RNN(W_x, W_h, b)\n",
    "            self.h = rnn.forward(xs[:, i, :], self.h)\n",
    "            hs.append(self.h)\n",
    "            \n",
    "            #hs[:, i, :] = self.h\n",
    "            self.layers.append(rnn)\n",
    "        \n",
    "        return np.array(hs).transpose(1, 0, 2)\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        _, T, _ = dhs.shape\n",
    "        \n",
    "        dxs = []\n",
    "        dh_next = 0\n",
    "        \n",
    "        for i in range(len(self.grads)):\n",
    "            self.grads[i][...] = 0\n",
    "            \n",
    "        for i in range(T-1, -1, -1):\n",
    "            dh_cur = dhs[:, i, :]\n",
    "            dx, dh_prev = self.layers[i].backward(dh_cur + dh_next)\n",
    "            dxs.append(dx)\n",
    "            \n",
    "            for j, grad in enumerate(self.layers[i].grads):\n",
    "                self.grads[j] += grad\n",
    "                \n",
    "            dh_next = dh_prev\n",
    "        \n",
    "        self.dh = dh_prev\n",
    "        \n",
    "        return np.flip(np.array(dxs), axis=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b965ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import Affine, Embedding, Softmax_with_Loss\n",
    "\n",
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = []\n",
    "\n",
    "    def forward(self, ws):\n",
    "        _, T = ws.shape\n",
    "        \n",
    "        xs = []\n",
    "        for i in range(T):\n",
    "            W = self.params[0]\n",
    "            embed_layer = Embedding(W)\n",
    "            embedding = embed_layer.forward(ws[:, i])\n",
    "            xs.append(embedding)\n",
    "            \n",
    "            self.layers.append(embed_layer)\n",
    "        \n",
    "        return np.array(xs).transpose(1, 0, 2) # T x N x D -> N x T x D\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "        self.grads[0][...] = 0  # 기존 gradient 초기화\n",
    "\n",
    "        for t in range(T):\n",
    "            self.layers[t].backward(dout[:, t, :])\n",
    "            self.grads[0] += self.layers[t].grads[0]\n",
    "    \n",
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.layers = []\n",
    "    \n",
    "    def forward(self, hs):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ys = []\n",
    "        for i in range(T):\n",
    "            W, b = self.params\n",
    "            affine_layer = Affine(W, b)\n",
    "            logit = affine_layer.forward(hs[:, i, :])\n",
    "            ys.append(logit)\n",
    "            \n",
    "            self.layers.append(affine_layer)\n",
    "        \n",
    "        return np.array(ys).transpose(1, 0, 2) # T x N x L -> N x T x L\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, V = dout.shape\n",
    "        \n",
    "        dys = []\n",
    "        \n",
    "        for i in range(len(self.grads)):\n",
    "            self.grads[i][...] = 0\n",
    "            \n",
    "        for i in range(T-1, -1, -1):\n",
    "            dy = self.layers[i].backward(dout[:, i, :])\n",
    "            dys.append(dy)\n",
    "            \n",
    "            for j, grad in enumerate(self.layers[i].grads):\n",
    "                self.grads[j] += grad\n",
    "        \n",
    "        return np.flip(np.array(dys), axis=0).transpose(1, 0, 2)\n",
    "    \n",
    "\n",
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.loss = 0\n",
    "        self.layers = []\n",
    "        self.ts = None  \n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "        self.ts = ts\n",
    "        self.loss = 0\n",
    "        self.layers = []\n",
    "\n",
    "        for i in range(T):\n",
    "            layer = Softmax_with_Loss()\n",
    "            loss = layer.forward(xs[:,i, :], ts[:, i])\n",
    "            self.loss += loss\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.loss /= T\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        N, T = self.ts.shape\n",
    "        dxs = []\n",
    "\n",
    "        dout /= T  # 평균에 맞춰서 스케일링\n",
    "\n",
    "        for i in range(T-1, -1, -1):\n",
    "            dx = self.layers[i].backward(dout)  # (N, V)\n",
    "            dxs.append(dx)\n",
    "\n",
    "        # (T, N, V) → (N, T, V)\n",
    "        return np.flip(np.array(dxs), axis=0).transpose(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ddafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, sw):\n",
    "        W = self.params[0]\n",
    "        self.idx = sw\n",
    "        out = W[sw]  # numpy indexing: (N, T) → (N, T, D)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW = self.grads[0]\n",
    "        dW[...] = 0\n",
    "\n",
    "        if self.idx.ndim == 2:\n",
    "            N, T = self.idx.shape\n",
    "            idx = self.idx.reshape(N * T)\n",
    "            dout = dout.reshape(N * T, -1)\n",
    "        else:\n",
    "            idx = self.idx\n",
    "\n",
    "        for i, word_id in enumerate(idx):\n",
    "            dW[word_id] += dout[i]\n",
    "\n",
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, H = x.shape\n",
    "        W, b = self.params\n",
    "        out = np.dot(x.reshape(N * T, H), W) + b  # (N*T, V)\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)              # (N, T, V)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, H = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N * T, -1)  # (N*T, V)\n",
    "        x_reshaped = x.reshape(N * T, H)\n",
    "\n",
    "        dW = np.dot(x_reshaped.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dx = np.dot(dout, W.T).reshape(N, T, H)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.loss = 0\n",
    "        self.layers = []\n",
    "        self.ts = None  \n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "        self.ts = ts\n",
    "        self.loss = 0\n",
    "        self.layers = []\n",
    "\n",
    "        for i in range(T):\n",
    "            layer = Softmax_with_Loss()\n",
    "            loss = layer.forward(xs[:,i, :], ts[:, i])\n",
    "            self.loss += loss\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.loss /= T\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        N, T = self.ts.shape\n",
    "        dxs = []\n",
    "\n",
    "        dout /= T  # 평균에 맞춰서 스케일링\n",
    "\n",
    "        for i in range(T-1, -1, -1):\n",
    "            dx = self.layers[i].backward(dout)  # (N, V)\n",
    "            dxs.append(dx)\n",
    "\n",
    "        # (T, N, V) → (N, T, V)\n",
    "        return np.flip(np.array(dxs), axis=0).transpose(1, 0, 2)\n",
    "    \n",
    "class SimpleRNNLM:\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size):\n",
    "        self.D = embedding_size\n",
    "        self.H = hidden_size\n",
    "        self.V = vocab_size\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.logit = None\n",
    "\n",
    "        # 안정적인 초기화\n",
    "        self.W_embed = 0.01 * np.random.randn(self.V, self.D)\n",
    "        self.W_x_rnn = (1 / np.sqrt(self.D)) * np.random.randn(self.D, self.H)\n",
    "        self.W_h_rnn = (1 / np.sqrt(self.H)) * np.random.randn(self.H, self.H)\n",
    "        self.b_rnn = np.zeros(self.H, dtype='float32')\n",
    "        self.W_affine = (1 / np.sqrt(self.H)) * np.random.randn(self.H, self.V)\n",
    "        self.b_affine = np.zeros(self.V, dtype='float32')\n",
    "\n",
    "        # 레이어 구성\n",
    "        self.layers = []\n",
    "        self.layers.append(TimeEmbedding(self.W_embed))\n",
    "        self.layers.append(TimeRNN(self.W_x_rnn, self.W_h_rnn, self.b_rnn))\n",
    "        self.layers.append(TimeAffine(self.W_affine, self.b_affine))\n",
    "        self.last_layer = TimeSoftmaxWithLoss()\n",
    "\n",
    "        for layer in self.layers:\n",
    "            for param, grad in zip(layer.params, layer.grads):\n",
    "                self.params.append(param)\n",
    "                self.grads.append(grad)\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        out = xs\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        self.logit = out\n",
    "        loss = self.last_layer.forward(out, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        dout = self.last_layer.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.layers[1].reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba073305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import Affine, Embedding, Softmax_with_Loss\n",
    "\n",
    "#class TimeEmbedding:\n",
    "#    def __init__(self, W):\n",
    "#        self.params = [W]\n",
    "#        self.grads = [np.zeros_like(W)]\n",
    "#        self.layers = []\n",
    "#\n",
    "#    def forward(self, ws):\n",
    "#        _, T = ws.shape\n",
    "#        \n",
    "#        xs = []\n",
    "#        for i in range(T):\n",
    "#            W = self.params[0]\n",
    "#            embed_layer = Embedding(W)\n",
    "#            embedding = embed_layer.forward(ws[:, i])\n",
    "#            xs.append(embedding)\n",
    "#            \n",
    "#            self.layers.append(embed_layer)\n",
    "#        \n",
    "#        return np.array(xs).transpose(1, 0, 2) # T x N x D -> N x T x D\n",
    "#    \n",
    "#    def backward(self, dout):\n",
    "#        N, T, D = dout.shape\n",
    "#        self.grads[0][...] = 0  # 기존 gradient 초기화\n",
    "#\n",
    "#        for t in range(T):\n",
    "#            self.layers[t].backward(dout[:, t, :])\n",
    "#            self.grads[0] += self.layers[t].grads[0]\n",
    "            \n",
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W = self.params[0]\n",
    "        self.idx = idx\n",
    "        out = W[idx]  # (N, T, D) ← 자동으로 벡터화됨\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW = self.grads[0]\n",
    "        dW[...] = 0\n",
    "\n",
    "        if self.idx.ndim == 2:\n",
    "            N, T = self.idx.shape\n",
    "            dout = dout.reshape(N * T, -1)\n",
    "            idx = self.idx.reshape(N * T)\n",
    "        else:\n",
    "            idx = self.idx\n",
    "\n",
    "        for i, word_id in enumerate(idx):\n",
    "            dW[word_id] += dout[i]\n",
    "    \n",
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.layers = []\n",
    "    \n",
    "    def forward(self, hs):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ys = []\n",
    "        for i in range(T):\n",
    "            W, b = self.params\n",
    "            affine_layer = Affine(W, b)\n",
    "            logit = affine_layer.forward(hs[:, i, :])\n",
    "            ys.append(logit)\n",
    "            \n",
    "            self.layers.append(affine_layer)\n",
    "        \n",
    "        return np.array(ys).transpose(1, 0, 2) # T x N x L -> N x T x L\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, V = dout.shape\n",
    "        \n",
    "        dys = []\n",
    "        \n",
    "        for i in range(len(self.grads)):\n",
    "            self.grads[i][...] = 0\n",
    "            \n",
    "        for i in range(T-1, -1, -1):\n",
    "            dy = self.layers[i].backward(dout[:, i, :])\n",
    "            dys.append(dy)\n",
    "            \n",
    "            for j, grad in enumerate(self.layers[i].grads):\n",
    "                self.grads[j] += grad\n",
    "        \n",
    "        return np.flip(np.array(dys), axis=0).transpose(1, 0, 2)\n",
    "    \n",
    "\n",
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.loss = 0\n",
    "        self.layers = []\n",
    "        self.ts = None  \n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "        self.ts = ts\n",
    "        self.loss = 0\n",
    "        self.layers = []\n",
    "\n",
    "        for i in range(T):\n",
    "            layer = Softmax_with_Loss()\n",
    "            loss = layer.forward(xs[:,i, :], ts[:, i])\n",
    "            self.loss += loss\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.loss /= T\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        N, T = self.ts.shape\n",
    "        dxs = []\n",
    "\n",
    "        dout /= T  # 평균에 맞춰서 스케일링\n",
    "\n",
    "        for i in range(T-1, -1, -1):\n",
    "            dx = self.layers[i].backward(dout)  # (N, V)\n",
    "            dxs.append(dx)\n",
    "\n",
    "        # (T, N, V) → (N, T, V)\n",
    "        return np.flip(np.array(dxs), axis=0).transpose(1, 0, 2)\n",
    "    \n",
    "class SimpleRNNLM:\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size):\n",
    "        self.D = embedding_size\n",
    "        self.H = hidden_size\n",
    "        self.V = vocab_size\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.logit = None\n",
    "        \n",
    "        self.W_embed = 0.01 * np.random.randn(self.V, self.D)\n",
    "        \n",
    "        self.W_x_rnn = (1/np.sqrt(self.D)) * np.random.randn(self.D, self.H)\n",
    "        self.W_h_rnn = (1/np.sqrt(self.H)) * np.random.randn(self.H, self.H)\n",
    "        self.b_rnn = np.zeros(self.H, dtype='float')\n",
    "        \n",
    "        self.W_affine = (1/np.sqrt(self.H)) * np.random.randn(self.H, self.V)\n",
    "        self.b_affine = np.zeros(self.V, dtype='float')\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(TimeEmbedding(self.W_embed))\n",
    "        self.layers.append(TimeRNN(self.W_x_rnn, self.W_h_rnn, self.b_rnn))\n",
    "        self.layers.append(TimeAffine(self.W_affine, self.b_affine))\n",
    "        self.last_layer = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            for i in range(len(layer.params)):\n",
    "                self.params.append(layer.params[i])\n",
    "                self.grads.append(layer.grads[i])\n",
    "                \n",
    "    def forward(self, xs, ts):\n",
    "        out = xs\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        self.logit = out\n",
    "\n",
    "        loss = self.last_layer.forward(out, ts)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        dout = self.last_layer.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.layers[1].reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bc61ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 | loss 5.9793\n",
      "| epoch 2 | loss 5.8982\n",
      "| epoch 3 | loss 5.8440\n",
      "| epoch 4 | loss 5.8005\n",
      "| epoch 5 | loss 5.7635\n",
      "| epoch 6 | loss 5.7311\n",
      "| epoch 7 | loss 5.7024\n",
      "| epoch 8 | loss 5.6765\n",
      "| epoch 9 | loss 5.6530\n",
      "| epoch 10 | loss 5.6316\n",
      "| epoch 11 | loss 5.6119\n",
      "| epoch 12 | loss 5.5938\n",
      "| epoch 13 | loss 5.5772\n",
      "| epoch 14 | loss 5.5619\n",
      "| epoch 15 | loss 5.5478\n",
      "| epoch 16 | loss 5.5348\n",
      "| epoch 17 | loss 5.5228\n",
      "| epoch 18 | loss 5.5117\n",
      "| epoch 19 | loss 5.5014\n",
      "| epoch 20 | loss 5.4919\n",
      "| epoch 21 | loss 5.4831\n",
      "| epoch 22 | loss 5.4749\n",
      "| epoch 23 | loss 5.4673\n",
      "| epoch 24 | loss 5.4601\n",
      "| epoch 25 | loss 5.4534\n",
      "| epoch 26 | loss 5.4472\n",
      "| epoch 27 | loss 5.4413\n",
      "| epoch 28 | loss 5.4357\n",
      "| epoch 29 | loss 5.4305\n",
      "| epoch 30 | loss 5.4256\n",
      "| epoch 31 | loss 5.4209\n",
      "| epoch 32 | loss 5.4165\n",
      "| epoch 33 | loss 5.4124\n",
      "| epoch 34 | loss 5.4084\n",
      "| epoch 35 | loss 5.4047\n",
      "| epoch 36 | loss 5.4011\n",
      "| epoch 37 | loss 5.3978\n",
      "| epoch 38 | loss 5.3946\n",
      "| epoch 39 | loss 5.3916\n",
      "| epoch 40 | loss 5.3888\n",
      "| epoch 41 | loss 5.3860\n",
      "| epoch 42 | loss 5.3835\n",
      "| epoch 43 | loss 5.3810\n",
      "| epoch 44 | loss 5.3787\n",
      "| epoch 45 | loss 5.3765\n",
      "| epoch 46 | loss 5.3744\n",
      "| epoch 47 | loss 5.3724\n",
      "| epoch 48 | loss 5.3705\n",
      "| epoch 49 | loss 5.3687\n",
      "| epoch 50 | loss 5.3670\n",
      "| epoch 51 | loss 5.3654\n",
      "| epoch 52 | loss 5.3638\n",
      "| epoch 53 | loss 5.3624\n",
      "| epoch 54 | loss 5.3610\n",
      "| epoch 55 | loss 5.3596\n",
      "| epoch 56 | loss 5.3583\n",
      "| epoch 57 | loss 5.3571\n",
      "| epoch 58 | loss 5.3560\n",
      "| epoch 59 | loss 5.3549\n",
      "| epoch 60 | loss 5.3538\n",
      "| epoch 61 | loss 5.3528\n",
      "| epoch 62 | loss 5.3519\n",
      "| epoch 63 | loss 5.3510\n",
      "| epoch 64 | loss 5.3501\n",
      "| epoch 65 | loss 5.3493\n",
      "| epoch 66 | loss 5.3485\n",
      "| epoch 67 | loss 5.3478\n",
      "| epoch 68 | loss 5.3471\n",
      "| epoch 69 | loss 5.3464\n",
      "| epoch 70 | loss 5.3458\n",
      "| epoch 71 | loss 5.3452\n",
      "| epoch 72 | loss 5.3447\n",
      "| epoch 73 | loss 5.3441\n",
      "| epoch 74 | loss 5.3436\n",
      "| epoch 75 | loss 5.3432\n",
      "| epoch 76 | loss 5.3427\n",
      "| epoch 77 | loss 5.3423\n",
      "| epoch 78 | loss 5.3419\n",
      "| epoch 79 | loss 5.3416\n",
      "| epoch 80 | loss 5.3412\n",
      "| epoch 81 | loss 5.3409\n",
      "| epoch 82 | loss 5.3406\n",
      "| epoch 83 | loss 5.3403\n",
      "| epoch 84 | loss 5.3401\n",
      "| epoch 85 | loss 5.3399\n",
      "| epoch 86 | loss 5.3397\n",
      "| epoch 87 | loss 5.3395\n",
      "| epoch 88 | loss 5.3393\n",
      "| epoch 89 | loss 5.3391\n",
      "| epoch 90 | loss 5.3390\n",
      "| epoch 91 | loss 5.3389\n",
      "| epoch 92 | loss 5.3388\n",
      "| epoch 93 | loss 5.3387\n",
      "| epoch 94 | loss 5.3386\n",
      "| epoch 95 | loss 5.3386\n",
      "| epoch 96 | loss 5.3386\n",
      "| epoch 97 | loss 5.3385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     time_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch_x, batch_t)\n\u001b[1;32m---> 62\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mparams, model\u001b[38;5;241m.\u001b[39mgrads):\n\u001b[0;32m     64\u001b[0m     param \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad\n",
      "Cell \u001b[1;32mIn[7], line 173\u001b[0m, in \u001b[0;36mSimpleRNNLM.backward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m--> 173\u001b[0m     dout \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dout\n",
      "Cell \u001b[1;32mIn[7], line 86\u001b[0m, in \u001b[0;36mTimeAffine.backward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads[i][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 86\u001b[0m     dy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     dys\u001b[38;5;241m.\u001b[39mappend(dy)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mgrads):\n",
      "File \u001b[1;32mc:\\Users\\andsosallycanwait00\\deep_learning_from_scratch\\layer.py:41\u001b[0m, in \u001b[0;36mAffine.backward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dout):\n\u001b[0;32m     40\u001b[0m     W, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dout, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     43\u001b[0m     dx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dout, W\u001b[38;5;241m.\u001b[39mT)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import ptb\n",
    "from optimizer import SGD\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100\n",
    "time_size = 5\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus = corpus[:1000]\n",
    "#vocab_size = len(word_to_id)\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "def create_batches(xs, ts, batch_size, time_size):\n",
    "    data_size = len(xs)\n",
    "    jump = data_size // batch_size\n",
    "    offsets = [i * jump for i in range(batch_size)]\n",
    "    batch_x = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "    batch_t = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "\n",
    "    time_idx = 0\n",
    "    for t in range(time_size):\n",
    "        for i, offset in enumerate(offsets):\n",
    "            batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "            batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "        time_idx += 1\n",
    "    return batch_x, batch_t\n",
    "\n",
    "model = SimpleRNNLM(wordvec_size, hidden_size, vocab_size)\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "loss_list = []\n",
    "data_size = len(xs)\n",
    "jump = data_size // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    total_loss = 0\n",
    "    loss_count = 0\n",
    "    time_idx = 0\n",
    "\n",
    "    for _ in range(data_size // (batch_size * time_size)):\n",
    "        batch_x = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        batch_t = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                idx = (offset + time_idx) % data_size\n",
    "                batch_x[i, t] = xs[idx]\n",
    "                batch_t[i, t] = ts[idx]\n",
    "            time_idx += 1\n",
    "\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        for param, grad in zip(model.params, model.grads):\n",
    "            param -= lr * grad\n",
    "\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    avg_loss = total_loss / loss_count\n",
    "    print(f\"| epoch {epoch + 1} | loss {avg_loss:.4f}\")\n",
    "    loss_list.append(avg_loss)\n",
    "\n",
    "# 시각화\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"SimpleRNNLM Loss\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7add31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26d947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_embed shape: (10000, 100)\n",
      "max index in batch_x: 386\n",
      "max index in batch_t: 387\n"
     ]
    }
   ],
   "source": [
    "print(\"W_embed shape:\", model.W_embed.shape)\n",
    "print(\"max index in batch_x:\", np.max(batch_x))\n",
    "print(\"max index in batch_t:\", np.max(batch_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91660f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3, 4],\n",
       "        [1, 2, 3, 4],\n",
       "        [1, 2, 3, 4]],\n",
       "\n",
       "       [[5, 6, 7, 8],\n",
       "        [5, 6, 7, 8],\n",
       "        [5, 6, 7, 8]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.transpose(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[2, 3, 4], [1, 2, 3]])\n",
    "params = [W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = np.zeros_like(W)\n",
    "dW = grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb85c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = np.array([[3, 2, 1, 5, 6, 8], [2, 1, 3, 5, 6, 3]])\n",
    "ws[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cebf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14]],\n",
       "\n",
       "       [[15, 16, 17, 18, 19],\n",
       "        [20, 21, 22, 23, 24],\n",
       "        [25, 26, 27, 28, 29]],\n",
       "\n",
       "       [[30, 31, 32, 33, 34],\n",
       "        [35, 36, 37, 38, 39],\n",
       "        [40, 41, 42, 43, 44]],\n",
       "\n",
       "       [[45, 46, 47, 48, 49],\n",
       "        [50, 51, 52, 53, 54],\n",
       "        [55, 56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3\n",
    "T = 4\n",
    "D = 5\n",
    "\n",
    "ws = np.arange(3*4*5).reshape(4, 3, 5) # T x N x D\n",
    "ws.transpose(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c8b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6,  6,  6,  6],\n",
       "        [12, 12, 12, 12]],\n",
       "\n",
       "       [[18, 18, 18, 18],\n",
       "        [24, 24, 24, 24]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\n",
    "W = np.array([[2, 2, 2, 2], [1, 1, 1, 1], [3, 3, 3, 3]])\n",
    "np.dot(x, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421c1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.66157558e-03,  6.40767763e-03, -1.53851598e-03,\n",
       "        -7.62095714e-03, -3.77932556e-03,  3.95013162e-03,\n",
       "        -5.06714142e-03,  8.78915853e-03,  4.12920235e-03,\n",
       "        -3.44054253e-03],\n",
       "       [-1.62109197e-02,  3.86510595e-03, -1.13390564e-02,\n",
       "        -1.19351633e-02, -1.33026933e-02, -1.16146051e-02,\n",
       "        -1.25979175e-02,  9.84825386e-03, -2.79067706e-02,\n",
       "         6.10069767e-03],\n",
       "       [ 1.36689429e-02, -1.27147275e-02, -6.11480173e-03,\n",
       "        -4.54789145e-03, -5.93886995e-03,  5.12244930e-03,\n",
       "         7.61207071e-03, -3.11376572e-03, -1.79358873e-02,\n",
       "        -1.97138611e-02],\n",
       "       [-9.24966056e-03, -1.32202243e-02, -1.17357047e-02,\n",
       "         9.48632385e-03,  1.98334097e-03, -4.93737094e-03,\n",
       "         8.23412158e-03, -4.11562310e-03,  1.25179984e-02,\n",
       "         1.24668987e-02],\n",
       "       [ 5.31447480e-04, -2.63412201e-02,  1.22413155e-02,\n",
       "        -4.57614858e-03,  1.15673417e-02, -1.08896187e-02,\n",
       "         4.85770222e-03,  2.40333865e-03, -9.06141544e-03,\n",
       "        -8.50458759e-03],\n",
       "       [-5.04490849e-06,  2.11039665e-03,  9.20683977e-04,\n",
       "         1.35528219e-02, -4.81458507e-03, -5.39214747e-03,\n",
       "         6.48157686e-03,  2.64353441e-03, -3.60100729e-03,\n",
       "         7.23434633e-03],\n",
       "       [ 1.70951294e-02,  4.27100825e-03,  9.42689655e-03,\n",
       "         5.98087511e-03,  5.66502346e-03, -1.17076241e-02,\n",
       "         8.04762661e-04, -1.42558349e-03,  5.66639049e-04,\n",
       "        -1.48578704e-03],\n",
       "       [ 4.27045835e-03, -1.47449451e-03,  1.70171170e-02,\n",
       "         7.14615633e-03, -5.27458418e-03,  2.60015273e-03,\n",
       "         7.27146143e-03, -3.55118501e-03,  1.51794178e-02,\n",
       "        -5.05553642e-03],\n",
       "       [-6.10041115e-03, -1.00271117e-02,  8.33411769e-04,\n",
       "        -1.09301489e-03,  1.20385215e-03,  1.30124662e-02,\n",
       "        -1.13715413e-02, -4.86143106e-03, -7.72390479e-03,\n",
       "        -5.50770196e-03],\n",
       "       [ 6.53499471e-03, -2.22425583e-02,  1.06578853e-02,\n",
       "         1.46138667e-02,  1.49822836e-02, -1.78981450e-03,\n",
       "        -6.64770995e-03,  1.50835943e-02,  1.40977517e-02,\n",
       "        -1.23152882e-02],\n",
       "       [-6.69814677e-03, -4.44694482e-04,  7.86980979e-03,\n",
       "         1.04022046e-03,  5.01280467e-03,  6.59053219e-03,\n",
       "         6.38571659e-03, -1.79706336e-02, -6.16452041e-03,\n",
       "        -1.07871352e-02],\n",
       "       [ 1.96585072e-02,  1.37879927e-02,  2.39446784e-04,\n",
       "        -6.99543869e-03, -3.22610569e-04,  7.55964885e-03,\n",
       "        -1.76684188e-02, -1.65420983e-02, -1.32027153e-02,\n",
       "         5.18431760e-03],\n",
       "       [-3.91649168e-03,  2.68151502e-02, -6.64343049e-03,\n",
       "        -1.29548003e-02, -7.31944961e-03, -8.86337842e-03,\n",
       "         7.79097391e-04, -1.08373797e-02,  1.36369569e-02,\n",
       "        -6.42914041e-03],\n",
       "       [ 9.08890767e-03, -2.28412131e-03,  7.15279330e-03,\n",
       "        -1.61590295e-02, -1.79074870e-02, -8.14459614e-03,\n",
       "        -1.78299121e-02, -1.46398233e-02, -7.64877442e-04,\n",
       "         3.04804764e-03],\n",
       "       [ 2.30577263e-03, -7.58639007e-06,  1.40833359e-02,\n",
       "        -3.68854956e-03, -5.93198819e-03,  2.48563720e-03,\n",
       "        -1.18822017e-03, -8.55388321e-03,  4.44229696e-03,\n",
       "         3.04677843e-03],\n",
       "       [-5.83492549e-03,  1.20434888e-03, -9.25169566e-03,\n",
       "         2.96805659e-03, -8.49999772e-03,  3.34098323e-03,\n",
       "         7.25389020e-03,  1.72395992e-02,  7.08801646e-03,\n",
       "         7.10761261e-03],\n",
       "       [-4.75372525e-03, -2.04990301e-03,  5.56467510e-03,\n",
       "        -4.18499989e-03, -1.99758397e-02, -2.30901863e-02,\n",
       "        -1.58633232e-03, -6.88082913e-03,  3.03022384e-03,\n",
       "         1.53787009e-02],\n",
       "       [ 2.01075875e-02, -5.23621625e-03,  4.48160991e-03,\n",
       "         1.52880479e-02, -1.17770916e-02, -1.13034354e-02,\n",
       "        -7.85232823e-03, -8.18969714e-03,  9.80881879e-03,\n",
       "         2.53157801e-02],\n",
       "       [ 3.22801873e-03, -1.62033628e-02, -7.80328683e-03,\n",
       "         5.02085968e-03,  6.93099690e-03, -2.23713354e-02,\n",
       "         2.19809542e-03, -1.29708895e-02,  1.08205909e-02,\n",
       "        -1.11544909e-02],\n",
       "       [ 5.39362879e-03,  8.90472187e-03, -1.98998813e-03,\n",
       "         2.23842790e-02,  4.61487325e-03, -9.67001584e-03,\n",
       "         6.18301044e-03, -8.67107636e-03,  3.41225506e-03,\n",
       "        -3.25799237e-03]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_embed = (0.01 * np.random.randn(20, 10))\n",
    "W_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695ffb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 | loss 9.1397\n",
      "| epoch 2 | loss 8.7844\n",
      "| epoch 3 | loss 8.1489\n",
      "| epoch 4 | loss 7.6512\n",
      "| epoch 5 | loss 7.3881\n",
      "| epoch 6 | loss 7.1942\n",
      "| epoch 7 | loss 7.0239\n",
      "| epoch 8 | loss 6.8715\n",
      "| epoch 9 | loss 6.7345\n",
      "| epoch 10 | loss 6.6123\n",
      "| epoch 11 | loss 6.5043\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m     time_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    109\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch_x, batch_t)\n\u001b[1;32m--> 110\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mparams, model\u001b[38;5;241m.\u001b[39mgrads):\n\u001b[0;32m    112\u001b[0m     param \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad\n",
      "Cell \u001b[1;32mIn[1], line 49\u001b[0m, in \u001b[0;36mSimpleRNNLM.backward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m---> 49\u001b[0m     dout \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mbackward(dout)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dout\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import ptb\n",
    "from optimizer import SGD\n",
    "from layer import TimeEmbedding, TimeAffine, TimeSoftmaxWithLoss, TimeRNN\n",
    "\n",
    "class SimpleRNNLM:\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size):\n",
    "        self.D = embedding_size\n",
    "        self.H = hidden_size\n",
    "        self.V = vocab_size\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.logit = None\n",
    "        \n",
    "        self.W_embed = 0.01 * np.random.randn(self.V, self.D)\n",
    "        \n",
    "        self.W_x_rnn = (1/np.sqrt(self.D)) * np.random.randn(self.D, self.H)\n",
    "        self.W_h_rnn = (1/np.sqrt(self.H)) * np.random.randn(self.H, self.H)\n",
    "        self.b_rnn = np.zeros(self.H, dtype='float')\n",
    "        \n",
    "        self.W_affine = (1/np.sqrt(self.H)) * np.random.randn(self.H, self.V)\n",
    "        self.b_affine = np.zeros(self.V, dtype='float')\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(TimeEmbedding(self.W_embed))\n",
    "        self.layers.append(TimeRNN(self.W_x_rnn, self.W_h_rnn, self.b_rnn))\n",
    "        self.layers.append(TimeAffine(self.W_affine, self.b_affine))\n",
    "        self.last_layer = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            for i in range(len(layer.params)):\n",
    "                self.params.append(layer.params[i])\n",
    "                self.grads.append(layer.grads[i])\n",
    "                \n",
    "    def forward(self, xs, ts):\n",
    "        out = xs\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        self.logit = out\n",
    "\n",
    "        loss = self.last_layer.forward(out, ts)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        dout = self.last_layer.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.layers[1].reset_state()\n",
    "        \n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100\n",
    "time_size = 5\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus = corpus[:1000]\n",
    "vocab_size = len(word_to_id)\n",
    "#vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "def create_batches(xs, ts, batch_size, time_size):\n",
    "    data_size = len(xs)\n",
    "    jump = data_size // batch_size\n",
    "    offsets = [i * jump for i in range(batch_size)]\n",
    "    batch_x = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "    batch_t = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "\n",
    "    time_idx = 0\n",
    "    for t in range(time_size):\n",
    "        for i, offset in enumerate(offsets):\n",
    "            batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "            batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "        time_idx += 1\n",
    "    return batch_x, batch_t\n",
    "\n",
    "model = SimpleRNNLM(wordvec_size, hidden_size, vocab_size)\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "loss_list = []\n",
    "data_size = len(xs)\n",
    "jump = data_size // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    total_loss = 0\n",
    "    loss_count = 0\n",
    "    time_idx = 0\n",
    "\n",
    "    for _ in range(data_size // (batch_size * time_size)):\n",
    "        batch_x = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        batch_t = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                idx = (offset + time_idx) % data_size\n",
    "                batch_x[i, t] = xs[idx]\n",
    "                batch_t[i, t] = ts[idx]\n",
    "            time_idx += 1\n",
    "\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        for param, grad in zip(model.params, model.grads):\n",
    "            param -= lr * grad\n",
    "\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    avg_loss = total_loss / loss_count\n",
    "    print(f\"| epoch {epoch + 1} | loss {avg_loss:.4f}\")\n",
    "    loss_list.append(avg_loss)\n",
    "\n",
    "# 시각화\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"SimpleRNNLM Loss\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d358ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
