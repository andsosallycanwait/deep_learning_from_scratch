{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = {}\n",
    "        self.grads = None\n",
    "        self.index = None\n",
    "        \n",
    "        self.params[\"W\"] = W\n",
    "        \n",
    "    def forward(self, index):\n",
    "        self.index = index\n",
    "        return self.params[\"W\"][id]\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW = np.zeros_like(self.params[\"W\"])\n",
    "        for idx, id in enumerate(self.index):\n",
    "            dW[id] += dout[idx]\n",
    "        self.grads = dW\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "309d3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = {}\n",
    "        self.grads = np.zeros_like(W)\n",
    "        self.cache = None\n",
    "        \n",
    "        self.params[\"W\"] = W\n",
    "        \n",
    "    def forward(self, h, index):\n",
    "        word_vec = self.params[\"W\"][index]\n",
    "        self.cache = (h, word_vec)\n",
    "        \n",
    "        dot_res = np.sum(h * word_vec, axis=0)\n",
    "        return dot_res\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d15b03f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "\n",
    "        negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d7c1582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4],\n",
       "       [2, 1],\n",
       "       [4, 2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1, 3, 0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0878d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def CEE(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "class Sigmoid_with_Loss:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, X, t):\n",
    "        self.y = sigmoid(X)\n",
    "        self.t = t\n",
    "        loss = CEE(self.y, self.t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        batch_size = self.t.shape[0]\n",
    "        return (self.y - self.t) / batch_size\n",
    "    \n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power, sample_size):\n",
    "        self.W = W\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [Sigmoid_with_Loss() for i in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for i in range(sample_size + 1)]\n",
    "        self.params, self.grads = []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params.append(layer.params)\n",
    "            self.grads.append(layer.grads)\n",
    "    \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "        \n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[i + 1].forward(h, negative_target)\n",
    "            loss += self.loss_layers[i + 1].forward(score, negative_label)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        \n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "            \n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        \n",
    "        self.in_layers = []\n",
    "        for i in range(2*window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "            \n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "        \n",
    "        layers = self.in_layers + self.ns_loss\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.params\n",
    "            \n",
    "        self.word_vecs = W_in\n",
    "        \n",
    "    def forward(self, contexts, targets):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, :i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1/len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        \n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3849a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower().replace(\".\", \" .\")\n",
    "    words = text.split(\" \")\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    corpus = np.array([], int)\n",
    "    id = 0\n",
    "    for word in words:\n",
    "        if word not in word_to_id.keys():\n",
    "            word_to_id[word] = id\n",
    "            id_to_word[id] = word\n",
    "            id += 1\n",
    "        corpus = np.append(corpus, word_to_id[word])\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size))\n",
    "    max_index = len(corpus) - 1\n",
    "    for index, word_id in enumerate(corpus):\n",
    "        window_left_index = index - window_size\n",
    "        window_right_index = index + window_size\n",
    "        \n",
    "        if index < window_size:\n",
    "            window_left_index = 0\n",
    "        if index > max_index - window_size:\n",
    "            window_right_index = max_index\n",
    "\n",
    "        co_matrix[word_id][corpus[window_left_index:window_right_index+1]] += 1\n",
    "        co_matrix[word_id][word_id] -= 1\n",
    "\n",
    "    return co_matrix        \n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    corpus, word_to_id, id_to_word = preprocess(corpus)\n",
    "    corpus_len = len(corpus)\n",
    "    \n",
    "    targets = corpus[window_size: corpus_len - window_size]\n",
    "    contexts = None\n",
    "    \n",
    "    for index in range(window_size, corpus_len - window_size):\n",
    "        context = np.concatenate((corpus[index - window_size:index], corpus[index + 1:index + window_size+1])).reshape(1, 2)\n",
    "        if contexts is None:\n",
    "            contexts = context\n",
    "        else:\n",
    "            contexts = np.concatenate((contexts, context), axis=0)\n",
    "    \n",
    "    return contexts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fefd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Conmplete create contexts and target =======\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 185\u001b[0m\n\u001b[0;32m    182\u001b[0m contexts, target \u001b[38;5;241m=\u001b[39m create_contexts_target(corpus, window_size)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======= Conmplete create contexts and target =======\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 185\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCBOW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdaGrad()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# 학습 루프\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 144\u001b[0m, in \u001b[0;36mCBOW.__init__\u001b[1;34m(self, vocab_size, hidden_size, window_size, corpus)\u001b[0m\n\u001b[0;32m    141\u001b[0m     layer \u001b[38;5;241m=\u001b[39m Embedding(W_in)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_layers\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns_loss \u001b[38;5;241m=\u001b[39m \u001b[43mNegativeSamplingLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_layers \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns_loss\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[33], line 102\u001b[0m, in \u001b[0;36mNegativeSamplingLoss.__init__\u001b[1;34m(self, W, corpus, power, sample_size)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_layers \u001b[38;5;241m=\u001b[39m [Sigmoid_with_Loss() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sample_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dot_layers \u001b[38;5;241m=\u001b[39m [EmbeddingDot(W) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sample_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dot_layers:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mappend(layer\u001b[38;5;241m.\u001b[39mparams)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataset import ptb\n",
    "from optimizer import AdaGrad\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower().replace(\".\", \" .\")\n",
    "    words = text.split(\" \")\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    corpus = np.array([], int)\n",
    "    id = 0\n",
    "    for word in words:\n",
    "        if word not in word_to_id.keys():\n",
    "            word_to_id[word] = id\n",
    "            id_to_word[id] = word\n",
    "            id += 1\n",
    "        corpus = np.append(corpus, word_to_id[word])\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size))\n",
    "    max_index = len(corpus) - 1\n",
    "    for index, word_id in enumerate(corpus):\n",
    "        window_left_index = index - window_size\n",
    "        window_right_index = index + window_size\n",
    "        \n",
    "        if index < window_size:\n",
    "            window_left_index = 0\n",
    "        if index > max_index - window_size:\n",
    "            window_right_index = max_index\n",
    "\n",
    "        co_matrix[word_id][corpus[window_left_index:window_right_index+1]] += 1\n",
    "        co_matrix[word_id][word_id] -= 1\n",
    "\n",
    "    return co_matrix        \n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    corpus_len = len(corpus)\n",
    "    \n",
    "    targets = corpus[window_size: corpus_len - window_size]\n",
    "    contexts = None\n",
    "    \n",
    "    for index in range(window_size, corpus_len - window_size):\n",
    "        context = np.concatenate((corpus[index - window_size:index], corpus[index + 1:index + window_size+1])).reshape(1, window_size*2)\n",
    "        if contexts is None:\n",
    "            contexts = context\n",
    "        else:\n",
    "            contexts = np.concatenate((contexts, context), axis=0)\n",
    "    \n",
    "    return contexts, targets\n",
    "\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = {}\n",
    "        self.grads = np.zeros_like(W)\n",
    "        self.cache = None\n",
    "        \n",
    "        self.params[\"W\"] = W\n",
    "        \n",
    "    def forward(self, h, index):\n",
    "        word_vec = self.params[\"W\"][index]\n",
    "        self.cache = (h, word_vec)\n",
    "        \n",
    "        dot_res = np.sum(h * word_vec, axis=0)\n",
    "        return dot_res\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh\n",
    "    \n",
    "class Sigmoid_with_Loss:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, X, t):\n",
    "        self.y = sigmoid(X)\n",
    "        self.t = t\n",
    "        loss = CEE(self.y, self.t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        batch_size = self.t.shape[0]\n",
    "        return (self.y - self.t) / batch_size\n",
    "    \n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power, sample_size):\n",
    "        self.W = W\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [Sigmoid_with_Loss() for i in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for i in range(sample_size + 1)]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "        \n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[i + 1].forward(h, negative_target)\n",
    "            loss += self.loss_layers[i + 1].forward(score, negative_label)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        \n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "            \n",
    "        return dh\n",
    "    \n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        \n",
    "        self.in_layers = []\n",
    "        for i in range(2*window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "            \n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "        \n",
    "        layers = self.in_layers + self.ns_loss\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.params\n",
    "            \n",
    "        self.word_vecs = W_in\n",
    "        \n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, :i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1/len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "# =================================================================================\n",
    "\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "print(\"======= Conmplete create contexts and target =======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "431a2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts.shape, target.shape\n",
    "np.save('ptb_contexts.npy', contexts)\n",
    "np.save('ptb_target.npy', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94ffeef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Conmplete create contexts and target =======\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m contexts, target \u001b[38;5;241m=\u001b[39m create_contexts_target(corpus, window_size)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======= Conmplete create contexts and target =======\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCBOW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdaGrad()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 학습 루프\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 144\u001b[0m, in \u001b[0;36mCBOW.__init__\u001b[1;34m(self, vocab_size, hidden_size, window_size, corpus)\u001b[0m\n\u001b[0;32m    141\u001b[0m     layer \u001b[38;5;241m=\u001b[39m Embedding(W_in)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_layers\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns_loss \u001b[38;5;241m=\u001b[39m \u001b[43mNegativeSamplingLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_layers \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns_loss\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[33], line 102\u001b[0m, in \u001b[0;36mNegativeSamplingLoss.__init__\u001b[1;34m(self, W, corpus, power, sample_size)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_layers \u001b[38;5;241m=\u001b[39m [Sigmoid_with_Loss() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sample_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dot_layers \u001b[38;5;241m=\u001b[39m [EmbeddingDot(W) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sample_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dot_layers:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mappend(layer\u001b[38;5;241m.\u001b[39mparams)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "print(\"======= Conmplete create contexts and target =======\")\n",
    "\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = AdaGrad()\n",
    "\n",
    "\n",
    "# 학습 루프\n",
    "loss_list = []\n",
    "data_size = len(contexts)\n",
    "max_iters = data_size // batch_size\n",
    "print_interval = max_iters // 10\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    print(f\"Epoch {epoch+1}/{max_epoch}\")\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    loss_count = 0\n",
    "\n",
    "    # 셔플\n",
    "    idx = np.random.permutation(np.arange(data_size))\n",
    "    contexts = contexts[idx]\n",
    "    target = target[idx]\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        batch_contexts = contexts[iters * batch_size:(iters + 1) * batch_size]\n",
    "        batch_target = target[iters * batch_size:(iters + 1) * batch_size]\n",
    "\n",
    "        # 순전파, 역전파, 파라미터 갱신\n",
    "        loss = model.forward(batch_contexts, batch_target)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        if (iters + 1) % print_interval == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print(f\"| iteration {iters+1}/{max_iters} | avg loss {avg_loss:.4f}\")\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1} finished in {epoch_time:.2f}s\")\n",
    "\n",
    "# 학습된 word vector 저장\n",
    "word_vecs = model.word_vecs\n",
    "np.save('cbow_word_vectors.npy', word_vecs)\n",
    "print(\"Word vectors saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970eeb47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
