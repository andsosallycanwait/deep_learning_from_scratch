{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "798f9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layer import Rnnlm\n",
    "from util import softmax\n",
    "\n",
    "class RnnlmGen(Rnnlm):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__(vocab_size, embedding_size, hidden_size)\n",
    "\n",
    "    def generate(self, start_id, skip_ids, sample_size):\n",
    "        input = start_id\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        while len(word_ids) < sample_size:\n",
    "            input = np.array(input, dtype=np.int32).reshape(1, 1)  # ⬅️ 정수형 유지\n",
    "            logit = self.predict(input).reshape(-1)\n",
    "            prob = softmax(logit)\n",
    "\n",
    "            sample = np.random.choice(len(prob), size=1, p=prob)[0]\n",
    "            if sample in skip_ids:\n",
    "                continue\n",
    "            word_ids.append(sample)\n",
    "            input = sample\n",
    "\n",
    "        return word_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e0dc32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you claimed iowa issues coupled lawrence boosted predicted private roth shocks loath surveys depended contributed offerings defense statements prentice yearly compromise recruit no. particular mccall unix split goods tesoro crackdown'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "wordvec_size = 100\n",
    "hidden_size = 100\n",
    "\n",
    "model = RnnlmGen(vocab_size, wordvec_size, hidden_size)\n",
    "#model.load_params('rnnlm_params.npy')\n",
    "\n",
    "start_id = word_to_id['you']\n",
    "skip_word = ['N', '<unk>', '$']\n",
    "skip_ids = []\n",
    "for word in skip_word:\n",
    "    skip_ids.append(word_to_id[word])\n",
    "skip_ids\n",
    "\n",
    "generated_word_ids = model.generate(start_id, skip_ids, sample_size=30)\n",
    "generated_words = []\n",
    "for id in generated_word_ids:\n",
    "    generated_words.append(id_to_word[id])\n",
    "    \n",
    "generated_text = ' '.join(generated_words)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3efc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.37227930e-03, -2.24177679e-03,  4.72477404e-03,\n",
       "         4.15270799e-04, -4.68666200e-04,  6.59893500e-04,\n",
       "        -5.18815359e-04,  9.47673107e-05, -5.87560062e-04,\n",
       "        -1.13624008e-03, -8.60574364e-04,  6.00228086e-05,\n",
       "         4.11789119e-03,  6.51015376e-04,  8.93977587e-04,\n",
       "         2.75384198e-04, -1.43559824e-03, -8.36388965e-04,\n",
       "        -2.60920357e-03, -5.47516393e-03,  4.67574631e-04,\n",
       "        -2.98718689e-03,  2.61759432e-03,  2.03606859e-03,\n",
       "        -2.39678426e-03, -1.37167948e-03,  3.24085180e-04,\n",
       "         1.27934525e-03, -6.81686797e-04,  4.73596109e-03,\n",
       "         6.78029435e-04, -1.00032950e-03,  2.08116276e-03,\n",
       "         1.49233371e-03, -6.59230805e-04,  2.48755794e-03,\n",
       "         3.20925144e-03, -1.36918097e-03, -2.72739453e-05,\n",
       "         2.93381920e-04, -1.87923515e-03,  1.01112784e-03,\n",
       "         1.48276135e-03,  1.29908673e-03,  1.64771895e-03,\n",
       "         2.72686360e-03, -2.25090585e-03,  2.96247890e-04,\n",
       "        -1.49486194e-04,  2.53493758e-03,  1.21083576e-06,\n",
       "        -5.07513003e-04, -1.98909407e-03,  2.45947624e-03,\n",
       "         1.55771303e-03, -9.25448141e-04,  1.62937318e-03,\n",
       "         2.24511744e-03, -2.80170096e-03,  3.73694231e-04,\n",
       "         9.87329055e-04, -3.34828324e-03, -1.01416292e-04,\n",
       "        -3.67915258e-04,  2.06340672e-04, -2.54580122e-03,\n",
       "        -4.13971115e-03,  1.81623187e-03, -2.64708279e-03,\n",
       "         3.56027787e-03,  1.94331631e-03,  1.34917733e-03,\n",
       "         1.89296214e-03,  3.92378727e-03,  2.74939754e-04,\n",
       "         2.85600225e-04,  1.03820290e-04,  4.89804894e-03,\n",
       "         5.71325218e-05,  1.98893156e-03,  1.16979587e-04,\n",
       "        -3.07320361e-03,  1.83119019e-03, -8.14639963e-04,\n",
       "        -2.27435655e-03,  2.02117968e-04,  9.60394274e-04,\n",
       "        -4.62542148e-03,  2.27546170e-05,  6.50994992e-03,\n",
       "         1.93784409e-03,  3.29077884e-05,  2.48072646e-03,\n",
       "        -1.45363272e-03, -1.64824445e-03, -5.30374586e-04,\n",
       "         5.94038283e-06, -2.28430796e-03,  3.01569147e-04,\n",
       "         3.27495858e-03,  2.92980368e-03, -2.30360939e-03,\n",
       "        -1.63703936e-03, -9.34817479e-04,  4.48198710e-03,\n",
       "         1.76932849e-03,  2.55301571e-03,  2.09262292e-03,\n",
       "        -6.18265360e-04, -8.37723026e-04, -8.27719807e-04,\n",
       "         5.14242856e-04, -3.26963328e-03,  1.19913268e-04,\n",
       "         1.01807481e-03,  1.67780486e-03,  8.46049574e-04,\n",
       "        -2.58506439e-03,  5.22896473e-04, -4.99296980e-03,\n",
       "        -9.05281049e-05, -4.64223296e-04, -1.57436635e-03,\n",
       "         4.73720429e-04, -1.17626775e-03,  4.38797812e-04,\n",
       "         1.60372467e-03, -1.53170619e-03,  2.69420608e-03,\n",
       "         1.18227319e-04, -1.55943295e-03, -4.92118474e-04,\n",
       "        -3.60004953e-03,  2.76883459e-03,  1.48793240e-03,\n",
       "         1.95997488e-03, -7.31163658e-04, -3.52145266e-03,\n",
       "        -6.40155165e-04, -2.35911389e-03, -1.82464626e-03,\n",
       "         5.94660989e-04,  2.09588883e-03, -4.05444298e-05,\n",
       "         2.10617413e-03,  1.65651843e-03, -5.41753182e-03,\n",
       "         6.29168237e-04, -1.02603040e-03, -9.11804731e-04,\n",
       "        -1.24748680e-03, -1.67082623e-03,  4.10694222e-04,\n",
       "        -3.09049874e-03, -2.39418168e-03, -1.33957353e-03,\n",
       "        -4.33553988e-03,  7.18148774e-04, -3.76415602e-03,\n",
       "         2.10736948e-03, -4.13189922e-03,  3.58762118e-05,\n",
       "        -1.53690879e-03, -2.43205065e-03, -1.32301566e-03,\n",
       "         4.49740328e-03, -4.06019250e-03, -1.62257685e-03,\n",
       "         3.11323744e-03, -1.27651670e-03, -1.58557959e-03,\n",
       "        -1.24046346e-03,  4.78409696e-03,  1.75970595e-03,\n",
       "        -5.55733172e-03, -4.16236231e-03, -2.57763197e-03,\n",
       "        -2.65047979e-03,  1.16371573e-03, -2.66166916e-03,\n",
       "        -2.25856411e-03,  2.72718770e-03,  6.08930946e-04,\n",
       "         8.40985100e-04, -2.61501246e-03, -1.95011485e-03,\n",
       "         3.25839792e-04, -7.21159624e-04,  2.35635508e-03,\n",
       "        -3.31126899e-03, -2.62855249e-03, -5.45349962e-04,\n",
       "        -7.31226755e-04,  5.22816030e-04, -4.50346805e-03,\n",
       "         3.27865873e-03, -2.33529415e-03,  5.34526422e-04,\n",
       "         3.51271126e-03, -3.40242917e-03, -1.12174102e-03,\n",
       "         3.03014321e-03, -3.57409241e-03,  2.69954442e-03,\n",
       "         1.05718580e-04, -3.56651173e-04, -6.71844813e-04,\n",
       "        -1.54754217e-03,  2.09269347e-03,  1.61533942e-03,\n",
       "        -8.51473422e-04, -2.24529227e-04,  2.47005519e-04,\n",
       "        -1.91441446e-03, -2.11867597e-03, -8.62142188e-05,\n",
       "         1.86256634e-03,  1.28101092e-03,  4.50560078e-03,\n",
       "         1.83140219e-04,  1.11769524e-03, -3.02920281e-03,\n",
       "        -9.98806441e-04, -1.96905201e-03,  2.87686009e-04,\n",
       "         2.86523090e-03, -1.07052934e-03,  4.25415346e-03,\n",
       "         1.09017936e-04,  1.36373634e-03, -1.20847148e-03,\n",
       "        -1.67210877e-04,  2.54790229e-03, -3.89541406e-03,\n",
       "         2.47089518e-03, -1.51575747e-04,  5.60736796e-03,\n",
       "        -3.12348991e-03,  2.58078356e-03,  3.29361385e-04,\n",
       "        -3.20855528e-04, -8.02042661e-04,  5.65718103e-04,\n",
       "         1.76109956e-03,  4.19388758e-04, -8.99591134e-04,\n",
       "         2.59358843e-04, -2.02507433e-03,  2.33276398e-03,\n",
       "        -6.59649260e-04, -2.09377473e-03, -1.43970072e-04,\n",
       "         1.43044558e-03,  2.53123976e-03, -1.03260344e-03,\n",
       "        -2.56539788e-03,  1.31111243e-04,  1.99866388e-03,\n",
       "         1.04692904e-03, -3.96226719e-03,  2.56695086e-04,\n",
       "        -3.36073618e-03, -2.51506804e-03,  7.82866264e-04,\n",
       "         1.04699738e-03,  1.95567729e-04,  4.07711230e-03,\n",
       "         3.62317893e-04, -2.53074977e-04,  2.90044583e-03,\n",
       "        -1.71507150e-03, -3.14353092e-04, -2.63626548e-03,\n",
       "         3.55194858e-03,  7.04503851e-04,  1.98274525e-03,\n",
       "        -1.56874245e-03, -5.38532797e-04,  2.77479365e-03,\n",
       "         2.20719888e-03,  3.21377045e-03,  2.37315777e-03,\n",
       "         1.45838188e-04, -2.45181005e-03,  1.54675310e-03,\n",
       "         1.94438588e-04,  9.08519025e-04, -4.91315220e-03,\n",
       "        -3.50308511e-03, -2.24105432e-03,  2.21566320e-03,\n",
       "         3.68737173e-03,  4.25325707e-04, -2.22620409e-04,\n",
       "        -7.07248691e-04,  4.68529342e-03, -1.10634603e-03,\n",
       "        -2.43761088e-03, -1.97209977e-03,  2.29888805e-03,\n",
       "         2.15959735e-03,  1.74344575e-03, -1.35389972e-03,\n",
       "        -2.33629346e-03,  7.12659210e-04, -1.07965712e-03,\n",
       "        -8.31097481e-04,  3.66614037e-03,  3.78634734e-03,\n",
       "        -6.37893099e-05,  9.52238915e-04, -9.60709411e-04,\n",
       "         1.40026188e-03,  3.50773055e-03, -2.98005366e-03,\n",
       "        -2.97473336e-04, -9.48403031e-04, -1.93078921e-03,\n",
       "         2.61110230e-03,  1.13919785e-03,  3.96955147e-04,\n",
       "         9.17818106e-04, -8.96542915e-04, -5.27573889e-03,\n",
       "        -3.23116983e-04,  2.48804165e-04,  1.94484496e-03,\n",
       "        -5.92260621e-05, -7.53843924e-04, -7.57191679e-04,\n",
       "         3.94756207e-03, -4.45932336e-03,  1.06306956e-03,\n",
       "        -1.57940108e-03,  4.15982166e-03, -3.50571994e-04,\n",
       "         8.08722223e-04,  5.52134297e-04,  1.13267009e-03,\n",
       "        -2.47238483e-03,  1.40559743e-03, -2.35181767e-03,\n",
       "         1.09140063e-03, -1.77372037e-03,  8.62705114e-04,\n",
       "        -6.99755130e-03, -3.67774256e-03, -5.34690637e-03,\n",
       "         2.28023692e-03,  2.10441160e-03, -2.87228404e-03,\n",
       "         2.65757460e-03,  8.03355011e-04, -1.71235355e-03,\n",
       "        -4.68572252e-05, -5.42605820e-04,  1.09353370e-03,\n",
       "         2.78674113e-03, -1.12197385e-03,  1.40072708e-03,\n",
       "         1.23173010e-03, -9.95394774e-04,  3.47825116e-03,\n",
       "         2.36982177e-03, -2.21761712e-03, -1.02420873e-03,\n",
       "         2.40421016e-03, -3.86848114e-03,  1.39108405e-03,\n",
       "        -5.67911135e-04,  7.43135752e-06,  1.47842627e-04,\n",
       "        -1.64785684e-04,  3.98612674e-03,  2.88137817e-03,\n",
       "         2.43804272e-04,  8.73295590e-04,  2.15259424e-04,\n",
       "         5.21934126e-04,  3.16068227e-03,  1.21543894e-03,\n",
       "        -3.96894384e-03,  1.92815252e-03,  3.66128865e-03,\n",
       "         3.06650810e-03, -2.15566810e-03,  1.77362491e-03,\n",
       "        -1.43348100e-03, -1.52217178e-03,  1.86959005e-04,\n",
       "         4.35944879e-03, -4.80396347e-03, -6.76749391e-04,\n",
       "        -1.76638365e-03, -2.53661559e-03,  2.32210732e-03,\n",
       "         6.72521302e-04, -1.86935510e-03,  2.68632802e-03,\n",
       "        -1.13217440e-03, -7.88452569e-04, -2.48414394e-03,\n",
       "         1.32238737e-03,  2.04980792e-03, -3.41699197e-04,\n",
       "        -8.68624367e-04, -4.06090054e-04, -1.45661179e-05,\n",
       "        -2.28001649e-04,  7.33854133e-04, -1.30401296e-03,\n",
       "         9.05790250e-04, -3.28633795e-03, -1.18208770e-03,\n",
       "         3.57889407e-03,  3.00885411e-03, -1.00554177e-03,\n",
       "         9.96517017e-04, -2.84500513e-03,  8.74560385e-04,\n",
       "         2.63053575e-04, -1.44847331e-03,  2.09019333e-03,\n",
       "        -4.21597244e-04, -9.07999056e-05,  1.29385269e-04,\n",
       "         2.38270825e-03, -2.35012011e-03, -5.03800693e-04,\n",
       "         2.65606563e-03,  1.39292143e-03,  6.01182983e-05,\n",
       "         5.71953878e-03,  5.59189066e-04,  2.26587337e-03,\n",
       "        -1.52299937e-04, -1.09846261e-03, -2.20993417e-03,\n",
       "        -9.64221195e-04,  3.25723697e-04,  9.33060714e-04,\n",
       "        -3.81803227e-04,  5.39743342e-05,  2.61733832e-04,\n",
       "         4.41958837e-04, -2.43569585e-03, -9.76500742e-04,\n",
       "        -2.69056298e-03,  7.51147338e-04, -1.11175235e-03,\n",
       "        -1.23493723e-03, -4.71501466e-04, -7.42102857e-05,\n",
       "         2.70882947e-03, -1.30904245e-03,  7.68450554e-04,\n",
       "         2.01090565e-03,  2.41980283e-03,  3.72033013e-04,\n",
       "         1.27448409e-03,  1.82656921e-03,  1.46606908e-04,\n",
       "        -8.33543250e-04, -2.87087896e-04,  1.88681274e-03,\n",
       "        -2.15959945e-03, -3.94407340e-04,  1.17978267e-03,\n",
       "        -1.48229697e-03, -9.15568206e-04, -1.72213931e-03,\n",
       "        -4.75373212e-03,  2.14975444e-03, -4.23766393e-03,\n",
       "        -2.43730890e-03, -2.35388312e-03,  1.36867573e-03,\n",
       "         3.22254701e-03,  1.67758623e-03, -2.92976154e-03,\n",
       "        -6.99641241e-04,  6.15677563e-04, -2.39378703e-03,\n",
       "        -5.20320097e-03, -5.10492595e-04,  1.60790849e-04,\n",
       "        -4.17760632e-04,  6.81956473e-04, -2.38153012e-03,\n",
       "        -2.89969705e-03, -2.78124324e-04, -5.58522157e-03,\n",
       "        -1.94798096e-03, -4.53260465e-04, -9.18530510e-04,\n",
       "        -1.43817533e-03, -1.98261579e-03, -7.05594139e-04,\n",
       "         1.73765235e-03, -2.05731741e-03,  5.83187118e-03,\n",
       "        -3.40689672e-04, -1.02995592e-03,  9.16847028e-04,\n",
       "         1.37805694e-03, -3.76796513e-03,  1.21437246e-03,\n",
       "         4.04295599e-04, -1.50752615e-03,  1.08704239e-03,\n",
       "        -1.09144894e-03, -2.74977414e-03, -8.50650948e-04,\n",
       "        -1.35092530e-03,  1.33868773e-03,  1.64377841e-03,\n",
       "        -1.49851048e-03,  1.25898712e-03,  9.16428631e-04,\n",
       "        -5.37672138e-04, -2.43931613e-03,  1.79843779e-03,\n",
       "        -1.62597420e-03,  7.21781631e-04, -5.20368048e-04,\n",
       "         2.06791260e-03,  1.12126255e-03,  1.69175153e-03,\n",
       "        -5.64539898e-03,  6.13508630e-04,  1.26973202e-03,\n",
       "         1.15429924e-03,  2.02638004e-03, -6.67561893e-04,\n",
       "        -1.22891983e-03, -1.36112922e-03, -2.59818556e-03,\n",
       "         3.74669628e-03,  6.34565193e-04, -2.27687604e-04,\n",
       "         1.09981408e-03, -5.75780286e-05,  1.42718654e-03,\n",
       "        -3.01799155e-05, -3.01602972e-03,  1.61539298e-03,\n",
       "         1.78723119e-03, -3.87429609e-03,  5.41768386e-04,\n",
       "        -4.98692854e-04, -1.96817657e-03,  1.19019460e-05,\n",
       "        -5.56560094e-03,  1.33279758e-03, -9.47457796e-04,\n",
       "        -2.51734862e-03,  1.68820284e-03,  1.47163181e-03,\n",
       "        -9.81728430e-04,  1.34428730e-03, -1.14941003e-03,\n",
       "         1.23699929e-03,  1.79934560e-03, -1.03457295e-03,\n",
       "        -9.02239190e-05,  6.60361489e-04,  7.08389445e-04,\n",
       "        -8.24332586e-04, -1.56203762e-03,  3.03370296e-03,\n",
       "        -8.92829616e-04, -2.18935916e-03, -2.18371209e-03,\n",
       "        -4.04325919e-03, -6.41823630e-04, -3.05876019e-03,\n",
       "        -1.91043410e-03, -1.96018419e-03,  2.21029669e-03,\n",
       "        -2.30543781e-03, -1.15967297e-03, -9.79604898e-04,\n",
       "         1.77634927e-03, -2.01810920e-03, -2.79559381e-03,\n",
       "        -3.82434053e-04, -8.00615526e-04,  1.86233758e-03,\n",
       "        -1.33026403e-03, -4.08560969e-04,  9.91416164e-04,\n",
       "         1.11138052e-03,  3.32211773e-03,  1.75119680e-03,\n",
       "        -5.08430460e-03, -1.02022686e-03,  2.04920972e-04,\n",
       "         1.13975047e-03,  4.27071704e-03, -4.58020077e-04,\n",
       "        -4.17593215e-03, -1.26826204e-03, -3.76569456e-03,\n",
       "         6.14060846e-04, -3.63438530e-03,  5.25173452e-03,\n",
       "         2.71620392e-03, -4.07393702e-04,  4.27863910e-04,\n",
       "         2.16034055e-03,  2.97172996e-03,  2.38289242e-03,\n",
       "        -4.52766079e-04, -9.90201952e-04,  9.08366987e-04,\n",
       "        -2.55718129e-03,  2.89937551e-03, -5.29473322e-03,\n",
       "        -3.02494317e-03,  3.48375033e-04,  6.78399461e-04,\n",
       "         4.56325099e-04,  9.38995043e-04, -7.59232265e-04,\n",
       "        -2.18786392e-03, -1.74307101e-03,  9.53354174e-05,\n",
       "         4.13871836e-03, -2.66410946e-03,  5.19607333e-04,\n",
       "         1.05600433e-04, -1.45939237e-03, -1.19101576e-04,\n",
       "         3.29112983e-04, -7.13951013e-04, -3.18810460e-03,\n",
       "         3.77453840e-03, -1.22724636e-03, -2.26527220e-03,\n",
       "        -8.28969351e-04,  2.61278765e-04,  2.46012607e-03,\n",
       "         5.93553705e-06, -2.62042647e-03, -1.06931839e-03,\n",
       "        -3.43686435e-03, -1.04838330e-03, -1.97197404e-03,\n",
       "         1.23936089e-03,  1.54283969e-03, -2.74729379e-03,\n",
       "        -1.25739397e-03,  1.28200953e-03, -5.10648184e-04,\n",
       "        -9.19952436e-05,  1.20359892e-03, -3.52431135e-03,\n",
       "        -1.32384687e-03, -1.59839785e-03, -8.71915254e-05,\n",
       "        -5.15999447e-04,  1.23144127e-03, -7.52430642e-04,\n",
       "         2.87307054e-03, -3.01652262e-03,  1.66234863e-03,\n",
       "         9.30801441e-04, -1.23455294e-03, -2.19211821e-03,\n",
       "        -1.35288562e-03,  2.74402113e-03,  3.21191316e-03,\n",
       "        -1.23941363e-03,  2.93185120e-03,  6.22473599e-04,\n",
       "        -2.79663643e-03, -2.80384719e-03,  2.83130328e-03,\n",
       "         5.81542903e-04,  1.65616244e-03,  1.17954891e-03,\n",
       "         9.11666430e-05, -1.05974893e-03,  2.36159685e-05,\n",
       "         3.07339244e-03,  4.68200957e-03, -6.54009054e-04,\n",
       "        -5.83557994e-04, -1.17656635e-03,  2.50379788e-04,\n",
       "         6.75551826e-04,  2.08814233e-03, -2.72019417e-03,\n",
       "        -6.65257627e-04, -7.71780906e-04, -3.11049982e-04,\n",
       "         1.19929772e-03, -8.91516102e-05,  2.62248446e-03,\n",
       "         2.18084687e-03, -1.28930015e-03, -1.98683445e-03,\n",
       "        -2.64101103e-03,  2.37865699e-03,  3.49287339e-03,\n",
       "         4.21037170e-04, -3.99066368e-03,  1.23682106e-03,\n",
       "        -2.67433439e-04, -5.17727202e-03,  1.77066121e-03,\n",
       "        -1.92974904e-03, -2.28544371e-03, -1.29222649e-03,\n",
       "        -4.90291743e-04,  1.43950840e-03, -1.06126652e-03,\n",
       "        -2.08481448e-03,  1.75052881e-03, -1.11207634e-03,\n",
       "         9.40009777e-04, -6.66205247e-04, -1.86196365e-03,\n",
       "        -1.31634530e-03, -1.00685854e-03, -2.05558143e-03,\n",
       "        -7.51055602e-04,  6.67638451e-05, -4.06446774e-03,\n",
       "        -1.00541557e-03,  1.53822510e-03, -5.47126867e-04,\n",
       "         3.78054660e-03,  1.93544780e-04,  2.79973075e-03,\n",
       "        -3.25334864e-03,  4.40800376e-03, -3.25100357e-03,\n",
       "        -4.95844637e-04, -3.86027340e-03, -1.26787706e-03,\n",
       "        -5.83564397e-04, -1.74896582e-03,  7.66920159e-03,\n",
       "         1.05191814e-03,  2.69131124e-05, -3.43416655e-03,\n",
       "         1.99375107e-04, -5.08665631e-04, -1.89727452e-03,\n",
       "         7.22011551e-04,  3.01390770e-03,  7.30586122e-04,\n",
       "        -2.00796127e-03, -5.85744740e-04, -1.62587059e-03,\n",
       "        -2.44033569e-03,  2.35742121e-03, -2.63745897e-03,\n",
       "         2.61929911e-03,  5.43558621e-04, -6.75629883e-04,\n",
       "        -2.69335136e-03, -2.97493825e-04, -2.63561087e-05,\n",
       "         4.29223728e-04,  1.53755071e-03,  2.85889511e-03,\n",
       "         2.76697055e-03,  4.59097937e-05, -9.06565925e-04,\n",
       "        -4.46714344e-04, -1.09091366e-03,  1.71138986e-03,\n",
       "        -9.05318768e-04, -1.74694974e-03,  2.49858457e-03,\n",
       "         3.64437443e-03,  1.84330426e-03, -1.71919982e-03,\n",
       "         1.70004484e-03, -6.20662875e-04, -2.15874636e-03,\n",
       "        -7.94717926e-06,  2.28125806e-04,  3.89888592e-04,\n",
       "         1.44855713e-03, -2.23200116e-03, -4.52326844e-04,\n",
       "        -1.94931834e-03,  5.35029219e-04,  3.08530242e-03,\n",
       "         2.83692288e-03,  5.82282012e-03, -1.56081130e-03,\n",
       "         1.05941121e-03, -1.92253699e-03,  1.91130734e-03,\n",
       "        -8.77247076e-04, -1.81517738e-03, -2.39818357e-03,\n",
       "         2.07623467e-03, -1.59964606e-03,  6.04725210e-03,\n",
       "        -1.87998160e-03, -3.73069488e-04,  2.36854376e-03,\n",
       "        -1.47466559e-03, -2.25344021e-03, -2.12919549e-03,\n",
       "         2.60833441e-03,  9.23755797e-05, -3.69971531e-04,\n",
       "        -1.30495953e-03,  2.74218852e-03,  5.19231951e-04,\n",
       "        -1.13603415e-03,  1.66150450e-03,  3.20864515e-03,\n",
       "        -1.34251034e-03,  1.02393515e-03,  4.45173075e-03,\n",
       "        -1.00446935e-03,  2.69609550e-03, -1.91648782e-03,\n",
       "         9.26585635e-04,  1.56654255e-03, -1.52540495e-04,\n",
       "        -4.40308591e-03,  2.29850900e-03,  2.44680326e-03,\n",
       "         1.37059356e-03, -3.78929777e-04,  2.02991930e-03,\n",
       "        -2.74297618e-03, -8.38177162e-04,  8.30765872e-04,\n",
       "        -9.32380324e-04, -6.10787771e-04,  1.79427234e-03,\n",
       "         6.39902777e-04, -2.45674211e-03,  1.39062642e-03,\n",
       "         4.23532067e-04,  6.06479705e-04,  1.26186773e-04,\n",
       "        -2.71331301e-05,  4.15486796e-03, -2.46253854e-04,\n",
       "         9.74558876e-04,  1.30226603e-03, -6.36040932e-04,\n",
       "         1.30599554e-04, -3.83848790e-04, -1.66800292e-03,\n",
       "        -2.04810640e-05, -4.73085372e-03, -2.93061766e-03,\n",
       "         2.90507800e-04,  1.58550078e-03,  4.67030011e-04,\n",
       "        -7.74993387e-04,  3.79586476e-03, -1.09615910e-03,\n",
       "         1.89576403e-03, -4.96249413e-04, -3.60522536e-03,\n",
       "         4.62639087e-04, -1.18036289e-03,  1.65610411e-03,\n",
       "         2.45334872e-04, -1.28651271e-04,  1.20080216e-03,\n",
       "        -2.68707564e-03, -4.79389913e-03,  3.23228585e-03,\n",
       "        -2.07740534e-03,  5.69576281e-04, -1.91662926e-03,\n",
       "         2.43139802e-03,  1.33033213e-03,  1.03695982e-03,\n",
       "        -2.10797996e-03, -4.29086940e-04,  6.96366886e-04,\n",
       "         1.29341334e-03, -3.53111280e-03, -1.48419640e-03,\n",
       "         2.85950513e-03,  1.23690511e-03,  1.58392685e-03,\n",
       "        -1.18002808e-03, -5.44217182e-05,  9.89400432e-04,\n",
       "         1.65388163e-03, -3.02854413e-03,  3.30777752e-04,\n",
       "         3.89568135e-03, -1.43908896e-03,  3.51052993e-04,\n",
       "        -2.45186640e-03,  1.05219742e-03, -3.06194392e-03,\n",
       "        -2.22626049e-03, -1.76029187e-03,  5.03074657e-03,\n",
       "        -1.40680827e-03,  1.15612033e-03,  1.08348043e-03,\n",
       "        -4.95930109e-03,  1.88498595e-03,  2.22872337e-03,\n",
       "        -3.02325934e-03,  1.70467235e-03,  4.50774236e-03,\n",
       "         1.07786409e-03,  2.19270284e-03, -3.38581880e-03,\n",
       "         9.35814314e-05, -3.93073889e-04,  6.01962907e-04,\n",
       "        -8.38676933e-05, -1.17421523e-03,  2.69102817e-03,\n",
       "         5.12180338e-03, -2.32784427e-03, -1.99139165e-03,\n",
       "        -1.68964310e-04,  1.25766080e-03,  2.17790552e-03,\n",
       "        -1.33577865e-04,  3.64497164e-03, -4.93354816e-03,\n",
       "         4.54563461e-03,  1.58523931e-03, -7.83125462e-04,\n",
       "        -2.72853114e-03, -2.48256116e-03, -2.69754534e-03,\n",
       "        -4.77143982e-03,  2.39159632e-03, -2.55333632e-03,\n",
       "        -1.15923898e-03, -1.18514290e-05,  1.09365082e-03,\n",
       "         2.32783356e-03, -3.73560423e-03,  3.70576000e-03,\n",
       "         1.51584495e-03, -2.70827673e-04,  6.43170380e-04,\n",
       "         1.03530474e-05,  4.85480996e-04,  5.29532088e-04,\n",
       "         7.21140823e-05,  6.50925213e-05, -1.92035479e-03,\n",
       "         4.12272755e-03,  2.44995044e-03, -1.60082069e-03,\n",
       "         5.66141156e-04, -2.87701515e-03, -3.42906662e-03,\n",
       "         4.48763976e-03, -2.11149920e-03,  1.05147902e-03,\n",
       "        -8.19099136e-04, -8.89973366e-04, -1.91653753e-03,\n",
       "         2.84545170e-03,  1.97614427e-03,  1.83751667e-03,\n",
       "         2.39547854e-03, -1.20869244e-03, -2.23099766e-03,\n",
       "         3.89142311e-04,  1.37275085e-03,  1.15339360e-04,\n",
       "        -5.30527625e-03,  2.79704342e-03, -2.04687146e-03,\n",
       "        -4.24455851e-03, -2.50519789e-03, -2.31191050e-03,\n",
       "        -1.70849008e-03, -6.87342312e-04,  5.67806070e-04,\n",
       "        -6.42742030e-04, -9.75125644e-04, -2.23620236e-03,\n",
       "         3.40029434e-03, -1.69656519e-03,  8.19565670e-04,\n",
       "         2.84785475e-03,  3.37709533e-03,  2.14246032e-03,\n",
       "         2.42150947e-03, -1.43026584e-03,  3.20988498e-03,\n",
       "        -1.78259867e-03]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from layer import TimeEmbedding, TimeLSTM\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        self.V = vocab_size\n",
    "        self.D = wordvec_size\n",
    "        self.H = hidden_size\n",
    "        \n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "        self.W_embed = (0.01 * np.random.randn(self.V, self.D)).astype('float32')\n",
    "        self.W_x_lstm = ((1 / np.sqrt(self.D)) * np.random.randn(self.D, 4*self.H)).astype('float32')\n",
    "        self.W_h_lstm = ((1 / np.sqrt(self.H)) * np.random.randn(self.H, 4*self.H)).astype('float32')\n",
    "        self.b_lstm = np.zeros(4*self.H).astype('float32')\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(TimeEmbedding(self.W_embed))\n",
    "        self.layers.append(TimeLSTM(self.W_x_lstm, self.W_h_lstm, self.b_lstm, stateful=False))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            for param, grad in zip(layer.params, layer.grads):\n",
    "                self.params.append(param)\n",
    "                self.grads.append(grad)\n",
    "    \n",
    "    \n",
    "    def forward(self, xs):\n",
    "        out = xs\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        \n",
    "        self.hs = out\n",
    "        return out[:, -1, :]\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh\n",
    "        dout = dhs\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae95645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import TimeEmbedding, TimeLSTM, TimeAffine\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        self.V = vocab_size\n",
    "        self.D = wordvec_size\n",
    "        self.H = hidden_size\n",
    "        \n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "        self.W_embed = (0.01 * np.random.randn(self.V, self.D)).astype('float32')\n",
    "        self.W_x_lstm = ((1 / np.sqrt(self.D)) * np.random.randn(self.D, 4*self.H)).astype('float32')\n",
    "        self.W_h_lstm = ((1 / np.sqrt(self.H)) * np.random.randn(self.H, 4*self.H)).astype('float32')\n",
    "        self.b_lstm = np.zeros(4*self.H).astype('float32')\n",
    "        self.W_affine = ((1 / np.sqrt(self.H)) * np.random.randn(self.H, self.V)).astype('float32')\n",
    "        self.b_affine = np.zeros(self.V).astype('float32')\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(TimeEmbedding(self.W_embed))\n",
    "        self.layers.append(TimeLSTM(self.W_x_lstm, self.W_h_lstm, self.b_lstm))\n",
    "        self.layers.append(TimeAffine(self.W_affine, self.b_affine))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            for param, grad in zip(layer.params, layer.grads):\n",
    "                self.params.append(param)\n",
    "                self.grads.append(grad)\n",
    "                \n",
    "    def forward(self, xs, h):\n",
    "        out = xs\n",
    "        self.layers[1].h = h\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        \n",
    "        self.hs = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        self.dh = self.layers[1].dh\n",
    "        \n",
    "        return dout\n",
    "    \n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        self.layers[1].h = h\n",
    "        \n",
    "        input = start_id\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        while len(word_ids) < sample_size:\n",
    "            out = np.array(input, dtype=np.int32).reshape(1, 1)\n",
    "            for layer in self.layers:\n",
    "                out = layer.forward(out)\n",
    "\n",
    "            sample = int(np.argmax(out))\n",
    "\n",
    "            word_ids.append(sample)\n",
    "            input = sample\n",
    "\n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5f27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import TimeSoftmaxWithLoss\n",
    "\n",
    "class Seq2seq:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        self.V = vocab_size\n",
    "        self.D = wordvec_size\n",
    "        self.H = hidden_size\n",
    "        \n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size, wordvec_size, hidden_size)\n",
    "        self.decoder = Decoder(vocab_size, wordvec_size, hidden_size)\n",
    "        self.softmax_with_loss = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        for param, grad in zip(self.encoder.params, self.encoder.grads):\n",
    "            self.params.append(param)\n",
    "            self.grads.append(grad)\n",
    "            \n",
    "        for param, grad in zip(self.decoder.params, self.decoder.grads):\n",
    "            self.params.append(param)\n",
    "            self.grads.append(grad)\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "        h = self.encoder.forward(xs)\n",
    "        logit = self.decoder.forward(decoder_xs, h)\n",
    "        loss = self.softmax_with_loss.forward(logit, decoder_ts)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.softmax_with_loss.backward()\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dxs = self.encoder.backward(dh)\n",
    "        \n",
    "        return dxs\n",
    "    \n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        h = self.encoder.forward(xs)\n",
    "        word_ids = self.decoder.generate(h, start_id, sample_size)\n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08103947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from optimizer import AdaGrad\n",
    "from trainer import Trainer\n",
    "from dataset import sequence\n",
    "\n",
    "(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import TimeEmbedding, TimeLSTM, TimeAffine\n",
    "\n",
    "class PeekyDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        self.V = vocab_size\n",
    "        self.D = wordvec_size\n",
    "        self.H = hidden_size\n",
    "        \n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "        self.W_embed = (0.01 * np.random.randn(self.V, self.D)).astype('float32')\n",
    "        self.W_x_lstm = ((1 / np.sqrt(self.D + self.H)) * np.random.randn(self.D + self.H, 4*self.H)).astype('float32')\n",
    "        self.W_h_lstm = ((1 / np.sqrt(self.H)) * np.random.randn(self.H, 4*self.H)).astype('float32')\n",
    "        self.b_lstm = np.zeros(4*self.H).astype('float32')\n",
    "        self.W_affine = ((1 / np.sqrt(self.H + self.H)) * np.random.randn(self.H + self.H, self.V)).astype('float32')\n",
    "        self.b_affine = np.zeros(self.V).astype('float32')\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(TimeEmbedding(self.W_embed))\n",
    "        self.layers.append(TimeLSTM(self.W_x_lstm, self.W_h_lstm, self.b_lstm))\n",
    "        self.layers.append(TimeAffine(self.W_affine, self.b_affine))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            for param, grad in zip(layer.params, layer.grads):\n",
    "                self.params.append(param)\n",
    "                self.grads.append(grad)\n",
    "        \n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, xs, h):\n",
    "        N, T = xs.shape\n",
    "        _, H = h.shape\n",
    "        \n",
    "        self.layers[1].h = h\n",
    "        \n",
    "        out = self.layers[0].forward(xs)\n",
    "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "        \n",
    "        out = self.layers[1].forward(out)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "        \n",
    "        logit = self.layers[2].forward(out)\n",
    "        \n",
    "        self.cache = H\n",
    "        self.hs = out\n",
    "        \n",
    "        return logit\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        self.dh = self.layers[1].dh\n",
    "        \n",
    "        return dout\n",
    "    \n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        self.layers[1].h = h\n",
    "        \n",
    "        input = start_id\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        while len(word_ids) < sample_size:\n",
    "            out = np.array(input, dtype=np.int32).reshape(1, 1)\n",
    "            for layer in self.layers:\n",
    "                out = layer.forward(out)\n",
    "\n",
    "            sample = int(np.argmax(out))\n",
    "\n",
    "            word_ids.append(sample)\n",
    "            input = sample\n",
    "\n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a483a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeekySeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        self.V = vocab_size\n",
    "        self.D = wordvec_size\n",
    "        self.H = hidden_size\n",
    "        \n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size, wordvec_size, hidden_size)\n",
    "        self.decoder = PeekyDecoder(vocab_size, wordvec_size, hidden_size)\n",
    "        self.softmax_with_loss = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        for param, grad in zip(self.encoder.params, self.encoder.grads):\n",
    "            self.params.append(param)\n",
    "            self.grads.append(grad)\n",
    "            \n",
    "        for param, grad in zip(self.decoder.params, self.decoder.grads):\n",
    "            self.params.append(param)\n",
    "            self.grads.append(grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
